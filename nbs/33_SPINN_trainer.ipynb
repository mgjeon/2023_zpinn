{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPINN Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp spinn_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import jvp\n",
    "import optax\n",
    "from flax import linen as nn \n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "from typing import Sequence\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hvp_fwdfwd(f, primals, tangents, return_primals=False):\n",
    "    g = lambda primals: jvp(f, (primals,), tangents)[1]\n",
    "    primals_out, tangents_out = jvp(g, primals, tangents)\n",
    "    if return_primals:\n",
    "        return primals_out, tangents_out\n",
    "    else:\n",
    "        return tangents_out\n",
    "    \n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def update_model(optim, gradient, params, state):\n",
    "    updates, state = optim.update(gradient, state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SPINN3d(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    r: int\n",
    "    out_dim: int\n",
    "    pos_enc: int\n",
    "    mlp: str\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, z):\n",
    "        '''\n",
    "        inputs: input factorized coordinates\n",
    "        outputs: feature output of each body network\n",
    "        xy: intermediate tensor for feature merge btw. x and y axis\n",
    "        pred: final model prediction (e.g. for 2d output, pred=[u, v])\n",
    "        '''\n",
    "        if self.pos_enc != 0:\n",
    "            # positional encoding only to spatial coordinates\n",
    "            freq = jnp.expand_dims(jnp.arange(1, self.pos_enc+1, 1), 0)\n",
    "            x = jnp.concatenate((jnp.ones((x.shape[0], 1)), jnp.sin(x@freq), jnp.cos(x@freq)), 1)\n",
    "            y = jnp.concatenate((jnp.ones((y.shape[0], 1)), jnp.sin(y@freq), jnp.cos(y@freq)), 1)\n",
    "            z = jnp.concatenate((jnp.ones((z.shape[0], 1)), jnp.sin(z@freq), jnp.cos(z@freq)), 1)\n",
    "\n",
    "            # causal PINN version (also on time axis)\n",
    "            #  freq_x = jnp.expand_dims(jnp.power(10.0, jnp.arange(0, 3)), 0)\n",
    "            # x = x@freq_x\n",
    "            \n",
    "        inputs, outputs, xy, pred = [x, y, z], [], [], []\n",
    "        init = nn.initializers.glorot_normal()\n",
    "\n",
    "        if self.mlp == 'mlp':\n",
    "            for X in inputs:\n",
    "                for fs in self.features[:-1]:\n",
    "                    X = nn.Dense(fs, kernel_init=init)(X)\n",
    "                    # X = nn.activation.tanh(X)\n",
    "                    X = jnp.sin(X)\n",
    "                X = nn.Dense(self.r*self.out_dim, kernel_init=init)(X)\n",
    "                outputs += [jnp.transpose(X, (1, 0))]\n",
    "\n",
    "        elif self.mlp == 'modified_mlp':\n",
    "            for X in inputs:\n",
    "                U = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                V = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                H = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                for fs in self.features[:-1]:\n",
    "                    Z = nn.Dense(fs, kernel_init=init)(H)\n",
    "                    # Z = nn.activation.tanh(Z)\n",
    "                    Z = jnp.sin(Z)\n",
    "                    H = (jnp.ones_like(Z)-Z)*U + Z*V\n",
    "                H = nn.Dense(self.r*self.out_dim, kernel_init=init)(H)\n",
    "                outputs += [jnp.transpose(H, (1, 0))]\n",
    "        \n",
    "        for i in range(self.out_dim):\n",
    "            xy += [jnp.einsum('fx, fy->fxy', outputs[0][self.r*i:self.r*(i+1)], outputs[1][self.r*i:self.r*(i+1)])]\n",
    "            pred += [jnp.einsum('fxy, fz->xyz', xy[i], outputs[-1][self.r*i:self.r*(i+1)])]\n",
    "\n",
    "        if len(pred) == 1:\n",
    "            # 1-dimensional output\n",
    "            return pred[0]\n",
    "        else:\n",
    "            # n-dimensional output\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jax.jit, static_argnums=(0, 1, 2, 3, 4, 5))\n",
    "def generate_train_data(nx, ny, nz, n_max_x, n_max_y, n_max_z):\n",
    "\n",
    "    xc = jnp.linspace(0, n_max_x, nx).reshape(-1, 1)\n",
    "    yc = jnp.linspace(0, n_max_y, ny).reshape(-1, 1)\n",
    "    zc = jnp.linspace(0, n_max_z, nz).reshape(-1, 1)\n",
    "\n",
    "    # # boundary points\n",
    "    xb = [jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=0   bottom\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=2   top\n",
    "          jnp.array([[0.]]),                     # x=0   lateral_1\n",
    "          jnp.array([[n_max_x]]),                     # x=2   lateral_2\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # y=0   lateral_3\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1)] # y=2   lateral_4\n",
    "\n",
    "    yb = [jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_y]])]\n",
    "\n",
    "    zb = [jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_z]]), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1)]\n",
    "\n",
    "    return xc, yc, zc, xb, yb, zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
    "def generate_train_data_random(key, nx, ny, nz, n_max_x, n_max_y, n_max_z, nc, ncx=None, ncy=None, ncz=None, choice=False):\n",
    "    \n",
    "    keys = jax.random.split(key, 4)\n",
    "\n",
    "    if (ncx is not None) and (ncy is not None) and (ncz is not None):\n",
    "      xc = jax.random.uniform(keys[1], (ncx, 1), minval=0., maxval=n_max_x)\n",
    "      yc = jax.random.uniform(keys[2], (ncy, 1), minval=0., maxval=n_max_y)\n",
    "      zc = jax.random.uniform(keys[3], (ncz, 1), minval=0., maxval=n_max_z)\n",
    "    \n",
    "    elif choice is False:\n",
    "      xc = jax.random.uniform(keys[1], (nc, 1), minval=0., maxval=n_max_x)\n",
    "      yc = jax.random.uniform(keys[2], (nc, 1), minval=0., maxval=n_max_y)\n",
    "      zc = jax.random.uniform(keys[3], (nc, 1), minval=0., maxval=n_max_z)\n",
    "    else:\n",
    "      xc = jnp.linspace(0, n_max_x, nx).reshape(-1, 1)\n",
    "      yc = jnp.linspace(0, n_max_y, ny).reshape(-1, 1)\n",
    "      zc = jnp.linspace(0, n_max_z, nz).reshape(-1, 1)\n",
    "      xc = jax.random.choice(keys[1], xc, shape=(nc,))\n",
    "      yc = jax.random.choice(keys[2], yc, shape=(nc,))\n",
    "      zc = jax.random.choice(keys[3], zc, shape=(nc,))\n",
    "\n",
    "    # boundary points\n",
    "    xb = [jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=0   bottom\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=2   top\n",
    "          jnp.array([[0.]]),                     # x=0   lateral_1\n",
    "          jnp.array([[n_max_x]]),                     # x=2   lateral_2\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # y=0   lateral_3\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1)] # y=2   lateral_4\n",
    "\n",
    "    yb = [jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_y]])]\n",
    "\n",
    "    zb = [jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_z]]), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1)]\n",
    "\n",
    "    return xc, yc, zc, xb, yb, zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def curlx(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_x = uz_y - uy_z\n",
    "    vec_z = jnp.ones(z.shape)\n",
    "    vec_y = jnp.ones(y.shape)\n",
    "    uy_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec_z,))[1]\n",
    "    uz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec_y,))[1]\n",
    "    wx = uz_y - uy_z\n",
    "    return wx\n",
    "\n",
    "\n",
    "def curly(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_y = ux_z - uz_x\n",
    "    vec_z = jnp.ones(z.shape)\n",
    "    vec_x = jnp.ones(x.shape)\n",
    "    ux_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec_z,))[1]\n",
    "    uz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec_x,))[1]\n",
    "    wy = ux_z - uz_x\n",
    "    return wy\n",
    "\n",
    "def curlz(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_z = uy_x - ux_y\n",
    "    vec_y = jnp.ones(y.shape)\n",
    "    vec_x = jnp.ones(x.shape)\n",
    "    ux_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec_y,))[1]\n",
    "    uy_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec_x,))[1]\n",
    "    wz = uy_x - ux_y\n",
    "    return wz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def apply_model_spinn(apply_fn, params, train_boundary_data, w_ff, w_div, w_bc):\n",
    "    def residual_loss(params, x, y, z, w_ff, w_div):\n",
    "        # calculate u\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)\n",
    "        \n",
    "        # calculate J\n",
    "        Jx = curlx(apply_fn, params, x, y, z)\n",
    "        Jy = curly(apply_fn, params, x, y, z)\n",
    "        Jz = curlz(apply_fn, params, x, y, z)\n",
    "        J = jnp.stack([Jx, Jy, Jz], axis=-1)\n",
    "\n",
    "        JxB = jnp.cross(J, B, axis=-1) \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # loss_ff = jnp.sum(JxB**2, axis=-1)\n",
    "        # loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        loss_ff = jnp.sum(JxB**2, axis=-1) / (jnp.sum(B**2, axis=-1) + 1e-7)\n",
    "        loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        # loss_ff = jnp.mean(JxB**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # assumes x, y, z have same shape (very important)\n",
    "        vec_x = jnp.ones(x.shape)\n",
    "        vec_y = jnp.ones(y.shape)\n",
    "        vec_z = jnp.ones(z.shape)\n",
    "        \n",
    "        Bx_x = jvp(lambda x: apply_fn(params, x, y, z)[0], (x,), (vec_x,))[1]\n",
    "        # Bx_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec,))[1]\n",
    "        # Bx_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec,))[1]\n",
    "\n",
    "        # By_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec,))[1]\n",
    "        By_y = jvp(lambda y: apply_fn(params, x, y, z)[1], (y,), (vec_y,))[1]\n",
    "        # By_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec,))[1]\n",
    "\n",
    "        # Bz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec,))[1]\n",
    "        # Bz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec,))[1]\n",
    "        Bz_z = jvp(lambda z: apply_fn(params, x, y, z)[2], (z,), (vec_z,))[1]\n",
    "\n",
    "        divB = Bx_x + By_y + Bz_z\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # loss_div = jnp.sum((divB)**2, axis=-1)\n",
    "        # loss_div = jnp.mean(loss_div)\n",
    "\n",
    "        loss_div = jnp.mean((divB)**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        loss = w_ff*loss_ff + w_div*loss_div\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def boundary_loss(params, x, y, z, *boundary_data):\n",
    "        \n",
    "        # loss = 0.\n",
    "        # for i in np.arange(4):\n",
    "        #     boundary_data_batched = boundary_batches[i, :, :, :]\n",
    "        #     xb = boundary_data_batched[:, 0, :][:, 0].reshape(-1, 1)\n",
    "        #     yb = boundary_data_batched[:, 0, :][:, 1].reshape(-1, 1)\n",
    "        #     zb = boundary_data_batched[:, 0, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bx, By, Bz = apply_fn(params, xb, yb, zb)\n",
    "        #     # Bx, By, Bz = Bx.reshape(-1, 1), By.reshape(-1, 1), Bz.reshape(-1, 1)\n",
    "\n",
    "        #     Bxb = boundary_data_batched[:, 1, :][:, 0].reshape(-1, 1)\n",
    "        #     Byb = boundary_data_batched[:, 1, :][:, 1].reshape(-1, 1)\n",
    "        #     Bzb = boundary_data_batched[:, 1, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bxb_mesh, Byb_mesh, Bzb_mesh = jnp.meshgrid(Bxb.ravel(), Byb.ravel(), Bzb.ravel(), indexing='ij')\n",
    "            \n",
    "        #     loss += jnp.mean((Bx - Bxb_mesh)**2) + jnp.mean((By - Byb_mesh)**2) + jnp.mean((Bz - Bzb_mesh)**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  \n",
    "        #2 x=0   lateral_1            \n",
    "        #3 x=2   lateral_2            \n",
    "        #4 y=0   lateral_3            \n",
    "        #5 y=2   lateral_4            \n",
    "\n",
    "        b_bottom, bp_top, bp_lateral_1, bp_lateral_2, bp_lateral_3, bp_lateral_4 = boundary_data\n",
    "        \n",
    "        loss = 0.\n",
    "        Bx, By, Bz = apply_fn(params,  x[0], y[0], z[0])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - b_bottom[:, :, 0])**2) + jnp.mean((By - b_bottom[:, :, 1])**2) + jnp.mean((Bz - b_bottom[:, :, 2])**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  -> Only normal(Bz), Bx=0, By=0\n",
    "        #2 x=0   lateral_1            -> Only tangential(By, Bz), Bx=0\n",
    "        #3 x=2   lateral_2            -> Only tangential(By, Bz), Bx=0\n",
    "        #4 y=0   lateral_3            -> Only tangential(Bx, Bz), By=0\n",
    "        #5 y=2   lateral_4            -> Only tangential(Bx, Bz), By=0\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2) + jnp.mean(By**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2)\n",
    "        \n",
    "        # Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(By**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(By**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_top[:, :, 0])**2) + jnp.mean((By - bp_top[:, :, 1])**2) + jnp.mean((Bz - bp_top[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_1[:, :, 0])**2) + jnp.mean((By - bp_lateral_1[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_1[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_2[:, :, 0])**2) + jnp.mean((By - bp_lateral_2[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_2[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_3[:, :, 0])**2) + jnp.mean((By - bp_lateral_3[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_3[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_4[:, :, 0])**2) + jnp.mean((By - bp_lateral_4[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_4[:, :, 2])**2)\n",
    "\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # unpack data\n",
    "    train_data = train_boundary_data[0]\n",
    "    boundary_data = train_boundary_data[1]\n",
    "    xc, yc, zc, xb, yb, zb = train_data\n",
    "\n",
    "    # isolate loss func from redundant arguments\n",
    "    loss_fn = lambda params: residual_loss(params, xc, yc, zc, w_ff, w_div) + w_bc*boundary_loss(params, xb, yb, zb, *boundary_data)\n",
    "\n",
    "    loss, gradient = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jax.jit, static_argnums=(0,6))\n",
    "def apply_model_spinn_random(apply_fn, params, train_boundary_data, w_ff, w_div, w_bc, bc_batch_size):\n",
    "    def residual_loss(params, x, y, z, w_ff, w_div):\n",
    "        # calculate u\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)\n",
    "        \n",
    "        # calculate J\n",
    "        Jx = curlx(apply_fn, params, x, y, z)\n",
    "        Jy = curly(apply_fn, params, x, y, z)\n",
    "        Jz = curlz(apply_fn, params, x, y, z)\n",
    "        J = jnp.stack([Jx, Jy, Jz], axis=-1)\n",
    "\n",
    "        JxB = jnp.cross(J, B, axis=-1) \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # loss_ff = jnp.sum(JxB**2, axis=-1)\n",
    "        # loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        loss_ff = jnp.sum(JxB**2, axis=-1) / (jnp.sum(B**2, axis=-1) + 1e-7)\n",
    "        loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        # loss_ff = jnp.mean(JxB**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # assumes x, y, z have same shape (very important)\n",
    "        vec_x = jnp.ones(x.shape)\n",
    "        vec_y = jnp.ones(y.shape)\n",
    "        vec_z = jnp.ones(z.shape)\n",
    "        \n",
    "        Bx_x = jvp(lambda x: apply_fn(params, x, y, z)[0], (x,), (vec_x,))[1]\n",
    "        # Bx_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec,))[1]\n",
    "        # Bx_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec,))[1]\n",
    "\n",
    "        # By_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec,))[1]\n",
    "        By_y = jvp(lambda y: apply_fn(params, x, y, z)[1], (y,), (vec_y,))[1]\n",
    "        # By_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec,))[1]\n",
    "\n",
    "        # Bz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec,))[1]\n",
    "        # Bz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec,))[1]\n",
    "        Bz_z = jvp(lambda z: apply_fn(params, x, y, z)[2], (z,), (vec_z,))[1]\n",
    "\n",
    "        divB = Bx_x + By_y + Bz_z\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # loss_div = jnp.sum((divB)**2, axis=-1)\n",
    "        # loss_div = jnp.mean(loss_div)\n",
    "\n",
    "        loss_div = jnp.mean((divB)**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        loss = w_ff*loss_ff + w_div*loss_div\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def boundary_loss(params, x, y, z, bc_batch_size, *boundary_data):\n",
    "        \n",
    "        # loss = 0.\n",
    "        # for i in np.arange(4):\n",
    "        #     boundary_data_batched = boundary_batches[i, :, :, :]\n",
    "        #     xb = boundary_data_batched[:, 0, :][:, 0].reshape(-1, 1)\n",
    "        #     yb = boundary_data_batched[:, 0, :][:, 1].reshape(-1, 1)\n",
    "        #     zb = boundary_data_batched[:, 0, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bx, By, Bz = apply_fn(params, xb, yb, zb)\n",
    "        #     # Bx, By, Bz = Bx.reshape(-1, 1), By.reshape(-1, 1), Bz.reshape(-1, 1)\n",
    "\n",
    "        #     Bxb = boundary_data_batched[:, 1, :][:, 0].reshape(-1, 1)\n",
    "        #     Byb = boundary_data_batched[:, 1, :][:, 1].reshape(-1, 1)\n",
    "        #     Bzb = boundary_data_batched[:, 1, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bxb_mesh, Byb_mesh, Bzb_mesh = jnp.meshgrid(Bxb.ravel(), Byb.ravel(), Bzb.ravel(), indexing='ij')\n",
    "            \n",
    "        #     loss += jnp.mean((Bx - Bxb_mesh)**2) + jnp.mean((By - Byb_mesh)**2) + jnp.mean((Bz - Bzb_mesh)**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  \n",
    "        #2 x=0   lateral_1            \n",
    "        #3 x=2   lateral_2            \n",
    "        #4 y=0   lateral_3            \n",
    "        #5 y=2   lateral_4            \n",
    "\n",
    "        b_bottom, bp_top, bp_lateral_1, bp_lateral_2, bp_lateral_3, bp_lateral_4 = boundary_data\n",
    "\n",
    "        loss = 0.\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[0], y[0], z[0])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        \n",
    "        bc_bottom_training_data = jnp.vstack([Bx.flatten(), b_bottom[:, :, 0].flatten()])\n",
    "        bc_bottom_training_data = jnp.concatenate([bc_bottom_training_data, jnp.vstack([By.flatten(), b_bottom[:, :, 1].flatten()])], axis=1)\n",
    "        bc_bottom_training_data = jnp.concatenate([bc_bottom_training_data, jnp.vstack([Bz.flatten(), b_bottom[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.vstack([Bx.flatten(), bp_top[:, :, 0].flatten()])\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_top[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_top[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_1[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_1[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_1[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_2[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_2[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_2[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_3[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_3[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_3[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_4[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_4[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_4[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "        bc_latent_batch_size = bc_batch_size // 6\n",
    "        bc_bottom_batch_size = bc_batch_size - bc_latent_batch_size\n",
    "\n",
    "        M = bc_training_data.shape[-1]\n",
    "        random_indices = random.sample(range(M), bc_latent_batch_size)\n",
    "        bc_training_data = bc_training_data[:, random_indices]\n",
    "        loss += jnp.mean((bc_training_data[0] - bc_training_data[1])**2)\n",
    "\n",
    "        M =  bc_bottom_training_data.shape[-1]\n",
    "        random_indices = random.sample(range(M), bc_bottom_batch_size)\n",
    "        bc_bottom_training_data = bc_bottom_training_data[:, random_indices]\n",
    "        loss += jnp.mean((bc_bottom_training_data[0] - bc_bottom_training_data[1])**2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # unpack data\n",
    "    train_data = train_boundary_data[0]\n",
    "    boundary_data = train_boundary_data[1]\n",
    "    xc, yc, zc, xb, yb, zb = train_data\n",
    "\n",
    "    # isolate loss func from redundant arguments\n",
    "    loss_fn = lambda params: residual_loss(params, xc, yc, zc, w_ff, w_div) + w_bc*boundary_loss(params, xb, yb, zb, bc_batch_size, *boundary_data)\n",
    "\n",
    "    loss, gradient = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SPINN_Trainer:\n",
    "    def __init__(self, output_path, BC_path, b_bottom, Nz, b_norm, transfer_learning_path=None, parameters=None):\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        import logging\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        for hdlr in logger.handlers[:]:  # remove all old handlers\n",
    "            logger.removeHandler(hdlr)\n",
    "        logger.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(output_path, \"info_log\")))  # set the new file handler\n",
    "        logger.addHandler(logging.StreamHandler())  # set the new console handler\n",
    "        \n",
    "        self.logger = logger\n",
    "\n",
    "        Nx, Ny, _ = b_bottom.shape\n",
    "\n",
    "        if parameters is None:\n",
    "            features = 512\n",
    "            n_layers = 8\n",
    "            feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "            r = 512 \n",
    "            out_dim = 3 \n",
    "\n",
    "            lr = 5e-4\n",
    "\n",
    "            pos_enc = 0\n",
    "            mlp = 'modified_mlp'\n",
    "\n",
    "            n_max_x = 2*(Nx/Nx)\n",
    "            n_max_y = 2*(Ny/Nx)\n",
    "            n_max_z = 2*(Nz/Nx)\n",
    "\n",
    "            parameters = {'feat_sizes' : feat_sizes, \n",
    "                'r' : r, \n",
    "                'out_dim' : out_dim, \n",
    "                'Nx' : Nx, \n",
    "                'Ny' : Ny, \n",
    "                'Nz' : Nz, \n",
    "                'b_norm' : b_norm,\n",
    "                'pos_enc' : pos_enc,\n",
    "                'mlp' : mlp,\n",
    "                'lr': lr,\n",
    "                'n_max_x': n_max_x,\n",
    "                'n_max_y': n_max_y,\n",
    "                'n_max_z': n_max_z,}\n",
    "        else:\n",
    "            feat_sizes = parameters['feat_sizes']\n",
    "            r = parameters['r']\n",
    "            out_dim = parameters['out_dim']\n",
    "            Nx = parameters['Nx']\n",
    "            Ny = parameters['Ny']\n",
    "            Nz = parameters['Nz']\n",
    "            b_norm = parameters['b_norm']\n",
    "            pos_enc = parameters['pos_enc']\n",
    "            mlp = parameters['mlp']\n",
    "            lr = parameters['lr']\n",
    "            lr_decay_iterations = parameters['lr_decay_iterations']\n",
    "            n_max_x = parameters['n_max_x']\n",
    "            n_max_y = parameters['n_max_y']\n",
    "            n_max_z = parameters['n_max_z']\n",
    "            is_random = parameters['is_random']\n",
    "            Nc = parameters['Nc']\n",
    "            decay_rate = parameters['decay_rate']\n",
    "            series_lr = parameters['series_lr']\n",
    "\n",
    "        logger.info(parameters)\n",
    "        \n",
    "        parameters_path = os.path.join(output_path, \"parameters.pickle\")\n",
    "        with open(parameters_path, \"wb\") as f:\n",
    "            pickle.dump(parameters, f)\n",
    "\n",
    "        seed = 111\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "        model = SPINN3d(feat_sizes, r, out_dim, pos_enc=pos_enc, mlp=mlp)\n",
    "        if transfer_learning_path is None:\n",
    "            params = model.init(\n",
    "                        subkey,\n",
    "                        jnp.ones((Nx, 1)),\n",
    "                        jnp.ones((Ny, 1)),\n",
    "                        jnp.ones((Nz, 1))\n",
    "                    )\n",
    "            apply_fn = jax.jit(model.apply)\n",
    "\n",
    "            if lr_decay_iterations is not None: \n",
    "                optim = optax.adam(learning_rate=optax.exponential_decay(init_value=lr, transition_steps=lr_decay_iterations,\n",
    "                                                                         decay_rate=decay_rate))\n",
    "            else:\n",
    "                optim = optax.adam(learning_rate=lr)\n",
    "\n",
    "            state = optim.init(params)\n",
    "        else:\n",
    "            model.init(\n",
    "                        subkey,\n",
    "                        jnp.ones((Nx, 1)),\n",
    "                        jnp.ones((Ny, 1)),\n",
    "                        jnp.ones((Nz, 1))\n",
    "                    )\n",
    "            apply_fn = jax.jit(model.apply)\n",
    "            with open(transfer_learning_path, 'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "\n",
    "            if lr_decay_iterations is not None: \n",
    "                optim = optax.adam(learning_rate=optax.exponential_decay(init_value=series_lr, transition_steps=lr_decay_iterations,\n",
    "                                                                         decay_rate=decay_rate))\n",
    "            else:\n",
    "                optim = optax.adam(learning_rate=lr)\n",
    "                \n",
    "            state = optim.init(params)\n",
    "\n",
    "        with open(BC_path, 'rb') as f:\n",
    "            boundary_data = pickle.load(f)\n",
    "        \n",
    "        if is_random is True:\n",
    "            Ncx = parameters['Ncx']\n",
    "            Ncy = parameters['Ncy']\n",
    "            Ncz = parameters['Ncz']\n",
    "            choice = parameters['choice']\n",
    "            train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "        else:\n",
    "            train_data = generate_train_data(Nx, Ny, Nz, n_max_x, n_max_y, n_max_z)\n",
    "\n",
    "        train_boundary_data = [train_data, boundary_data]\n",
    "\n",
    "        self.apply_fn = apply_fn\n",
    "        self.params = params\n",
    "        self.train_boundary_data = train_boundary_data\n",
    "        self.optim = optim\n",
    "        self.state = state\n",
    "        self.output_path = output_path\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        self.parameters = parameters\n",
    "        self.boundary_data = boundary_data\n",
    "\n",
    "        self.key = key\n",
    "\n",
    "    def train(self, total_iterations, log_iterations, loss_threshold=0.001):\n",
    "        params = self.params\n",
    "        state = self.state\n",
    "\n",
    "        logger = self.logger\n",
    "\n",
    "        parameters = self.parameters \n",
    "        is_random = parameters['is_random']\n",
    "        Nc = parameters['Nc']\n",
    "        key = self.key \n",
    "\n",
    "        w_ff = parameters['w_ff']\n",
    "        w_div = parameters['w_div']\n",
    "        w_bc = parameters['w_bc']\n",
    "        w_bc_decay_iterations = parameters['w_bc_decay_iterations']\n",
    "        w_bc_decay = (1 / w_bc) ** (1 / w_bc_decay_iterations) if w_bc_decay_iterations is not None else 1\n",
    "\n",
    "        bc_batch_size = parameters['bc_batch_size']\n",
    "\n",
    "        if is_random is True:\n",
    "            random_interval = parameters['random_interval']\n",
    "            Nx = parameters['Nx']\n",
    "            Ny = parameters['Ny']\n",
    "            Nz = parameters['Nz']\n",
    "            n_max_x = parameters['n_max_x']\n",
    "            n_max_y = parameters['n_max_y']\n",
    "            n_max_z = parameters['n_max_z']\n",
    "            Ncx = parameters['Ncx']\n",
    "            Ncy = parameters['Ncy']\n",
    "            Ncz = parameters['Ncz']\n",
    "            key, subkey = jax.random.split(key, 2)\n",
    "            choice = parameters['choice']\n",
    "\n",
    "        losses = []\n",
    "        if logger is None:\n",
    "            print('Complie Start')\n",
    "        else: \n",
    "            logger.info('Complie Start')\n",
    "        start = time.time()\n",
    "\n",
    "        if bc_batch_size is not None:\n",
    "            loss, gradient = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "        else:\n",
    "            loss, gradient = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        params, state = update_model(self.optim, gradient, params, state)\n",
    "\n",
    "        if w_bc > 1:\n",
    "            w_bc *= w_bc_decay\n",
    "            if w_bc <= 1:\n",
    "                w_bc = 1\n",
    "\n",
    "        if is_random is True:\n",
    "            train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "            self.train_boundary_data = [train_data, self.boundary_data]\n",
    "        \n",
    "        if bc_batch_size is not None:\n",
    "            loss, gradient = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "        else:\n",
    "            loss, gradient = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "        losses.append(loss.item())\n",
    "        params, state = update_model(self.optim, gradient, params, state)\n",
    "        runtime = time.time() - start\n",
    "        if logger is None:\n",
    "            print(f'Complie End --> total: {runtime:.2f}sec')\n",
    "        else:\n",
    "            logger.info(f'Complie End --> total: {runtime:.2f}sec')\n",
    "\n",
    "        \n",
    "        start = time.time()\n",
    "        for e in trange(1, total_iterations + 1):\n",
    "\n",
    "            if w_bc > 1:\n",
    "                w_bc *= w_bc_decay\n",
    "                if w_bc <= 1:\n",
    "                   w_bc = 1\n",
    "\n",
    "            if is_random is True:\n",
    "                if e % random_interval == 0:\n",
    "                    # sample new input data\n",
    "                    key, subkey = jax.random.split(key, 2)\n",
    "                    train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                    self.train_boundary_data = [train_data, self.boundary_data]\n",
    "\n",
    "            if bc_batch_size is not None:\n",
    "                loss, gradient = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "            else:\n",
    "                loss, gradient = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "            losses.append(loss.item())\n",
    "            if loss.item() < loss_threshold:\n",
    "                if logger is None:\n",
    "                    print(f'Epoch: {e}/{total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                else:\n",
    "                    logger.info(f'Epoch: {e}/{total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                break\n",
    "            \n",
    "            params, state = update_model(self.optim, gradient, params, state)\n",
    "            \n",
    "            if e % log_iterations == 0:\n",
    "                if logger is None:\n",
    "                    print(f'Epoch: {e}/{total_iterations} --> total loss: {loss:.8f}')\n",
    "                else:\n",
    "                    logger.info(f'Epoch: {e}/{total_iterations} --> total loss: {loss:.8f}')\n",
    "                params_path = os.path.join(self.output_path, f\"params_{e}.pickle\")\n",
    "                with open(params_path, \"wb\") as f:\n",
    "                    pickle.dump(params, f)\n",
    "\n",
    "        final_params_path = os.path.join(self.output_path, f\"final_params.pickle\")\n",
    "        with open(final_params_path, \"wb\") as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "        np.save(os.path.join(self.output_path, 'losses.npy'), losses)\n",
    "\n",
    "        runtime = time.time() - start\n",
    "        if logger is None:\n",
    "            print(f'Runtime --> total: {runtime:.2f}sec ({(runtime/(total_iterations-1)*1000):.2f}ms/iter.)')\n",
    "        else:\n",
    "            logger.info(f'Runtime --> total: {runtime:.2f}sec ({(runtime/(total_iterations-1)*1000):.2f}ms/iter.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pyvista as pv\n",
    "import pickle\n",
    "from cmspinn.mag_viz import create_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class spinn_cube:\n",
    "    def __init__(self, param_path, parameters_path):\n",
    "        self.param_path = param_path\n",
    "        self.parameters_path = parameters_path\n",
    "    \n",
    "    def calculate_magnetic_fields(self):\n",
    "        param_path = self.param_path\n",
    "        parameters_path = self.parameters_path\n",
    "\n",
    "        with open(parameters_path, \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "\n",
    "        feat_sizes = parameters['feat_sizes']\n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "\n",
    "        subkey = jax.random.PRNGKey(0)\n",
    "        model = SPINN3d(feat_sizes, r, out_dim, pos_enc=pos_enc, mlp=mlp)\n",
    "        model.init(\n",
    "                    subkey,\n",
    "                    jnp.ones((Nx, 1)),\n",
    "                    jnp.ones((Ny, 1)),\n",
    "                    jnp.ones((Nz, 1))\n",
    "                   )\n",
    "        apply_fn = jax.jit(model.apply)\n",
    "\n",
    "        with open(param_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        x = jnp.linspace(0, n_max_x, Nx).reshape(-1, 1)\n",
    "        y = jnp.linspace(0, n_max_y, Ny).reshape(-1, 1)\n",
    "        z = jnp.linspace(0, n_max_z, Nz).reshape(-1, 1)\n",
    "        x, y, z = jax.lax.stop_gradient(x), jax.lax.stop_gradient(y), jax.lax.stop_gradient(z)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)*b_norm\n",
    "        \n",
    "        Bx = B[..., 0]\n",
    "        By = B[..., 1]\n",
    "        Bz = B[..., 2]\n",
    "\n",
    "        co_bounds = (0, Nx-1, 0, Ny-1, 0, Nz-1)\n",
    "        co_coords = create_coordinates(co_bounds).reshape(-1, 3)\n",
    "        co_coord = co_coords.reshape(Nx, Ny, Nz, 3)\n",
    "        x = co_coord[..., 0]\n",
    "        y = co_coord[..., 1]\n",
    "        z = co_coord[..., 2]\n",
    "        mesh = pv.StructuredGrid(x, y, z)\n",
    "        vectors = np.stack([Bx, By, Bz], axis=-1).transpose(2, 1, 0, 3).reshape(-1, 3)\n",
    "        mesh['B'] = vectors\n",
    "        mesh.active_vectors_name = 'B'\n",
    "        magnitude = np.linalg.norm(vectors, axis=-1)\n",
    "        mesh['mag'] = magnitude\n",
    "        mesh.active_scalars_name = 'mag'\n",
    "\n",
    "        self.grid = mesh \n",
    "        return self.grid"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
