{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old NF2 code (before lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pinn_nf2_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.cuda import get_device_name\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "\n",
    "from astropy.nddata import block_reduce, block_replicate\n",
    "from sunpy.map import Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, b_n, r_p):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b_n', b_n)\n",
    "        self.register_buffer('r_p', r_p)\n",
    "        c = np.zeros((1, 3))\n",
    "        c[:, 2] = (1 / np.sqrt(2 * np.pi))\n",
    "        c = torch.tensor(c, dtype=torch.float32, )\n",
    "        self.register_buffer('c', c)\n",
    "\n",
    "    def forward(self, coord):\n",
    "        v1 = self.b_n[:, None]\n",
    "        v2 = 2 * np.pi * ((-self.r_p[:, None] + coord[None, :] + self.c[None]) ** 2).sum(-1) ** 0.5\n",
    "        potential = torch.sum(v1 / v2, dim=0)\n",
    "        return potential\n",
    "\n",
    "\n",
    "def get_potential(b_n, height, batch_size=2048, strides=(1, 1, 1), progress=True):\n",
    "    cube_shape = (*b_n.shape, height)\n",
    "    strides = (strides, strides, strides) if isinstance(strides, int) else strides\n",
    "    b_n = b_n.reshape((-1,)).astype(np.float32)\n",
    "    coords = np.stack(np.mgrid[:cube_shape[0]:strides[0], :cube_shape[1]:strides[1], :cube_shape[2]:strides[2]], -1).reshape((-1, 3))\n",
    "    r_p = np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], :1], -1).reshape((-1, 3))\n",
    "\n",
    "    # torch code\n",
    "    # r = (x * y, 3); coords = (x*y*z, 3), c = (1, 3)\n",
    "    # --> (x * y, x * y * z, 3) --> (x * y, x * y * z) --> (x * y * z)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        b_n = torch.tensor(b_n, dtype=torch.float32, )\n",
    "        r_p = torch.tensor(r_p, dtype=torch.float32, )\n",
    "        model = nn.DataParallel(PotentialModel(b_n, r_p)).to(device)\n",
    "\n",
    "        coords = torch.tensor(coords, dtype=torch.float32)\n",
    "        potential = []\n",
    "        loader = DataLoader(TensorDataset(coords), batch_size=batch_size, num_workers=8)\n",
    "        it = tqdm(loader, desc='Potential Field') if progress else loader\n",
    "        for coord, in it:\n",
    "            coord = coord.to(device)\n",
    "            p_batch = model(coord)\n",
    "            potential += [p_batch]\n",
    "\n",
    "    potential = torch.cat(potential).view(cube_shape).cpu().numpy()\n",
    "    if strides != (1, 1, 1):\n",
    "        potential = block_replicate(potential, strides, conserve_sum=False)\n",
    "    return potential\n",
    "\n",
    "\n",
    "def get_potential_boundary(b_n, height, batch_size=2048):\n",
    "    assert not np.any(np.isnan(b_n)), 'Invalid data value'\n",
    "\n",
    "    cube_shape = (*b_n.shape, height)\n",
    "\n",
    "    b_n = b_n.reshape((-1)).astype(np.float32)\n",
    "    coords = [np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], cube_shape[2] - 2:cube_shape[2] + 1], -1),\n",
    "              np.stack(np.mgrid[:cube_shape[0], -1:2, :cube_shape[2]], -1),\n",
    "              np.stack(np.mgrid[:cube_shape[0], cube_shape[1] - 2:cube_shape[1] + 1, :cube_shape[2]], -1),\n",
    "              np.stack(np.mgrid[-1:2, :cube_shape[1], :cube_shape[2]], -1),\n",
    "              np.stack(np.mgrid[cube_shape[0] - 2:cube_shape[0] + 1, :cube_shape[1], :cube_shape[2]], -1), ]\n",
    "    coords_shape = [c.shape[:-1] for c in coords]\n",
    "    flat_coords = np.concatenate([c.reshape(((-1, 3))) for c in coords])\n",
    "\n",
    "    r_p = np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], :1], -1).reshape((-1, 3))\n",
    "\n",
    "    # torch code\n",
    "    # r = (x * y, 3); coords = (x*y*z, 3), c = (1, 3)\n",
    "    # --> (x * y, x * y * z, 3) --> (x * y, x * y * z) --> (x * y * z)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        b_n = torch.tensor(b_n, dtype=torch.float32, )\n",
    "        r_p = torch.tensor(r_p, dtype=torch.float32, )\n",
    "        model = nn.DataParallel(PotentialModel(b_n, r_p, )).to(device)\n",
    "\n",
    "        flat_coords = torch.tensor(flat_coords, dtype=torch.float32, )\n",
    "\n",
    "        potential = []\n",
    "        for coord, in tqdm(DataLoader(TensorDataset(flat_coords), batch_size=batch_size, num_workers=2),\n",
    "                           desc='Potential Boundary'):\n",
    "            coord = coord.to(device)\n",
    "            p_batch = model(coord)\n",
    "            potential += [p_batch.cpu()]\n",
    "\n",
    "    potential = torch.cat(potential).numpy()\n",
    "    idx = 0\n",
    "    fields = []\n",
    "    for s in coords_shape:\n",
    "        p = potential[idx:idx + np.prod(s)].reshape(s)\n",
    "        b = - 1 * np.stack(np.gradient(p, axis=[0, 1, 2], edge_order=2), axis=-1)\n",
    "        fields += [b]\n",
    "        idx += np.prod(s)\n",
    "\n",
    "    fields = [fields[0][:, :, 1].reshape((-1, 3)),\n",
    "              fields[1][:, 1, :].reshape((-1, 3)), fields[2][:, 1, :].reshape((-1, 3)),\n",
    "              fields[3][1, :, :].reshape((-1, 3)), fields[4][1, :, :].reshape((-1, 3))]\n",
    "    coords = [coords[0][:, :, 1].reshape((-1, 3)),\n",
    "              coords[1][:, 1, :].reshape((-1, 3)), coords[2][:, 1, :].reshape((-1, 3)),\n",
    "              coords[3][1, :, :].reshape((-1, 3)), coords[4][1, :, :].reshape((-1, 3))]\n",
    "    return np.concatenate(coords), np.concatenate(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prep_b_data(b_bottom,\n",
    "                height, spatial_norm, b_norm,\n",
    "                potential_boundary=True, potential_strides=4,\n",
    "                plot=False, plot_path=None):\n",
    "    # load coordinates\n",
    "    mf_coords = np.stack(np.mgrid[:b_bottom.shape[0], :b_bottom.shape[1], :1], -1)\n",
    "    # flatten data\n",
    "    mf_coords = mf_coords.reshape((-1, 3))\n",
    "    mf_values = b_bottom.reshape((-1, 3))\n",
    "    # mf_err = error_cube.reshape((-1, 3))\n",
    "    # load potential field\n",
    "    if potential_boundary:\n",
    "        pf_coords, pf_values = _load_potential_field_data(b_bottom, height, potential_strides)\n",
    "        # concatenate pf data points\n",
    "        coords = np.concatenate([pf_coords, mf_coords])\n",
    "        values = np.concatenate([pf_values, mf_values])\n",
    "        # err = np.concatenate([pf_err, mf_err])\n",
    "    else:\n",
    "        coords = mf_coords\n",
    "        values = mf_values\n",
    "        # err = mf_err\n",
    "\n",
    "    coords = coords.astype(np.float32)\n",
    "    values = values.astype(np.float32)\n",
    "    # err = err.astype(np.float32)\n",
    "\n",
    "    # normalize data\n",
    "    values = Normalize(-b_norm, b_norm, clip=False)(values) * 2 - 1\n",
    "    # err = Normalize(0, b_norm, clip=False)(err)\n",
    "\n",
    "    # apply spatial normalization\n",
    "    coords = coords / spatial_norm\n",
    "\n",
    "    # stack to numpy array\n",
    "    data = np.stack([coords, values], 1)\n",
    "\n",
    "    if plot:\n",
    "        _plot_data(b_bottom, plot_path, b_norm)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_hmi_data(data_path):\n",
    "    if isinstance(data_path, str):\n",
    "        hmi_p = sorted(glob.glob(os.path.join(data_path, '*Bp.fits')))[0]  # x\n",
    "        hmi_t = sorted(glob.glob(os.path.join(data_path, '*Bt.fits')))[0]  # y\n",
    "        hmi_r = sorted(glob.glob(os.path.join(data_path, '*Br.fits')))[0]  # z\n",
    "        # err_p = sorted(glob.glob(os.path.join(data_path, '*Bp_err.fits')))[0]  # x\n",
    "        # err_t = sorted(glob.glob(os.path.join(data_path, '*Bt_err.fits')))[0]  # y\n",
    "        # err_r = sorted(glob.glob(os.path.join(data_path, '*Br_err.fits')))[0]  # z\n",
    "    else:\n",
    "        hmi_p, err_p, hmi_r, err_r, hmi_t, err_t = data_path\n",
    "    # laod maps\n",
    "    b_bottom = np.stack([Map(hmi_p).data, -Map(hmi_t).data, Map(hmi_r).data]).transpose()\n",
    "    # error_cube = np.stack([Map(err_p).data, Map(err_t).data, Map(err_r).data]).transpose()\n",
    "    return b_bottom, Map(hmi_r).meta\n",
    "\n",
    "\n",
    "def _load_potential_field_data(b_bottom, height, reduce):\n",
    "    if reduce > 1:\n",
    "        b_bottom = block_reduce(b_bottom, (reduce, reduce, 1), func=np.mean)\n",
    "        height = height // reduce\n",
    "    pf_batch_size = int(1024 * 512 ** 2 / np.prod(b_bottom.shape[:2]))  # adjust batch to AR size\n",
    "    pf_coords, pf_values = get_potential_boundary(b_bottom[:, :, 2], height, batch_size=pf_batch_size)\n",
    "    pf_values = np.array(pf_values, dtype=np.float32)\n",
    "    pf_coords = np.array(pf_coords, dtype=np.float32) * reduce # expand to original coordinate spacing\n",
    "    # pf_err = np.zeros_like(pf_values)\n",
    "    return pf_coords, pf_values\n",
    "\n",
    "\n",
    "def _plot_data(n_b_bottom, plot_path, b_norm):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(n_b_bottom[..., 0].transpose(), vmin=-b_norm, vmax=b_norm, cmap='gray', origin='lower')\n",
    "    axs[1].imshow(n_b_bottom[..., 1].transpose(), vmin=-b_norm, vmax=b_norm, cmap='gray', origin='lower')\n",
    "    axs[2].imshow(n_b_bottom[..., 2].transpose(), vmin=-b_norm, vmax=b_norm, cmap='gray', origin='lower')\n",
    "    plt.savefig(os.path.join(plot_path, 'b.jpg'))\n",
    "    plt.close()\n",
    "    # fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    # axs[0].imshow(error_cube[..., 0].transpose(), vmin=0, cmap='gray', origin='lower')\n",
    "    # axs[1].imshow(error_cube[..., 1].transpose(), vmin=0, cmap='gray', origin='lower')\n",
    "    # axs[2].imshow(error_cube[..., 2].transpose(), vmin=0, cmap='gray', origin='lower')\n",
    "    # plt.savefig(os.path.join(plot_path, 'b_err.jpg'))\n",
    "    # plt.close()\n",
    "\n",
    "\n",
    "class RandomCoordinateSampler():\n",
    "\n",
    "    def __init__(self, cube_shape, spatial_norm, batch_size, cuda=True):\n",
    "        self.cube_shape = cube_shape\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.batch_size = batch_size\n",
    "        self.float_tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    def load_sample(self):\n",
    "        random_coords = self.float_tensor(self.batch_size, 3).uniform_()\n",
    "        random_coords[:, 0] *= self.cube_shape[0] / self.spatial_norm\n",
    "        random_coords[:, 1] *= self.cube_shape[1] / self.spatial_norm\n",
    "        random_coords[:, 2] *= self.cube_shape[2] / self.spatial_norm\n",
    "        return random_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BoundaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, batches_path):\n",
    "        \"\"\"Data set for lazy loading a pre-batched numpy data array.\n",
    "\n",
    "        :param batches_path: path to the numpy array.\n",
    "        \"\"\"\n",
    "        self.batches_path = batches_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.load(self.batches_path, mmap_mode='r').shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy load data\n",
    "        d = np.load(self.batches_path, mmap_mode='r')[idx]\n",
    "        d = np.copy(d)\n",
    "        coord, field = d[:, 0],  d[:, 1]\n",
    "        return coord, field\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, cube_shape, norm, z=0):\n",
    "        coordinates = np.stack(np.mgrid[:cube_shape[0],\n",
    "                               :cube_shape[1]], -1)\n",
    "        self.coordinates = coordinates\n",
    "        self.coordinates_flat = coordinates.reshape((-1, 2))\n",
    "        self.norm = norm\n",
    "        self.z = z / self.norm\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return self.coordinates_flat.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coord = self.coordinates_flat[idx]\n",
    "        scaled_coord = [coord[0] / self.norm,\n",
    "                        coord[1] / self.norm,\n",
    "                        self.z]\n",
    "        return np.array(scaled_coord, dtype=np.float32)\n",
    "\n",
    "\n",
    "class CubeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, cube_shape, norm, block_shape=(32, 32, 32), coords=[], strides=1):\n",
    "        self.cube_shape = cube_shape\n",
    "        self.block_shape = block_shape\n",
    "        self.coords = coords\n",
    "        self.strides = strides\n",
    "        self.norm = norm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coord = self.coords[idx]\n",
    "        return self.getCube(coord)\n",
    "\n",
    "    def getCube(self, coord):\n",
    "        coord_cube = np.stack(np.mgrid[coord[0]:coord[0] + self.block_shape[0]:self.strides,\n",
    "                              coord[1]:coord[1] + self.block_shape[1]:self.strides,\n",
    "                              coord[2]:coord[2] + self.block_shape[2]:self.strides, ], -1)\n",
    "        coord_cube = np.stack([coord_cube[..., 0] / self.norm,\n",
    "                               coord_cube[..., 1] / self.norm,\n",
    "                               coord_cube[..., 2] / self.norm, ], -1)\n",
    "        return coord_cube.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class BModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_coords, out_values, dim, pos_encoding=False):\n",
    "        super().__init__()\n",
    "        if pos_encoding:\n",
    "            posenc = PositionalEncoding(8, 20)\n",
    "            d_in = nn.Linear(in_coords * 40, dim)\n",
    "            self.d_in = nn.Sequential(posenc, d_in)\n",
    "        else:\n",
    "            self.d_in = nn.Linear(in_coords, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, out_values)\n",
    "        self.activation = Sine()  # torch.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        x = self.d_out(x)\n",
    "        return x\n",
    "\n",
    "class VectorPotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_coords, dim, pos_encoding=False):\n",
    "        super().__init__()\n",
    "        if pos_encoding:\n",
    "            posenc = PositionalEncoding(8, 20)\n",
    "            d_in = nn.Linear(in_coords * 40, dim)\n",
    "            self.d_in = nn.Sequential(posenc, d_in)\n",
    "        else:\n",
    "            self.d_in = nn.Linear(in_coords, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, 3)\n",
    "        self.activation = Sine()  # torch.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        coord = x\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        a = self.d_out(x)\n",
    "        #\n",
    "        jac_matrix = jacobian(a, coord)\n",
    "        dAy_dx = jac_matrix[:, 1, 0]\n",
    "        dAz_dx = jac_matrix[:, 2, 0]\n",
    "        dAx_dy = jac_matrix[:, 0, 1]\n",
    "        dAz_dy = jac_matrix[:, 2, 1]\n",
    "        dAx_dz = jac_matrix[:, 0, 2]\n",
    "        dAy_dz = jac_matrix[:, 1, 2]\n",
    "        rot_x = dAz_dy - dAy_dz\n",
    "        rot_y = dAx_dz - dAz_dx\n",
    "        rot_z = dAy_dx - dAx_dy\n",
    "        b = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "        #\n",
    "        return b\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding of the input coordinates.\n",
    "\n",
    "    encodes x to (..., sin(2^k x), cos(2^k x), ...)\n",
    "    k takes \"num_freqs\" number of values equally spaced between [0, max_freq]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_freq, num_freqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_freq (int): maximum frequency in the positional encoding.\n",
    "            num_freqs (int): number of frequencies between [0, max_freq]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        freqs = 2 ** torch.linspace(0, max_freq, num_freqs)\n",
    "        self.register_buffer(\"freqs\", freqs)  # (num_freqs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: (batch, num_samples, in_features)\n",
    "        Outputs:\n",
    "            out: (batch, num_samples, 2*num_freqs*in_features)\n",
    "        \"\"\"\n",
    "        x_proj = x.unsqueeze(dim=-2) * self.freqs.unsqueeze(dim=-1)  # (num_rays, num_samples, num_freqs, in_features)\n",
    "        x_proj = x_proj.reshape(*x.shape[:-1], -1)  # (num_rays, num_samples, num_freqs*in_features)\n",
    "        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)],\n",
    "                        dim=-1)  # (num_rays, num_samples, 2*num_freqs*in_features)\n",
    "        return out\n",
    "\n",
    "\n",
    "def jacobian(output, coords):\n",
    "    jac_matrix = [torch.autograd.grad(output[:, i], coords,\n",
    "                                      grad_outputs=torch.ones_like(output[:, i]).to(output),\n",
    "                                      retain_graph=True,\n",
    "                                      create_graph=True)[0]\n",
    "                  for i in range(output.shape[1])]\n",
    "    jac_matrix = torch.stack(jac_matrix, dim=1)\n",
    "    return jac_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NF2Trainer:\n",
    "\n",
    "    def __init__(self, base_path, b_bottom, height, spatial_norm, b_norm, meta_info=None, dim=256,\n",
    "                 positional_encoding=False, meta_path=None, use_potential_boundary=True, potential_strides=4, use_vector_potential=False,\n",
    "                 w_div=0.1, w_ff=0.1, decay_iterations=None,\n",
    "                 device=None, work_directory=None):\n",
    "        \"\"\"Magnetic field extrapolations trainer\n",
    "\n",
    "        :param base_path: path to the results folder.\n",
    "        :param b_bottom: magnetic field data (x, y, (Bp, -Bt, Br)).\n",
    "        :param error_cube: associated error information.\n",
    "        :param height: height of simulation volume.\n",
    "        :param spatial_norm: normalization of coordinate axis.\n",
    "        :param b_norm: normalization of magnetic field strength.\n",
    "        :param meta_info: additional data information. stored in the save state.\n",
    "        :param dim: number of neurons per layer (8 layers).\n",
    "        :param positional_encoding: use positional encoding.\n",
    "        :param meta_path: start from a pre-learned simulation state.\n",
    "        :param use_potential_boundary: use potential field as boundary condition. If None use an open boundary.\n",
    "        :param potential_strides: use binned potential field boundary condition. Only applies if use_potential_boundary = True.\n",
    "        :param use_vector_potential: derive the magnetic field from a vector potential.\n",
    "        :param w_div: weighting parameter for divergence freeness of the simulation.\n",
    "        :param w_ff: weighting parameter for force freeness of the simulation.\n",
    "        :param decay_iterations: decay weighting for boundary condition (w_bc=1000) over n iterations to 1.\n",
    "        :param device: device for model training.\n",
    "        :param work_directory: directory to store scratch data (prepared batches).\n",
    "        \"\"\"\n",
    "\n",
    "        # general parameters\n",
    "        self.base_path = base_path\n",
    "        work_directory = base_path if work_directory is None else work_directory\n",
    "        self.work_directory = work_directory\n",
    "        # self.save_path = os.path.join(base_path, 'extrapolation_result.nf2')\n",
    "        self.checkpoint_path = os.path.join(base_path, 'checkpoint.pt')\n",
    "\n",
    "        # data parameters\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.height = height\n",
    "        self.b_norm = b_norm\n",
    "        self.meta_info = meta_info\n",
    "\n",
    "        # init directories\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        os.makedirs(work_directory, exist_ok=True)\n",
    "\n",
    "        # init logging\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        for hdlr in logger.handlers[:]:  # remove all old handlers\n",
    "            logger.removeHandler(hdlr)\n",
    "        logger.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(base_path, \"info_log\")))  # set the new file handler\n",
    "        logger.addHandler(logging.StreamHandler())  # set the new console handler\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        # log settings\n",
    "        self.logger.info('Configuration:')\n",
    "        self.logger.info(\n",
    "            'dim: %d, w_div: %f, w_ff: %f, decay_iterations: %s, potential: %s, vector_potential: %s, ' % (\n",
    "                dim, w_div, w_ff, str(decay_iterations), str(use_potential_boundary),\n",
    "                str(use_vector_potential)))\n",
    "\n",
    "        # # setup device\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        device_names = [get_device_name(i) for i in range(n_gpus)]\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.logger.info('Using device: %s (gpus %d) %s' % (str(device), n_gpus, str(device_names)))\n",
    "        self.device = device\n",
    "\n",
    "        # prepare data\n",
    "        self.b_bottom = b_bottom\n",
    "        # self.error_cube = error_cube\n",
    "\n",
    "        # load dataset\n",
    "        self.data = prep_b_data(b_bottom, height, spatial_norm, b_norm,\n",
    "                                plot=True, plot_path=base_path,\n",
    "                                potential_boundary=use_potential_boundary, potential_strides=potential_strides)\n",
    "        self.cube_shape = [*b_bottom.shape[:-1], height]\n",
    "\n",
    "        # init model\n",
    "        # if use_vector_potential:\n",
    "        #     model = VectorPotentialModel(3, dim, pos_encoding=positional_encoding)\n",
    "        # else:\n",
    "        self.model = nn.DataParallel(BModel(3, 3, dim, pos_encoding=positional_encoding)).to(device)\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=5e-4)\n",
    "\n",
    "        # load last state\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            state_dict = torch.load(self.checkpoint_path, map_location=device)\n",
    "            self.model.load_state_dict(state_dict['m'])\n",
    "            self.opt.load_state_dict(state_dict['o'])\n",
    "\n",
    "            start_iteration = state_dict['iteration']\n",
    "            self.w_bc = state_dict['w_bc']\n",
    "            self.logger.info('Resuming training from iteration %d' % start_iteration)\n",
    "        else:\n",
    "            if meta_path:\n",
    "                state_dict = torch.load(meta_path, map_location=device)['m']\n",
    "                self.model.load_state_dict(state_dict)\n",
    "                self.opt = torch.optim.Adam(self.model.parameters(), lr=5e-5)\n",
    "                self.logger.info('Loaded meta state: %s' % meta_path)\n",
    "\n",
    "            # init\n",
    "            start_iteration = 0\n",
    "            self.w_bc = 1000 if decay_iterations else 1\n",
    "\n",
    "\n",
    "        self.start_iteration = start_iteration\n",
    "        self.w_bc_decay = (1 / 1000) ** (1 / decay_iterations) if decay_iterations is not None else 1\n",
    "        self.w_div, self.w_ff = w_div, w_ff\n",
    "\n",
    "    def train(self, total_iterations, lr_decay_iterations, co_batch_size, bc_batch_size, log_interval=100, validation_interval=100, num_workers=None):\n",
    "        \"\"\"Start magnetic field extrapolation fit.\n",
    "\n",
    "        :param total_iterations: number of iterations for training.\n",
    "        :param batch_size: number of samples per iteration.\n",
    "        :param log_interval: log training details every nth iteration.\n",
    "        :param validation_interval: evaluate simulation every nth iteration.\n",
    "        :param num_workers: number of workers for data loading (default system spec).\n",
    "        :return: path of the final save state.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        self.log_interval = log_interval\n",
    "        num_workers = os.cpu_count() if num_workers is None else num_workers\n",
    "\n",
    "        # model = self.parallel_model\n",
    "        opt = self.opt\n",
    "        # history = self.history\n",
    "        device = self.device\n",
    "\n",
    "        # init\n",
    "        sampler = RandomCoordinateSampler(self.cube_shape, self.spatial_norm, co_batch_size)\n",
    "        self.scheduler = ExponentialLR(opt, gamma=(5e-5 / 5e-4) ** (1 / lr_decay_iterations))\n",
    "        # iterations = total_iterations - self.start_iteration\n",
    "        self.total_iterations = total_iterations\n",
    "        iterations = self.total_iterations - self.start_iteration\n",
    "        if iterations <= 0:\n",
    "            print('Training already finished!')\n",
    "            return\n",
    "\n",
    "        # init loader\n",
    "        data_loader, batches_path = self._init_loader(bc_batch_size, self.data, num_workers, iterations)\n",
    "\n",
    "        losses = []\n",
    "        losses_no_weight = []\n",
    "\n",
    "        # total_loss_bc = []\n",
    "        # total_loss_div = []\n",
    "        # total_loss_ff = []\n",
    "        self.model.train()\n",
    "        for iter, (boundary_coords, b_true) in tqdm(enumerate(data_loader, start=self.start_iteration),\n",
    "                                                           total=len(data_loader), desc='Training'):\n",
    "            opt.zero_grad()\n",
    "            # load input data\n",
    "            boundary_coords, b_true = boundary_coords.to(device), b_true.to(device)\n",
    "            random_coords = sampler.load_sample()\n",
    "\n",
    "            # concatenate boundary and random points\n",
    "            n_boundary_coords = boundary_coords.shape[0]\n",
    "            coords = torch.cat([boundary_coords, random_coords], 0)\n",
    "            coords.requires_grad = True\n",
    "\n",
    "            # forward step\n",
    "            b = self.model(coords)\n",
    "\n",
    "            # compute boundary loss\n",
    "            boundary_b = b[:n_boundary_coords]\n",
    "            loss_bc = torch.clip(torch.abs(boundary_b - b_true), 0)\n",
    "            self.loss_bc = torch.mean(loss_bc.pow(2).sum(-1))\n",
    "\n",
    "            # compute div and ff loss\n",
    "            loss_div, loss_ff = calculate_loss(b, coords)\n",
    "            self.loss_div, self.loss_ff = loss_div.mean(), loss_ff.mean()\n",
    "\n",
    "            # reset grad from auto-gradient operation\n",
    "            opt.zero_grad()\n",
    "            # compute loss\n",
    "            self.loss = (self.loss_bc * self.w_bc + \n",
    "                         self.loss_div * self.w_div + \n",
    "                         self.loss_ff * self.w_ff)\n",
    "            losses.append(self.loss.detach().cpu().numpy())\n",
    "            loss_no_weight = self.loss_bc + self.loss_ff + self.loss_div\n",
    "            losses_no_weight.append(loss_no_weight.detach().cpu().numpy())\n",
    "            \n",
    "            if iter == 0:\n",
    "                self.print_log(iter)\n",
    "                self.save(iter)\n",
    "\n",
    "            self.loss.backward()\n",
    "            # update step\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
    "            opt.step()\n",
    "\n",
    "            # save loss information\n",
    "            # total_loss_bc += [loss_bc.detach().cpu().numpy()]\n",
    "            # total_loss_div += [loss_div.mean().detach().cpu().numpy()]\n",
    "            # total_loss_ff += [loss_ff.mean().detach().cpu().numpy()]\n",
    "\n",
    "            if (self.log_interval > 0 and (iter + 1) % self.log_interval == 0):\n",
    "                self.print_log(iter+1)\n",
    "                self.save(iter+1)\n",
    "                self.save_state_dict(iter+1, 'checkpoint.pt')\n",
    "\n",
    "            # update training parameters\n",
    "            if self.w_bc > 1:\n",
    "                self.w_bc *= self.w_bc_decay\n",
    "                if self.w_bc <= 1:\n",
    "                    self.w_bc = 1\n",
    "            if self.scheduler.get_last_lr()[0] > 5e-5:\n",
    "                self.scheduler.step()\n",
    "            # logging\n",
    "            # if log_interval > 0 and (iter + 1) % log_interval == 0:\n",
    "            #     # log loss\n",
    "            #     logging.info('[Iteration %06d/%06d] [B-Field: %.08f; Div: %.08f; For: %.08f] [%s]' %\n",
    "            #                  (iter + 1, iterations,\n",
    "            #                   np.mean(total_loss_bc),\n",
    "            #                   np.mean(total_loss_div),\n",
    "            #                   np.mean(total_loss_ff),\n",
    "            #                   datetime.now() - start_time))\n",
    "            #     # reset\n",
    "            #     total_loss_bc = []\n",
    "            #     total_loss_div = []\n",
    "            #     total_loss_ff = []\n",
    "\n",
    "            #     # plot sample\n",
    "            #     model.eval()\n",
    "            #     self.plot_sample(iter, batch_size=batch_size)\n",
    "            #     model.train()\n",
    "\n",
    "            #     # log decay parameters\n",
    "            #     logging.info('Lambda B: %f' % (self.w_bc))\n",
    "            #     logging.info('LR: %f' % (scheduler.get_last_lr()[0]))\n",
    "            # # validation\n",
    "            # if validation_interval > 0 and (iter + 1) % validation_interval == 0:\n",
    "            #     model.eval()\n",
    "            #     self.save(iter)\n",
    "            #     # validate and plot\n",
    "            #     mean_b, total_divergence, mean_force, sigma_angle = self.validate(self.height, batch_size)\n",
    "            #     logging.info('Validation [Cube: B: %.03f; Div: %.03f; For: %.03f; Sig: %.03f]' %\n",
    "            #                  (mean_b, total_divergence, mean_force, sigma_angle))\n",
    "            #     #\n",
    "            #     history['iteration'].append(iter + 1)\n",
    "            #     history['b_loss'].append(mean_b.mean())\n",
    "            #     history['loss_div'].append(total_divergence)\n",
    "            #     history['loss_ff'].append(mean_force)\n",
    "            #     history['sigma_angle'].append(sigma_angle)\n",
    "            #     self.plotHistory()\n",
    "            #     #\n",
    "            #     model.train()\n",
    "        # save final model state\n",
    "        # torch.save({'m': self.model.state_dict(),\n",
    "        #             'o': self.opt.state_dict(), },\n",
    "        #            os.path.join(self.base_path, 'final.pt'))\n",
    "        # torch.save({'model': self.model,\n",
    "        #             'cube_shape': self.cube_shape,\n",
    "        #             'b_norm': self.b_norm,\n",
    "        #             'spatial_norm': self.spatial_norm,\n",
    "        #             'meta_info': self.meta_info}, self.save_path)\n",
    "        # cleanup\n",
    "\n",
    "        np.save(os.path.join(self.base_path, 'losses.npy'), np.array(losses))\n",
    "        np.save(os.path.join(self.base_path, 'losses_no_weight.npy'), np.array(losses_no_weight))\n",
    "        self.print_log(self.total_iterations)\n",
    "        self.save(self.total_iterations)\n",
    "        self.save(self.total_iterations, 'model_final.pt')\n",
    "        self.save_state_dict(self.total_iterations, 'checkpoint_final.pt')\n",
    "        os.remove(batches_path)\n",
    "\n",
    "        # return self.save_path\n",
    "\n",
    "    def _init_loader(self, batch_size, data, num_workers, iterations):\n",
    "        # shuffle data\n",
    "        r = np.random.permutation(data.shape[0])\n",
    "        data = data[r]\n",
    "        # adjust to batch size\n",
    "        pad = batch_size - data.shape[0] % batch_size\n",
    "        data = np.concatenate([data, data[:pad]])\n",
    "        # split data into batches\n",
    "        n_batches = data.shape[0] // batch_size\n",
    "        batches = np.array(np.split(data, n_batches), dtype=np.float32)\n",
    "        # store batches to disk\n",
    "        batches_path = os.path.join(self.work_directory, 'batches.npy')\n",
    "        np.save(batches_path, batches)\n",
    "        # create data loaders\n",
    "        dataset = BoundaryDataset(batches_path)\n",
    "        # create loader\n",
    "        data_loader = DataLoader(dataset, batch_size=None, num_workers=num_workers, pin_memory=True,\n",
    "                                 sampler=RandomSampler(dataset, replacement=True, num_samples=iterations))\n",
    "        return data_loader, batches_path\n",
    "\n",
    "    # def save(self, iteration):\n",
    "    #     torch.save({'model': self.model,\n",
    "    #                 'cube_shape': self.cube_shape,\n",
    "    #                 'b_norm': self.b_norm,\n",
    "    #                 'spatial_norm': self.spatial_norm,\n",
    "    #                 'meta_info': self.meta_info}, self.save_path)\n",
    "    #     torch.save({'iteration': iteration + 1,\n",
    "    #                 'm': self.model.state_dict(),\n",
    "    #                 'o': self.opt.state_dict(),\n",
    "    #                 'history': self.history,\n",
    "    #                 'w_bc': self.w_bc},\n",
    "    #                self.checkpoint_path)\n",
    "        \n",
    "    def save(self, iteration, filename=None):\n",
    "        if filename is None:\n",
    "            filename = 'model_%06d.pt' % iteration\n",
    "\n",
    "        torch.save({'iteration': iteration,\n",
    "                    'model': self.model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': self.b_norm,\n",
    "                    'spatial_norm': self.spatial_norm,\n",
    "                    'loss_bc': self.loss_bc.detach().cpu().numpy(),\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'loss_div': self.loss_div.detach().cpu().numpy(),\n",
    "                    'w_div': self.w_div,\n",
    "                    'loss_ff': self.loss_ff.detach().cpu().numpy(),\n",
    "                    'w_ff': self.w_ff,\n",
    "                    'LR':self.scheduler.get_last_lr()[0]}, \n",
    "                    os.path.join(self.base_path, filename))\n",
    "        \n",
    "    def save_state_dict(self, iteration, filename):\n",
    "        torch.save({'iteration': iteration,\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'm': self.model.state_dict(),\n",
    "                    'o': self.opt.state_dict()},\n",
    "                    os.path.join(self.base_path, filename))\n",
    "\n",
    "    def print_log(self, iteration):\n",
    "        self.logger.info('[Iteration %06d/%06d] [loss: %.08f] [loss_bc: %.08f; loss_div: %.08f; loss_ff: %.08f] [w_bc: %f, LR: %f]' %\n",
    "                (iteration, self.total_iterations,\n",
    "                self.loss,\n",
    "                self.w_bc*self.loss_bc,\n",
    "                self.w_ff*self.loss_ff,\n",
    "                self.w_div*self.loss_div,\n",
    "                self.w_bc,\n",
    "                self.scheduler.get_last_lr()[0]))\n",
    "\n",
    "    def plot_sample(self, iteration, n_samples=10, batch_size=4096):\n",
    "        fig, axs = plt.subplots(3, n_samples, figsize=(n_samples * 4, 12))\n",
    "        heights = np.linspace(0, 1, n_samples) ** 2 * (self.height - 1)  # more samples from lower heights\n",
    "        imgs = np.array([self.get_image(h, batch_size) for h in heights])\n",
    "        for i in range(3):\n",
    "            for j in range(10):\n",
    "                v_min_max = np.max(np.abs(imgs[j, ..., i]))\n",
    "                axs[i, j].imshow(imgs[j, ..., i].transpose(), cmap='gray', vmin=-v_min_max, vmax=v_min_max,\n",
    "                                 origin='lower')\n",
    "                axs[i, j].set_axis_off()\n",
    "        for j, h in enumerate(heights):\n",
    "            axs[0, j].set_title('%.01f' % h)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(self.base_path, '%06d.jpg' % (iteration + 1)))\n",
    "        plt.close(fig)\n",
    "\n",
    "    def get_image(self, z=0, batch_size=4096):\n",
    "        image_loader = DataLoader(ImageDataset([*self.cube_shape, 3], self.spatial_norm, z),\n",
    "                                  batch_size=batch_size, shuffle=False)\n",
    "        image = []\n",
    "        for coord in image_loader:\n",
    "            coord.requires_grad = True\n",
    "            pred_pix = self.model(coord.to(self.device))\n",
    "            image.extend(pred_pix.detach().cpu().numpy())\n",
    "        image = np.array(image).reshape((*self.cube_shape[:2], 3))\n",
    "        return image\n",
    "\n",
    "    def validate(self, z, batch_size):\n",
    "        b, j, div, coords = self.get_cube(z, batch_size)\n",
    "        b = b.unsqueeze(0) * self.b_norm\n",
    "        j = j.unsqueeze(0) * self.b_norm / self.spatial_norm\n",
    "        div = div.unsqueeze(0) * self.b_norm / self.spatial_norm\n",
    "\n",
    "        norm = b.pow(2).sum(-1).pow(0.5) * j.pow(2).sum(-1).pow(0.5)\n",
    "        angle = torch.cross(j, b, dim=-1).pow(2).sum(-1).pow(0.5) / norm\n",
    "        sig = torch.asin(torch.clip(angle, -1. + 1e-7, 1. - 1e-7)) * (180 / np.pi)\n",
    "        sig = torch.abs(sig)\n",
    "        weighted_sig = np.average(sig.numpy(), weights=j.pow(2).sum(-1).pow(0.5).numpy())\n",
    "\n",
    "        loss_bc = torch.abs(b[0, :, :, 0, :] - self.b_bottom)\n",
    "        loss_bc = torch.clip(loss_bc, 0, None)\n",
    "        loss_bc = torch.sqrt((loss_bc ** 2).sum(-1))\n",
    "\n",
    "        b_norm = b.pow(2).sum(-1).pow(0.5) + 1e-7\n",
    "        div_loss = div / b_norm\n",
    "        for_loss = torch.cross(j, b, dim=-1).pow(2).sum(-1).pow(0.5) / b_norm\n",
    "\n",
    "        return loss_bc.mean().numpy(), torch.mean(div_loss).numpy(), \\\n",
    "               torch.mean(for_loss).numpy(), weighted_sig\n",
    "\n",
    "    def get_cube(self, max_height, batch_size=int(1e4)):\n",
    "        b = []\n",
    "        j = []\n",
    "        div = []\n",
    "\n",
    "        coords = np.stack(np.mgrid[:self.cube_shape[0], :self.cube_shape[1], :max_height], -1)\n",
    "        coords = torch.tensor(coords / self.spatial_norm, dtype=torch.float32)\n",
    "        coords_shape = coords.shape[:-1]\n",
    "        coords = coords.view((-1, 3))\n",
    "        for k in tqdm(range(int(np.ceil(coords.shape[0] / batch_size))), desc='Validation'):\n",
    "            coord = coords[k * batch_size: (k + 1) * batch_size]\n",
    "            coord.requires_grad = True\n",
    "            coord = coord.to(self.device)\n",
    "            b_batch = self.model(coord)\n",
    "\n",
    "            jac_matrix = jacobian(b_batch, coord)\n",
    "            dBx_dx = jac_matrix[:, 0, 0]\n",
    "            dBy_dx = jac_matrix[:, 1, 0]\n",
    "            dBz_dx = jac_matrix[:, 2, 0]\n",
    "            dBx_dy = jac_matrix[:, 0, 1]\n",
    "            dBy_dy = jac_matrix[:, 1, 1]\n",
    "            dBz_dy = jac_matrix[:, 2, 1]\n",
    "            dBx_dz = jac_matrix[:, 0, 2]\n",
    "            dBy_dz = jac_matrix[:, 1, 2]\n",
    "            dBz_dz = jac_matrix[:, 2, 2]\n",
    "            #\n",
    "            rot_x = dBz_dy - dBy_dz\n",
    "            rot_y = dBx_dz - dBz_dx\n",
    "            rot_z = dBy_dx - dBx_dy\n",
    "            #\n",
    "            j_batch = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "            div_batch = torch.abs(dBx_dx + dBy_dy + dBz_dz)\n",
    "            #\n",
    "            b += [b_batch.detach().cpu()]\n",
    "            j += [j_batch.detach().cpu()]\n",
    "            div += [div_batch.detach().cpu()]\n",
    "\n",
    "        b = torch.cat(b, dim=0).view((*coords_shape, 3))\n",
    "        j = torch.cat(j, dim=0).view((*coords_shape, 3))\n",
    "        div = torch.cat(div, dim=0).view(coords_shape)\n",
    "        return b, j, div, coords\n",
    "\n",
    "    def plotHistory(self):\n",
    "        history = self.history\n",
    "        plt.figure(figsize=(12, 16))\n",
    "        plt.subplot(411)\n",
    "        plt.plot(history['iteration'], history['b_loss'], label='B')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('B')\n",
    "        plt.subplot(412)\n",
    "        plt.plot(history['iteration'], history['loss_div'], label='Divergence')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Divergence')\n",
    "        plt.subplot(413)\n",
    "        plt.plot(history['iteration'], history['loss_ff'], label='Force')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Force')\n",
    "        plt.subplot(414)\n",
    "        plt.plot(history['iteration'], history['sigma_angle'], label='Angle')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Angle')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.base_path, 'history.jpg'))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def calculate_loss(b, coords):\n",
    "    jac_matrix = jacobian(b, coords)\n",
    "    dBx_dx = jac_matrix[:, 0, 0]\n",
    "    dBy_dx = jac_matrix[:, 1, 0]\n",
    "    dBz_dx = jac_matrix[:, 2, 0]\n",
    "    dBx_dy = jac_matrix[:, 0, 1]\n",
    "    dBy_dy = jac_matrix[:, 1, 1]\n",
    "    dBz_dy = jac_matrix[:, 2, 1]\n",
    "    dBx_dz = jac_matrix[:, 0, 2]\n",
    "    dBy_dz = jac_matrix[:, 1, 2]\n",
    "    dBz_dz = jac_matrix[:, 2, 2]\n",
    "    #\n",
    "    rot_x = dBz_dy - dBy_dz\n",
    "    rot_y = dBx_dz - dBz_dx\n",
    "    rot_z = dBy_dx - dBx_dy\n",
    "    #\n",
    "    j = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "    jxb = torch.cross(j, b, -1)\n",
    "    loss_ff = torch.sum(jxb ** 2, dim=-1) / (torch.sum(b ** 2, dim=-1) + 1e-7)\n",
    "    loss_div = (dBx_dx + dBy_dy + dBz_dz) ** 2\n",
    "    return loss_div, loss_ff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
