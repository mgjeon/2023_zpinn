{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b68fb47-c95d-4213-b91d-f6b13ba8c7b1",
   "metadata": {},
   "source": [
    "# PINN NF2 (2nd try)\n",
    "> https://github.com/RobertJaro/NF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b28a7f-3b88-477e-87e9-b010accfbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from zpinn.lowloumag import LowLouMag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3076354-5ba1-40a3-b62f-359f14c41826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = LowLouMag(resolutions=[128, 64, 200])\n",
    "b.calculate()\n",
    "Nx, Ny, _ =  b.grid.dimensions\n",
    "bottom_subset = (0, Nx-1, 0, Ny-1, 0, 0)\n",
    "bottom = b.grid.extract_subset(bottom_subset).extract_surface()\n",
    "b_bottom = bottom['B'].reshape(Nx, Ny, 3)\n",
    "b_bottom = np.array(b_bottom)\n",
    "b_bottom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7b15d-189a-4202-b9d8-c26305207f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda import get_device_name\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8634c1-b31a-4b98-8dfe-6372d6f8a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords(xbounds, ybounds, zbounds):\n",
    "    return np.stack(np.mgrid[xbounds[0]:xbounds[1]+1, ybounds[0]:ybounds[1]+1, zbounds[0]:zbounds[1]+1], axis=-1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862833ff-348d-49d4-ac4d-746bf18c312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, batches_path):\n",
    "        self.batches_path = batches_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.load(self.batches_path, mmap_mode='r').shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy load data\n",
    "        d = np.load(self.batches_path, mmap_mode='r')[idx]\n",
    "        d = np.copy(d)\n",
    "        coord, field = d[:, 0],  d[:, 1]\n",
    "        return coord, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43dec6-a6c4-4215-bc5d-a731bf104012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding of the input coordinates.\n",
    "\n",
    "    encodes x to (..., sin(2^k x), cos(2^k x), ...)\n",
    "    k takes \"num_freqs\" number of values equally spaced between [0, max_freq]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_freq, num_freqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_freq (int): maximum frequency in the positional encoding.\n",
    "            num_freqs (int): number of frequencies between [0, max_freq]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        freqs = 2 ** torch.linspace(0, max_freq, num_freqs)\n",
    "        self.register_buffer(\"freqs\", freqs)  # (num_freqs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: (batch, num_samples, in_features)\n",
    "        Outputs:\n",
    "            out: (batch, num_samples, 2*num_freqs*in_features)\n",
    "        \"\"\"\n",
    "        x_proj = x.unsqueeze(dim=-2) * self.freqs.unsqueeze(dim=-1)  # (num_rays, num_samples, num_freqs, in_features)\n",
    "        x_proj = x_proj.reshape(*x.shape[:-1], -1)  # (num_rays, num_samples, num_freqs*in_features)\n",
    "        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)],\n",
    "                        dim=-1)  # (num_rays, num_samples, 2*num_freqs*in_features)\n",
    "        return out\n",
    "\n",
    "class BModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_coords, out_values, dim, pos_encoding=False):\n",
    "        super().__init__()\n",
    "        if pos_encoding:\n",
    "            posenc = PositionalEncoding(8, 20)\n",
    "            d_in = nn.Linear(in_coords * 40, dim)\n",
    "            self.d_in = nn.Sequential(posenc, d_in)\n",
    "        else:\n",
    "            self.d_in = nn.Linear(in_coords, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, out_values)\n",
    "        self.activation = Sine()  # torch.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        x = self.d_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fe8c0-a48c-4877-848b-4d1dce63a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, b_n, r_p):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b_n', b_n)\n",
    "        self.register_buffer('r_p', r_p)\n",
    "        c = np.array([[0, 0, 1/np.sqrt(2*np.pi)]])\n",
    "        c = torch.tensor(c, dtype=torch.float64)\n",
    "        self.register_buffer('c', c)\n",
    "\n",
    "    def forward(self, r):\n",
    "        numerator = self.b_n[:, None]\n",
    "        denominator = torch.sqrt(torch.sum((r[None, :] - self.r_p[:, None] + self.c[None])**2, -1))\n",
    "        potential = torch.sum(numerator/denominator, 0) / (2*np.pi)\n",
    "        return potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5031b4-9736-407c-b3db-d7d31c8c6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bc_data(b_bottom, height, device, b_norm, spatial_norm):\n",
    "    Nx, Ny, _ = b_bottom.shape\n",
    "    Nz = height\n",
    "\n",
    "    bottom_values = b_bottom.reshape(-1, 3)\n",
    "    bottom_values = np.double(bottom_values)\n",
    "    bottom_coords = coords((0, Nx-1), (0, Ny-1), (0, 0)).reshape(-1, 3)\n",
    "    bottom_coords = np.double(bottom_coords)\n",
    "\n",
    "    top_lateral_coordinates = [coords((0, Nx-1), (0, Ny-1), (Nz-1, Nz-1)).reshape(-1, 3),\n",
    "                        coords((0, 0), (0, Ny-1), (0, Nz-1)).reshape(-1, 3),\n",
    "                        coords((Nx-1, Nx-1), (0, Ny-1), (0, Nz-1)).reshape(-1, 3),\n",
    "                        coords((0, Nx-1), (0, 0), (0, Nz-1)).reshape(-1, 3),\n",
    "                        coords((0, Nx-1), (Ny-1, Ny-1), (0, Nz-1)).reshape(-1, 3)]\n",
    "\n",
    "    b_n = torch.tensor(bottom_values[:, 2], dtype=torch.float64)\n",
    "    r_p = torch.tensor(bottom_coords, dtype=torch.float64)\n",
    "\n",
    "    model = nn.DataParallel(PotentialModel(b_n, r_p)).to(device)\n",
    "\n",
    "    pf_fields = []\n",
    "    pf_coords = []\n",
    "    for r_coords in top_lateral_coordinates:\n",
    "        r_coords = torch.tensor(r_coords, dtype=torch.float64)\n",
    "        pf_batch_size = int(np.prod(r_coords.shape[:-1]) // 10)\n",
    "\n",
    "        fields = []\n",
    "        for r, in tqdm(DataLoader(TensorDataset(r_coords), batch_size=pf_batch_size, num_workers=2),\n",
    "                            desc='Potential Boundary'):\n",
    "            r = r.to(device).requires_grad_(True)\n",
    "            p_batch = model(r)\n",
    "            b_p = -1 * torch.autograd.grad(p_batch, r, torch.ones_like(p_batch), retain_graph=True, create_graph=True)[0]\n",
    "            fields += [b_p.clone().detach().cpu().numpy()]\n",
    "        pf_fields += [np.concatenate(fields)]\n",
    "        pf_coords += [r_coords.clone().detach().cpu().numpy()]\n",
    "\n",
    "    top_lateral_values = np.concatenate(pf_fields) \n",
    "    top_lateral_coords = np.concatenate(pf_coords)\n",
    "\n",
    "    boundary_values = np.concatenate([top_lateral_values, bottom_values])\n",
    "    boundary_coords = np.concatenate([top_lateral_coords, bottom_coords])\n",
    "\n",
    "    normalized_boundary_values = boundary_values / b_norm\n",
    "    normalized_boundary_coords = boundary_coords / spatial_norm\n",
    "\n",
    "    boundary_data = np.stack([normalized_boundary_coords, normalized_boundary_values], 1)\n",
    "\n",
    "    return boundary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a848838-daed-4ccd-9e1b-c5e95368b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF2Trainer:\n",
    "\n",
    "    def __init__(self, base_path, b_bottom, height, spatial_norm, b_norm, meta_info=None, dim=256,\n",
    "                 positional_encoding=False, meta_path=None, use_potential_boundary=True, potential_strides=1, use_vector_potential=False,\n",
    "                 w_div=0.1, w_ff=0.1, decay_iterations=None,\n",
    "                 device=None, work_directory=None):\n",
    "        \n",
    "        # logging\n",
    "        self.log = logging.getLogger()\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        for hdlr in self.log.handlers[:]:  # remove all old handlers\n",
    "            self.log.removeHandler(hdlr)\n",
    "        self.log.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(base_path, \"info_log\")))  # set the new file handler\n",
    "        self.log.addHandler(logging.StreamHandler())  # set the new console handler\n",
    "        self.log.info('Configuration:')\n",
    "        self.log.info(\n",
    "            'dim: %d, w_div: %f, w_ff: %f, decay_iterations: %s, potential: %s, vector_potential: %s, ' % (\n",
    "                dim, w_div, w_ff, str(decay_iterations), str(use_potential_boundary),\n",
    "                str(use_vector_potential)))\n",
    "        \n",
    "        # path\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(base_path, 'checkpoint.pt')\n",
    "        work_directory = base_path if work_directory is None else work_directory\n",
    "        self.work_directory = work_directory\n",
    "\n",
    "        # info\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.height = height\n",
    "        self.b_norm = b_norm\n",
    "        self.meta_info = meta_info\n",
    "\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        device_names = [get_device_name(i) for i in range(n_gpus)]\n",
    "        if device is None:\n",
    "            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.log.info('Using device: %s (gpus %d) %s' % (str(device), n_gpus, str(device_names)))\n",
    "        self.device = device\n",
    "\n",
    "        # prepare data\n",
    "        self.b_bottom = b_bottom\n",
    "\n",
    "        # load dataset\n",
    "        self.data = prepare_bc_data(b_bottom, height, device, b_norm, spatial_norm)\n",
    "        # self.cube_shape = [*b_bottom.shape[:-1], height]\n",
    "\n",
    "        Nx, Ny, _ = b_bottom.shape\n",
    "        Nz = height\n",
    "\n",
    "        self.cube_shape = (Nx, Ny, Nz)\n",
    "\n",
    "        # init model\n",
    "        if use_vector_potential:\n",
    "            model = VectorPotentialModel(3, dim, pos_encoding=positional_encoding)\n",
    "        else:\n",
    "            model = BModel(3, 3, dim, pos_encoding=positional_encoding)\n",
    "        parallel_model = nn.DataParallel(model)\n",
    "        parallel_model.to(device)\n",
    "        opt = torch.optim.Adam(parallel_model.parameters(), lr=5e-4)\n",
    "        self.model = model\n",
    "        self.parallel_model = parallel_model\n",
    "\n",
    "        # load last state\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            state_dict = torch.load(self.checkpoint_path, map_location=device)\n",
    "            start_iteration = state_dict['iteration']\n",
    "            model.load_state_dict(state_dict['m'])\n",
    "            opt.load_state_dict(state_dict['o'])\n",
    "            history = state_dict['history']\n",
    "            w_bc = state_dict['w_bc']\n",
    "            self.log.info('Resuming training from iteration %d' % start_iteration)\n",
    "        else:\n",
    "            if meta_path:\n",
    "                state_dict = torch.load(meta_path, map_location=device)['model'].state_dict() \\\n",
    "                    if meta_path.endswith('nf2') else torch.load(meta_path, map_location=device)['m']\n",
    "                model.load_state_dict(state_dict)\n",
    "                opt = torch.optim.Adam(parallel_model.parameters(), lr=5e-5)\n",
    "                self.log.info('Loaded meta state: %s' % meta_path)\n",
    "            # init\n",
    "            start_iteration = 0\n",
    "            w_bc = 1000 if decay_iterations else 1\n",
    "            history = {'iteration': [], 'height': [],\n",
    "                       'b_loss': [], 'divergence_loss': [], 'force_loss': [], 'sigma_angle': []}\n",
    "\n",
    "        self.opt = opt\n",
    "        self.start_iteration = start_iteration\n",
    "        self.history = history\n",
    "        self.w_bc = w_bc\n",
    "        self.w_bc_decay = (1 / 1000) ** (1 / decay_iterations) if decay_iterations is not None else 1\n",
    "        self.w_div, self.w_ff = w_div, w_ff\n",
    "\n",
    "        collocation_coords = coords((0, Nx-1), (0, Ny-1), (0, Nz-1)).reshape(-1, 3)\n",
    "        normalized_collocation_coords = collocation_coords / self.spatial_norm\n",
    "        self.normalized_collocation_coords = torch.tensor(normalized_collocation_coords)\n",
    "\n",
    "    def train(self, total_iterations, batch_size, log_interval=100, validation_interval=100, num_workers=None):\n",
    "        \"\"\"Start magnetic field extrapolation fit.\n",
    "\n",
    "        :param total_iterations: number of iterations for training.\n",
    "        :param batch_size: number of samples per iteration.\n",
    "        :param log_interval: log training details every nth iteration.\n",
    "        :param validation_interval: evaluate simulation every nth iteration.\n",
    "        :param num_workers: number of workers for data loading (default system spec).\n",
    "        :return: path of the final save state.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        num_workers = os.cpu_count() if num_workers is None else num_workers\n",
    "\n",
    "        model = self.parallel_model\n",
    "        opt = self.opt\n",
    "        device = self.device\n",
    "        w_div, w_ff = self.w_div, self.w_ff\n",
    "\n",
    "        # init\n",
    "        scheduler = ExponentialLR(opt, gamma=(5e-5 / 5e-4) ** (1 / total_iterations))\n",
    "        iterations = total_iterations - self.start_iteration\n",
    "        if iterations <= 0:\n",
    "            self.log.info('Training already finished!')\n",
    "            return self.save_path\n",
    "\n",
    "        # init loader\n",
    "        data_loader, batches_path = self._init_loader(batch_size, self.data, num_workers, iterations)\n",
    "        \n",
    "        model.train()\n",
    "        for iter, (boundary_coords, boundary_b) in tqdm(enumerate(data_loader, start=self.start_iteration),\n",
    "                                                           total=len(data_loader), desc='Training'):\n",
    "\n",
    "            boundary_coords, boundary_b= boundary_coords.to(device), boundary_b.to(device)\n",
    "\n",
    "            perm = torch.randperm(self.normalized_collocation_coords.shape[0])\n",
    "            idx = perm[:batch_size]\n",
    "            co_coords = self.normalized_collocation_coords[idx].to(device)\n",
    "\n",
    "            # concatenate boundary and random points\n",
    "            # n_boundary_coords = boundary_coords.shape[0]\n",
    "            # r = torch.cat([boundary_coords, co_coords], 0)\n",
    "            r = co_coords\n",
    "            r.requires_grad = True\n",
    "\n",
    "            # forward step\n",
    "            B = model(r)\n",
    "\n",
    "            # if iter == 0:\n",
    "            #     model.eval()\n",
    "            #     torch.save({'model': self.model,\n",
    "            #         'cube_shape': self.cube_shape,\n",
    "            #         'b_norm': self.b_norm,\n",
    "            #         'spatial_norm': self.spatial_norm,\n",
    "            #         'meta_info': self.meta_info}, os.path.join(self.base_path, 'fields_%06d.nf2' % iter))\n",
    "            #     self.plot_sample(iter-1, batch_size=batch_size)\n",
    "            #     model.train()\n",
    "\n",
    "            # compute boundary loss\n",
    "            # boundary_B = B[:n_boundary_coords]\n",
    "            boundary_B = model(boundary_coords)\n",
    "            # bc_loss = torch.abs(boundary_B - boundary_b)\n",
    "            # bc_loss = torch.mean(bc_loss.pow(2).sum(-1))\n",
    "\n",
    "            bc_loss = torch.sum((boundary_B - boundary_b)**2, dim=-1)\n",
    "            bc_loss = torch.mean(bc_loss)\n",
    "            # compute div and ff loss\n",
    "            # divergence_loss, force_free_loss = calculate_loss(b, coords)\n",
    "\n",
    "            dBx_dr = torch.autograd.grad(B[:, 0], r, torch.ones_like(B[:, 0]), retain_graph=True, create_graph=True)[0]\n",
    "            dBy_dr = torch.autograd.grad(B[:, 1], r, torch.ones_like(B[:, 1]), retain_graph=True, create_graph=True)[0]\n",
    "            dBz_dr = torch.autograd.grad(B[:, 2], r, torch.ones_like(B[:, 2]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "            dBx_dx = dBx_dr[:, 0]\n",
    "            dBx_dy = dBx_dr[:, 1]\n",
    "            dBx_dz = dBx_dr[:, 2]\n",
    "\n",
    "            dBy_dx = dBy_dr[:, 0]\n",
    "            dBy_dy = dBy_dr[:, 1]\n",
    "            dBy_dz = dBy_dr[:, 2]\n",
    "\n",
    "            dBz_dx = dBz_dr[:, 0]\n",
    "            dBz_dy = dBz_dr[:, 1]\n",
    "            dBz_dz = dBz_dr[:, 2]\n",
    "\n",
    "            rot_x = dBz_dy - dBy_dz\n",
    "            rot_y = dBx_dz - dBz_dx\n",
    "            rot_z = dBy_dx - dBx_dy\n",
    "\n",
    "            J = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "            JxB = torch.cross(J, B, dim=-1)\n",
    "\n",
    "            divB = dBx_dx + dBy_dy + dBz_dz\n",
    "\n",
    "            force_free_loss = torch.sum(JxB**2, dim=-1) / (torch.sum(B**2, dim=-1) + 1e-7)\n",
    "            force_free_loss = torch.mean(force_free_loss)\n",
    "            divergence_loss = torch.sum((divB)**2, dim=-1)\n",
    "            divergence_loss = torch.mean(divergence_loss)\n",
    "\n",
    "            loss = self.w_bc*bc_loss + w_ff*force_free_loss + w_div*divergence_loss\n",
    "\n",
    "            if iter == 0:\n",
    "                self.log.info('[Iteration %06d/%06d] [loss: %.08f] [bc_loss: %.08f; div_loss: %.08f; ff_loss: %.08f] [w_bc: %f, LR: %f] [%s]' %\n",
    "                        (iter + 1, total_iterations,\n",
    "                        loss,\n",
    "                        self.w_bc*bc_loss,\n",
    "                        w_ff*force_free_loss,\n",
    "                        w_div*divergence_loss,\n",
    "                        self.w_bc,\n",
    "                        scheduler.get_last_lr()[0],\n",
    "                        datetime.now() - start_time))\n",
    "                \n",
    "                torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'divergence_loss': divergence_loss.mean().detach().cpu().numpy(),\n",
    "                    'w_div': w_div,\n",
    "                    'force_loss': force_free_loss.mean().detach().cpu().numpy(),\n",
    "                    'w_ff': w_ff,}, os.path.join(self.base_path, 'loss_%06d.nf2' % iter))\n",
    "                torch.save({'model': self.model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': self.b_norm,\n",
    "                    'spatial_norm': self.spatial_norm,\n",
    "                    'meta_info': self.meta_info}, os.path.join(self.base_path, 'fields_%06d.nf2' % iter))\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            opt.step()\n",
    "\n",
    "            if (log_interval > 0 and (iter + 1) % log_interval == 0):\n",
    "                # log loss\n",
    "                self.log.info('[Iteration %06d/%06d] [loss: %.08f] [bc_loss: %.08f; div_loss: %.08f; ff_loss: %.08f] [w_bc: %f, LR: %f] [%s]' %\n",
    "                        (iter + 1, total_iterations,\n",
    "                        loss,\n",
    "                        self.w_bc*bc_loss,\n",
    "                        w_ff*force_free_loss,\n",
    "                        w_div*divergence_loss,\n",
    "                        self.w_bc,\n",
    "                        scheduler.get_last_lr()[0],\n",
    "                        datetime.now() - start_time))\n",
    "                \n",
    "                torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                            'lambda_BC': self.w_bc,\n",
    "                            'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "                            'lambda_div': w_div,\n",
    "                            'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "                            'lambda_ff': w_ff,\n",
    "                            'LR':scheduler.get_last_lr()[0]}, \n",
    "                            os.path.join(base_path, 'loss_%06d.nf2' % iter))\n",
    "                torch.save({'model': model,\n",
    "                            'cube_shape': self.cube_shape,\n",
    "                            'b_norm': b_norm,\n",
    "                            'spatial_norm': spatial_norm,\n",
    "                            'meta_info': meta_info}, \n",
    "                            os.path.join(base_path, 'fields_%06d.nf2' % iter))\n",
    "\n",
    "            # update training parameters\n",
    "            if self.w_bc > 1:\n",
    "                self.w_bc *= self.w_bc_decay\n",
    "                if self.w_bc <= 1:\n",
    "                    self.w_bc = 1\n",
    "            if scheduler.get_last_lr()[0] > 5e-5:\n",
    "                scheduler.step()\n",
    "\n",
    "        # save final model state\n",
    "        torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "                    'w_div': w_div,\n",
    "                    'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "                    'w_ff': w_ff,\n",
    "                    'LR':scheduler.get_last_lr()[0]}, \n",
    "                    os.path.join(base_path, 'loss_final.nf2'))\n",
    "        torch.save({'model': model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': b_norm,\n",
    "                    'spatial_norm': spatial_norm,\n",
    "                    'meta_info': meta_info}, \n",
    "                    os.path.join(base_path, 'fields_final.nf2'))\n",
    "        torch.save({'m': model.state_dict(),\n",
    "                    'o': opt.state_dict(), },\n",
    "                    os.path.join(base_path, 'model_final.pt'))\n",
    "        # cleanup\n",
    "        os.remove(batches_path)    \n",
    "\n",
    "    def _init_loader(self, batch_size, data, num_workers, iterations):\n",
    "        # shuffle data\n",
    "        r = np.random.permutation(data.shape[0])\n",
    "        data = data[r]\n",
    "        # adjust to batch size\n",
    "        pad = batch_size - data.shape[0] % batch_size\n",
    "        data = np.concatenate([data, data[:pad]])\n",
    "        # split data into batches\n",
    "        n_batches = data.shape[0] // batch_size\n",
    "        batches = np.array(np.split(data, n_batches), dtype=np.float32)\n",
    "        # store batches to disk\n",
    "        batches_path = os.path.join(self.work_directory, 'batches.npy')\n",
    "        np.save(batches_path, batches)\n",
    "        # create data loaders\n",
    "        dataset = BoundaryDataset(batches_path)\n",
    "        # create loader\n",
    "        data_loader = DataLoader(dataset, batch_size=None, num_workers=num_workers, pin_memory=True,\n",
    "                                 sampler=RandomSampler(dataset, replacement=True, num_samples=iterations))\n",
    "        return data_loader, batches_path\n",
    "\n",
    "    def save(self, iteration):\n",
    "        torch.save({'model': self.model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': self.b_norm,\n",
    "                    'spatial_norm': self.spatial_norm,\n",
    "                    'meta_info': self.meta_info}, self.save_path)\n",
    "        torch.save({'iteration': iteration + 1,\n",
    "                    'm': self.model.state_dict(),\n",
    "                    'o': self.opt.state_dict(),\n",
    "                    'history': self.history,\n",
    "                    'lambda_B': self.lambda_B},\n",
    "                   self.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1de48-5c99-471f-b65b-367f14379372",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config_run.json'\n",
    "\n",
    "with open(config_path) as config:\n",
    "    info = json.load(config)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= info['simul']['gpu_id']\n",
    "\n",
    "n = info['exact']['n']\n",
    "m = info['exact']['m']\n",
    "l = info['exact']['l']\n",
    "psi = eval(info['exact']['psi'])\n",
    "resolution = info['exact']['resolution']\n",
    "bounds = info['exact']['bounds']\n",
    "\n",
    "\n",
    "base_path = os.path.join(info['simul']['base_path'], \"run/\")\n",
    "meta_path = info['simul']['meta_path']\n",
    "\n",
    "bin = info['simul']['bin']\n",
    "\n",
    "height = info['simul']['height']\n",
    "spatial_norm = info['simul']['spatial_norm']\n",
    "b_norm = info['simul']['b_norm']\n",
    "\n",
    "meta_info = info['simul']['meta_info']\n",
    "dim = info['simul']['dim']\n",
    "positional_encoding = info['simul']['positional_encoding']\n",
    "use_potential_boundary = info['simul']['use_potential_boundary']\n",
    "potential_strides = info['simul']['potential_strides']\n",
    "use_vector_potential = info['simul']['use_vector_potential']\n",
    "w_div = info['simul']['w_div']\n",
    "w_ff = info['simul']['w_ff']\n",
    "decay_iterations = info['simul']['decay_iterations']\n",
    "device = info['simul']['device']\n",
    "work_directory = info['simul']['work_directory']\n",
    "\n",
    "total_iterations = info['simul']['total_iterations']\n",
    "batch_size = info['simul']['batch_size']\n",
    "log_interval = info['simul']['log_interval']\n",
    "validation_interval = info['simul']['validation_interval']\n",
    "num_workers = info['simul']['num_workers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2af7e9-91ef-430d-b4d1-1b5f00bab071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "dim: 256, w_div: 0.100000, w_ff: 0.100000, decay_iterations: 25000, potential: True, vector_potential: False, \n",
      "Using device: cuda (gpus 1) ['NVIDIA GeForce RTX 3060']\n",
      "Potential Boundary: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 11.26it/s]\n",
      "Potential Boundary: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 80.83it/s]\n",
      "Potential Boundary: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 85.27it/s]\n",
      "Potential Boundary: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 13.16it/s]\n",
      "Potential Boundary: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 13.05it/s]\n",
      "Training:   0%|                                                                                                                                                                                                                      | 0/50000 [00:00<?, ?it/s][Iteration 000001/050000] [loss: 48.48286438] [bc_loss: 48.48164749; div_loss: 0.00000064; ff_loss: 0.00121772] [w_bc: 1000.000000, LR: 0.000500] [0:00:00.204280]\n",
      "Training:   0%|▏                                                                                                                                                                                                          | 57/50000 [00:06<1:28:43,  9.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(base_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m NF2Trainer(base_path, b_bottom, height, spatial_norm, b_norm, \n\u001b[1;32m      4\u001b[0m                      meta_info\u001b[38;5;241m=\u001b[39mmeta_info, dim\u001b[38;5;241m=\u001b[39mdim, positional_encoding\u001b[38;5;241m=\u001b[39mpositional_encoding, \n\u001b[1;32m      5\u001b[0m                      meta_path\u001b[38;5;241m=\u001b[39mmeta_path, use_potential_boundary\u001b[38;5;241m=\u001b[39muse_potential_boundary, \n\u001b[1;32m      6\u001b[0m                      potential_strides\u001b[38;5;241m=\u001b[39mpotential_strides, use_vector_potential\u001b[38;5;241m=\u001b[39muse_vector_potential,\n\u001b[1;32m      7\u001b[0m                      w_div\u001b[38;5;241m=\u001b[39mw_div, w_ff\u001b[38;5;241m=\u001b[39mw_ff, decay_iterations\u001b[38;5;241m=\u001b[39mdecay_iterations,\n\u001b[1;32m      8\u001b[0m                      device\u001b[38;5;241m=\u001b[39mdevice, work_directory\u001b[38;5;241m=\u001b[39mwork_directory)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 222\u001b[0m, in \u001b[0;36mNF2Trainer.train\u001b[0;34m(self, total_iterations, batch_size, log_interval, validation_interval, num_workers)\u001b[0m\n\u001b[1;32m    220\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    221\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 222\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (log_interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28miter\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# log loss\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/zpinn/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "trainer = NF2Trainer(base_path, b_bottom, height, spatial_norm, b_norm, \n",
    "                     meta_info=meta_info, dim=dim, positional_encoding=positional_encoding, \n",
    "                     meta_path=meta_path, use_potential_boundary=use_potential_boundary, \n",
    "                     potential_strides=potential_strides, use_vector_potential=use_vector_potential,\n",
    "                     w_div=w_div, w_ff=w_ff, decay_iterations=decay_iterations,\n",
    "                     device=device, work_directory=work_directory)\n",
    "\n",
    "trainer.train(total_iterations, batch_size, \n",
    "              log_interval=log_interval, validation_interval=validation_interval, \n",
    "              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb742f0b-b705-4ede-b9ca-058a2bc8d461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
