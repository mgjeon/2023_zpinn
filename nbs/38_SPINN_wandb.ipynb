{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"SPINN\")\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import jvp\n",
    "import optax\n",
    "from flax import linen as nn \n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "from typing import Sequence\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "\n",
    "import random\n",
    "\n",
    "import pyvista as pv\n",
    "import pickle\n",
    "from cmspinn.mag_viz import create_coordinates\n",
    "\n",
    "from cmspinn.potential_field import cal_and_save_potential_boundary_for_spinn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "from cmspinn.evaluation import magnetic_energy, divergence, magnitude, curl, laplacian_vector\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp_fwdfwd(f, primals, tangents, return_primals=False):\n",
    "    g = lambda primals: jvp(f, (primals,), tangents)[1]\n",
    "    primals_out, tangents_out = jvp(g, primals, tangents)\n",
    "    if return_primals:\n",
    "        return primals_out, tangents_out\n",
    "    else:\n",
    "        return tangents_out\n",
    "    \n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def update_model(optim, gradient, params, state):\n",
    "    updates, state = optim.update(gradient, state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, state\n",
    "\n",
    "def curlx(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_x = uz_y - uy_z\n",
    "    vec_z = jnp.ones(z.shape)\n",
    "    vec_y = jnp.ones(y.shape)\n",
    "    uy_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec_z,))[1]\n",
    "    uz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec_y,))[1]\n",
    "    wx = uz_y - uy_z\n",
    "    return wx\n",
    "\n",
    "\n",
    "def curly(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_y = ux_z - uz_x\n",
    "    vec_z = jnp.ones(z.shape)\n",
    "    vec_x = jnp.ones(x.shape)\n",
    "    ux_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec_z,))[1]\n",
    "    uz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec_x,))[1]\n",
    "    wy = ux_z - uz_x\n",
    "    return wy\n",
    "\n",
    "def curlz(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_z = uy_x - ux_y\n",
    "    vec_y = jnp.ones(y.shape)\n",
    "    vec_x = jnp.ones(x.shape)\n",
    "    ux_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec_y,))[1]\n",
    "    uy_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec_x,))[1]\n",
    "    wz = uy_x - ux_y\n",
    "    return wz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPINN3d(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    r: int\n",
    "    out_dim: int\n",
    "    pos_enc: int\n",
    "    mlp: str\n",
    "    activation: str\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, z):\n",
    "        '''\n",
    "        inputs: input factorized coordinates\n",
    "        outputs: feature output of each body network\n",
    "        xy: intermediate tensor for feature merge btw. x and y axis\n",
    "        pred: final model prediction (e.g. for 2d output, pred=[u, v])\n",
    "        '''\n",
    "        if self.pos_enc != 0:\n",
    "            # positional encoding only to spatial coordinates\n",
    "            freq = jnp.expand_dims(jnp.arange(1, self.pos_enc+1, 1), 0)\n",
    "            x = jnp.concatenate((jnp.ones((x.shape[0], 1)), jnp.sin(x@freq), jnp.cos(x@freq)), 1)\n",
    "            y = jnp.concatenate((jnp.ones((y.shape[0], 1)), jnp.sin(y@freq), jnp.cos(y@freq)), 1)\n",
    "            z = jnp.concatenate((jnp.ones((z.shape[0], 1)), jnp.sin(z@freq), jnp.cos(z@freq)), 1)\n",
    "\n",
    "            # causal PINN version (also on time axis)\n",
    "            #  freq_x = jnp.expand_dims(jnp.power(10.0, jnp.arange(0, 3)), 0)\n",
    "            # x = x@freq_x\n",
    "            \n",
    "        inputs, outputs, xy, pred = [x, y, z], [], [], []\n",
    "        init = nn.initializers.glorot_normal()\n",
    "\n",
    "        if self.mlp == 'mlp':\n",
    "            for X in inputs:\n",
    "                for fs in self.features[:-1]:\n",
    "                    X = nn.Dense(fs, kernel_init=init)(X)\n",
    "                    if self.activation == \"tanh\":\n",
    "                        X = nn.activation.tanh(X)\n",
    "                    elif self.activation == \"sin\":\n",
    "                        X = jnp.sin(X)\n",
    "                X = nn.Dense(self.r*self.out_dim, kernel_init=init)(X)\n",
    "                outputs += [jnp.transpose(X, (1, 0))]\n",
    "\n",
    "        elif self.mlp == 'modified_mlp':\n",
    "            for X in inputs:\n",
    "                if self.activation == \"tanh\":\n",
    "                    U = nn.activation.tanh(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                    V = nn.activation.tanh(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                    H = nn.activation.tanh(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                elif self.activation == \"sin\":\n",
    "                    U = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                    V = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                    H = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                for fs in self.features[:-1]:\n",
    "                    Z = nn.Dense(fs, kernel_init=init)(H)\n",
    "                    if self.activation == \"tanh\":\n",
    "                        Z = nn.activation.tanh(Z)\n",
    "                    elif self.activation == \"sin\":\n",
    "                        Z = jnp.sin(Z)\n",
    "                    H = (jnp.ones_like(Z)-Z)*U + Z*V\n",
    "                H = nn.Dense(self.r*self.out_dim, kernel_init=init)(H)\n",
    "                outputs += [jnp.transpose(H, (1, 0))]\n",
    "        \n",
    "        for i in range(self.out_dim):\n",
    "            xy += [jnp.einsum('fx, fy->fxy', outputs[0][self.r*i:self.r*(i+1)], outputs[1][self.r*i:self.r*(i+1)])]\n",
    "            pred += [jnp.einsum('fxy, fz->xyz', xy[i], outputs[-1][self.r*i:self.r*(i+1)])]\n",
    "\n",
    "        if len(pred) == 1:\n",
    "            # 1-dimensional output\n",
    "            return pred[0]\n",
    "        else:\n",
    "            # n-dimensional output\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 2, 3, 4, 5))\n",
    "def generate_train_data(nx, ny, nz, n_max_x, n_max_y, n_max_z):\n",
    "\n",
    "    xc = jnp.linspace(0, n_max_x, nx).reshape(-1, 1)\n",
    "    yc = jnp.linspace(0, n_max_y, ny).reshape(-1, 1)\n",
    "    zc = jnp.linspace(0, n_max_z, nz).reshape(-1, 1)\n",
    "\n",
    "    # # boundary points\n",
    "    xb = [jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=0   bottom\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=2   top\n",
    "          jnp.array([[0.]]),                     # x=0   lateral_1\n",
    "          jnp.array([[n_max_x]]),                     # x=2   lateral_2\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # y=0   lateral_3\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1)] # y=2   lateral_4\n",
    "\n",
    "    yb = [jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_y]])]\n",
    "\n",
    "    zb = [jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_z]]), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1)]\n",
    "\n",
    "    return xc, yc, zc, xb, yb, zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
    "def generate_train_data_random(key, nx, ny, nz, n_max_x, n_max_y, n_max_z, nc, ncx=None, ncy=None, ncz=None, choice=False):\n",
    "    \n",
    "    keys = jax.random.split(key, 4)\n",
    "\n",
    "    if (ncx is not None) and (ncy is not None) and (ncz is not None):\n",
    "      xc = jax.random.uniform(keys[1], (ncx, 1), minval=0., maxval=n_max_x)\n",
    "      yc = jax.random.uniform(keys[2], (ncy, 1), minval=0., maxval=n_max_x)\n",
    "      zc = jax.random.uniform(keys[3], (ncz, 1), minval=0., maxval=n_max_x)\n",
    "    \n",
    "    elif choice is False:\n",
    "      xc = jax.random.uniform(keys[1], (nc, 1), minval=0., maxval=n_max_x)\n",
    "      yc = jax.random.uniform(keys[2], (nc, 1), minval=0., maxval=n_max_x)\n",
    "      zc = jax.random.uniform(keys[3], (nc, 1), minval=0., maxval=n_max_x)\n",
    "    else:\n",
    "      xc = jnp.linspace(0, n_max_x, nx).reshape(-1, 1)\n",
    "      yc = jnp.linspace(0, n_max_y, ny).reshape(-1, 1)\n",
    "      zc = jnp.linspace(0, n_max_z, nz).reshape(-1, 1)\n",
    "      xc = jax.random.choice(keys[1], xc, shape=(nc,))\n",
    "      yc = jax.random.choice(keys[2], yc, shape=(nc,))\n",
    "      zc = jax.random.choice(keys[3], zc, shape=(nc,))\n",
    "\n",
    "    # boundary points\n",
    "    xb = [jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=0   bottom\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=2   top\n",
    "          jnp.array([[0.]]),                     # x=0   lateral_1\n",
    "          jnp.array([[n_max_x]]),                     # x=2   lateral_2\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # y=0   lateral_3\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1)] # y=2   lateral_4\n",
    "\n",
    "    yb = [jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_y]])]\n",
    "\n",
    "    zb = [jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_z]]), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1)]\n",
    "\n",
    "    return xc, yc, zc, xb, yb, zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def apply_model_spinn(apply_fn, params, train_boundary_data, w_ff, w_div, w_bc):\n",
    "    def residual_loss(params, x, y, z, w_ff, w_div):\n",
    "        # calculate u\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)\n",
    "        \n",
    "        # calculate J\n",
    "        Jx = curlx(apply_fn, params, x, y, z)\n",
    "        Jy = curly(apply_fn, params, x, y, z)\n",
    "        Jz = curlz(apply_fn, params, x, y, z)\n",
    "        J = jnp.stack([Jx, Jy, Jz], axis=-1)\n",
    "\n",
    "        JxB = jnp.cross(J, B, axis=-1) \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # loss_ff = jnp.sum(JxB**2, axis=-1)\n",
    "        # loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        loss_ff = jnp.sum(JxB**2, axis=-1) / (jnp.sum(B**2, axis=-1) + 1e-7)\n",
    "        loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        # loss_ff = jnp.mean(JxB**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # assumes x, y, z have same shape (very important)\n",
    "        vec_x = jnp.ones(x.shape)\n",
    "        vec_y = jnp.ones(y.shape)\n",
    "        vec_z = jnp.ones(z.shape)\n",
    "        \n",
    "        Bx_x = jvp(lambda x: apply_fn(params, x, y, z)[0], (x,), (vec_x,))[1]\n",
    "        # Bx_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec,))[1]\n",
    "        # Bx_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec,))[1]\n",
    "\n",
    "        # By_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec,))[1]\n",
    "        By_y = jvp(lambda y: apply_fn(params, x, y, z)[1], (y,), (vec_y,))[1]\n",
    "        # By_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec,))[1]\n",
    "\n",
    "        # Bz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec,))[1]\n",
    "        # Bz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec,))[1]\n",
    "        Bz_z = jvp(lambda z: apply_fn(params, x, y, z)[2], (z,), (vec_z,))[1]\n",
    "\n",
    "        divB = Bx_x + By_y + Bz_z\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # loss_div = jnp.sum((divB)**2, axis=-1)\n",
    "        # loss_div = jnp.mean(loss_div)\n",
    "\n",
    "        loss_div = jnp.mean((divB)**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        loss = w_ff*loss_ff + w_div*loss_div\n",
    "\n",
    "        return loss, loss_ff, loss_div\n",
    "\n",
    "    def boundary_loss(params, x, y, z, *boundary_data):\n",
    "        \n",
    "        # loss = 0.\n",
    "        # for i in np.arange(4):\n",
    "        #     boundary_data_batched = boundary_batches[i, :, :, :]\n",
    "        #     xb = boundary_data_batched[:, 0, :][:, 0].reshape(-1, 1)\n",
    "        #     yb = boundary_data_batched[:, 0, :][:, 1].reshape(-1, 1)\n",
    "        #     zb = boundary_data_batched[:, 0, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bx, By, Bz = apply_fn(params, xb, yb, zb)\n",
    "        #     # Bx, By, Bz = Bx.reshape(-1, 1), By.reshape(-1, 1), Bz.reshape(-1, 1)\n",
    "\n",
    "        #     Bxb = boundary_data_batched[:, 1, :][:, 0].reshape(-1, 1)\n",
    "        #     Byb = boundary_data_batched[:, 1, :][:, 1].reshape(-1, 1)\n",
    "        #     Bzb = boundary_data_batched[:, 1, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bxb_mesh, Byb_mesh, Bzb_mesh = jnp.meshgrid(Bxb.ravel(), Byb.ravel(), Bzb.ravel(), indexing='ij')\n",
    "            \n",
    "        #     loss += jnp.mean((Bx - Bxb_mesh)**2) + jnp.mean((By - Byb_mesh)**2) + jnp.mean((Bz - Bzb_mesh)**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  \n",
    "        #2 x=0   lateral_1            \n",
    "        #3 x=2   lateral_2            \n",
    "        #4 y=0   lateral_3            \n",
    "        #5 y=2   lateral_4            \n",
    "\n",
    "        b_bottom, bp_top, bp_lateral_1, bp_lateral_2, bp_lateral_3, bp_lateral_4 = boundary_data\n",
    "        \n",
    "        loss = 0.\n",
    "        Bx, By, Bz = apply_fn(params,  x[0], y[0], z[0])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - b_bottom[:, :, 0])**2) + jnp.mean((By - b_bottom[:, :, 1])**2) + jnp.mean((Bz - b_bottom[:, :, 2])**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  -> Only normal(Bz), Bx=0, By=0\n",
    "        #2 x=0   lateral_1            -> Only tangential(By, Bz), Bx=0\n",
    "        #3 x=2   lateral_2            -> Only tangential(By, Bz), Bx=0\n",
    "        #4 y=0   lateral_3            -> Only tangential(Bx, Bz), By=0\n",
    "        #5 y=2   lateral_4            -> Only tangential(Bx, Bz), By=0\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2) + jnp.mean(By**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2)\n",
    "        \n",
    "        # Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(By**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(By**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_top[:, :, 0])**2) + jnp.mean((By - bp_top[:, :, 1])**2) + jnp.mean((Bz - bp_top[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_1[:, :, 0])**2) + jnp.mean((By - bp_lateral_1[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_1[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_2[:, :, 0])**2) + jnp.mean((By - bp_lateral_2[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_2[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_3[:, :, 0])**2) + jnp.mean((By - bp_lateral_3[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_3[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_4[:, :, 0])**2) + jnp.mean((By - bp_lateral_4[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_4[:, :, 2])**2)\n",
    "\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # unpack data\n",
    "    train_data = train_boundary_data[0]\n",
    "    boundary_data = train_boundary_data[1]\n",
    "    xc, yc, zc, xb, yb, zb = train_data\n",
    "\n",
    "    # isolate loss func from redundant arguments\n",
    "    loss_pde, loss_ff, loss_div = residual_loss(params, xc, yc, zc, w_ff, w_div)\n",
    "    loss_bc = boundary_loss(params, xb, yb, zb, *boundary_data)\n",
    "\n",
    "    loss_fn = lambda params: residual_loss(params, xc, yc, zc, w_ff, w_div)[0] + w_bc*boundary_loss(params, xb, yb, zb, *boundary_data)\n",
    "\n",
    "    loss, gradient = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    return loss, gradient, [loss_ff, loss_div, loss_bc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0,6))\n",
    "def apply_model_spinn_random(apply_fn, params, train_boundary_data, w_ff, w_div, w_bc, bc_batch_size):\n",
    "    def residual_loss(params, x, y, z, w_ff, w_div):\n",
    "        # calculate u\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)\n",
    "        \n",
    "        # calculate J\n",
    "        Jx = curlx(apply_fn, params, x, y, z)\n",
    "        Jy = curly(apply_fn, params, x, y, z)\n",
    "        Jz = curlz(apply_fn, params, x, y, z)\n",
    "        J = jnp.stack([Jx, Jy, Jz], axis=-1)\n",
    "\n",
    "        JxB = jnp.cross(J, B, axis=-1) \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # loss_ff = jnp.sum(JxB**2, axis=-1)\n",
    "        # loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        loss_ff = jnp.sum(JxB**2, axis=-1) / (jnp.sum(B**2, axis=-1) + 1e-7)\n",
    "        loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        # loss_ff = jnp.mean(JxB**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # assumes x, y, z have same shape (very important)\n",
    "        vec_x = jnp.ones(x.shape)\n",
    "        vec_y = jnp.ones(y.shape)\n",
    "        vec_z = jnp.ones(z.shape)\n",
    "        \n",
    "        Bx_x = jvp(lambda x: apply_fn(params, x, y, z)[0], (x,), (vec_x,))[1]\n",
    "        # Bx_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec,))[1]\n",
    "        # Bx_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec,))[1]\n",
    "\n",
    "        # By_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec,))[1]\n",
    "        By_y = jvp(lambda y: apply_fn(params, x, y, z)[1], (y,), (vec_y,))[1]\n",
    "        # By_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec,))[1]\n",
    "\n",
    "        # Bz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec,))[1]\n",
    "        # Bz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec,))[1]\n",
    "        Bz_z = jvp(lambda z: apply_fn(params, x, y, z)[2], (z,), (vec_z,))[1]\n",
    "\n",
    "        divB = Bx_x + By_y + Bz_z\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # loss_div = jnp.sum((divB)**2, axis=-1)\n",
    "        # loss_div = jnp.mean(loss_div)\n",
    "\n",
    "        loss_div = jnp.mean((divB)**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        loss = w_ff*loss_ff + w_div*loss_div\n",
    "\n",
    "        return loss, loss_ff, loss_div\n",
    "\n",
    "    def boundary_loss(params, x, y, z, bc_batch_size, *boundary_data):\n",
    "        \n",
    "        # loss = 0.\n",
    "        # for i in np.arange(4):\n",
    "        #     boundary_data_batched = boundary_batches[i, :, :, :]\n",
    "        #     xb = boundary_data_batched[:, 0, :][:, 0].reshape(-1, 1)\n",
    "        #     yb = boundary_data_batched[:, 0, :][:, 1].reshape(-1, 1)\n",
    "        #     zb = boundary_data_batched[:, 0, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bx, By, Bz = apply_fn(params, xb, yb, zb)\n",
    "        #     # Bx, By, Bz = Bx.reshape(-1, 1), By.reshape(-1, 1), Bz.reshape(-1, 1)\n",
    "\n",
    "        #     Bxb = boundary_data_batched[:, 1, :][:, 0].reshape(-1, 1)\n",
    "        #     Byb = boundary_data_batched[:, 1, :][:, 1].reshape(-1, 1)\n",
    "        #     Bzb = boundary_data_batched[:, 1, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bxb_mesh, Byb_mesh, Bzb_mesh = jnp.meshgrid(Bxb.ravel(), Byb.ravel(), Bzb.ravel(), indexing='ij')\n",
    "            \n",
    "        #     loss += jnp.mean((Bx - Bxb_mesh)**2) + jnp.mean((By - Byb_mesh)**2) + jnp.mean((Bz - Bzb_mesh)**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  \n",
    "        #2 x=0   lateral_1            \n",
    "        #3 x=2   lateral_2            \n",
    "        #4 y=0   lateral_3            \n",
    "        #5 y=2   lateral_4            \n",
    "\n",
    "        b_bottom, bp_top, bp_lateral_1, bp_lateral_2, bp_lateral_3, bp_lateral_4 = boundary_data\n",
    "\n",
    "        loss = 0.\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[0], y[0], z[0])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        \n",
    "        bc_bottom_training_data = jnp.vstack([Bx.flatten(), b_bottom[:, :, 0].flatten()])\n",
    "        bc_bottom_training_data = jnp.concatenate([bc_bottom_training_data, jnp.vstack([By.flatten(), b_bottom[:, :, 1].flatten()])], axis=1)\n",
    "        bc_bottom_training_data = jnp.concatenate([bc_bottom_training_data, jnp.vstack([Bz.flatten(), b_bottom[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.vstack([Bx.flatten(), bp_top[:, :, 0].flatten()])\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_top[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_top[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_1[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_1[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_1[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_2[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_2[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_2[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_3[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_3[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_3[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_4[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_4[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_4[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "        bc_latent_batch_size = bc_batch_size // 6\n",
    "        bc_bottom_batch_size = bc_batch_size - bc_latent_batch_size\n",
    "\n",
    "        M = bc_training_data.shape[-1]\n",
    "        random_indices = random.sample(range(M), bc_latent_batch_size)\n",
    "        bc_training_data = bc_training_data[:, random_indices]\n",
    "        loss += jnp.mean((bc_training_data[0] - bc_training_data[1])**2)\n",
    "\n",
    "        M =  bc_bottom_training_data.shape[-1]\n",
    "        random_indices = random.sample(range(M), bc_bottom_batch_size)\n",
    "        bc_bottom_training_data = bc_bottom_training_data[:, random_indices]\n",
    "        loss += jnp.mean((bc_bottom_training_data[0] - bc_bottom_training_data[1])**2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # unpack data\n",
    "    train_data = train_boundary_data[0]\n",
    "    boundary_data = train_boundary_data[1]\n",
    "    xc, yc, zc, xb, yb, zb = train_data\n",
    "\n",
    "    # isolate loss func from redundant arguments\n",
    "    loss_pde, loss_ff, loss_div = residual_loss(params, xc, yc, zc, w_ff, w_div)\n",
    "    loss_bc = boundary_loss(params, xb, yb, zb, bc_batch_size, *boundary_data)\n",
    "\n",
    "    loss_fn = lambda params: residual_loss(params, xc, yc, zc, w_ff, w_div)[0] + w_bc*boundary_loss(params, xb, yb, zb, bc_batch_size, *boundary_data)\n",
    "\n",
    "    loss, gradient = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    return loss, gradient, [loss_ff, loss_div, loss_bc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spinn_cube:\n",
    "    def __init__(self, param_path, parameters_path):\n",
    "        self.param_path = param_path\n",
    "        self.parameters_path = parameters_path\n",
    "    \n",
    "    def calculate_magnetic_fields(self):\n",
    "        param_path = self.param_path\n",
    "        parameters_path = self.parameters_path\n",
    "\n",
    "        with open(parameters_path, \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "\n",
    "        features = parameters['features']\n",
    "        n_layers = parameters['n_layers']\n",
    "        feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "\n",
    "        subkey = jax.random.PRNGKey(0)\n",
    "        model = SPINN3d(feat_sizes, r, out_dim, pos_enc=pos_enc, mlp=mlp)\n",
    "        model.init(\n",
    "                    subkey,\n",
    "                    jnp.ones((Nx, 1)),\n",
    "                    jnp.ones((Ny, 1)),\n",
    "                    jnp.ones((Nz, 1))\n",
    "                   )\n",
    "        apply_fn = jax.jit(model.apply)\n",
    "\n",
    "        with open(param_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        x = jnp.linspace(0, n_max_x, Nx).reshape(-1, 1)\n",
    "        y = jnp.linspace(0, n_max_y, Ny).reshape(-1, 1)\n",
    "        z = jnp.linspace(0, n_max_z, Nz).reshape(-1, 1)\n",
    "        x, y, z = jax.lax.stop_gradient(x), jax.lax.stop_gradient(y), jax.lax.stop_gradient(z)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)*b_norm\n",
    "        \n",
    "        Bx = B[..., 0]\n",
    "        By = B[..., 1]\n",
    "        Bz = B[..., 2]\n",
    "\n",
    "        co_bounds = (0, Nx-1, 0, Ny-1, 0, Nz-1)\n",
    "        co_coords = create_coordinates(co_bounds).reshape(-1, 3)\n",
    "        co_coord = co_coords.reshape(Nx, Ny, Nz, 3)\n",
    "        x = co_coord[..., 0]\n",
    "        y = co_coord[..., 1]\n",
    "        z = co_coord[..., 2]\n",
    "        mesh = pv.StructuredGrid(x, y, z)\n",
    "        vectors = np.stack([Bx, By, Bz], axis=-1).transpose(2, 1, 0, 3).reshape(-1, 3)\n",
    "        mesh['B'] = vectors\n",
    "        mesh.active_vectors_name = 'B'\n",
    "        magnitude = np.linalg.norm(vectors, axis=-1)\n",
    "        mesh['mag'] = magnitude\n",
    "        mesh.active_scalars_name = 'mag'\n",
    "\n",
    "        self.grid = mesh \n",
    "        return self.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPINN_series_Trainer:\n",
    "    def __init__(self, output_path, wdb, single=False):\n",
    "        self.parameters = wdb.config\n",
    "        \n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        for hdlr in logger.handlers[:]:\n",
    "            logger.removeHandler(hdlr)\n",
    "        logger.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(output_path, \"info_log\"))) \n",
    "        logger.addHandler(logging.StreamHandler()) \n",
    "        self.logger = logger\n",
    "\n",
    "        logger.info(self.parameters)\n",
    "        parameters_path = os.path.join(output_path, \"parameters.pickle\")\n",
    "        with open(parameters_path, \"wb\") as f:\n",
    "            pa = dict(self.parameters)\n",
    "            pickle.dump(pa, f)\n",
    "\n",
    "        self.result_path = output_path\n",
    "\n",
    "        self.wandb = wdb\n",
    "        self.single = single\n",
    "        \n",
    "    def setup(self, b_bottom_list):\n",
    "\n",
    "        single = self.single\n",
    "\n",
    "        parameters = self.parameters\n",
    "\n",
    "        features = parameters['features']\n",
    "        n_layers = parameters['n_layers']\n",
    "        feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        lr = parameters['lr']\n",
    "        lr_decay_iterations = parameters['lr_decay_iterations'][0]\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "        is_random = parameters['is_random']\n",
    "        Nc = parameters['Nc']\n",
    "        decay_rate = parameters['decay_rate']\n",
    "        potential_boundary_batch_size = parameters['potential_boundary_batch_size']\n",
    "        activation = parameters['activation']\n",
    "\n",
    "        seed = 111\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        self.subkey = subkey\n",
    "\n",
    "        model = SPINN3d(feat_sizes, r, out_dim, pos_enc=pos_enc, mlp=mlp, activation=activation)\n",
    "\n",
    "        params = model.init(\n",
    "                        subkey,\n",
    "                        jnp.ones((Nx, 1)),\n",
    "                        jnp.ones((Ny, 1)),\n",
    "                        jnp.ones((Nz, 1))\n",
    "                    )\n",
    "        apply_fn = jax.jit(model.apply)\n",
    "        self.apply_fn = apply_fn\n",
    "\n",
    "        b_bottom_path = b_bottom_list[0]\n",
    "        self.b_bottom_path = b_bottom_path\n",
    "        with open(b_bottom_path, 'rb') as f:\n",
    "            b_bottom = np.load(f)\n",
    "        self.b_bottom = b_bottom\n",
    "\n",
    "        b_bottom_date = os.path.basename(b_bottom_path)[9:-4]\n",
    "        self.output_path  = os.path.join(self.result_path, b_bottom_date)\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "\n",
    "        final_params_path = os.path.join(self.output_path, f\"final_params.pickle\")\n",
    "\n",
    "        if os.path.exists(final_params_path) and (single is False):\n",
    "            with open(final_params_path, 'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "        \n",
    "        if lr_decay_iterations is not None: \n",
    "            optim = optax.adam(learning_rate=optax.exponential_decay(init_value=lr, transition_steps=lr_decay_iterations,\n",
    "                                                                     decay_rate=decay_rate))\n",
    "        else:\n",
    "            optim = optax.adam(learning_rate=lr)\n",
    "\n",
    "        state = optim.init(params)\n",
    "\n",
    "        self.optim = optim\n",
    "        \n",
    "        self.b_bottom_list = b_bottom_list\n",
    "\n",
    "        if os.path.exists(final_params_path) and (single is False):\n",
    "            pass\n",
    "        else:\n",
    "            parameters = self.parameters\n",
    "            subkey = self.subkey\n",
    "            logger = self.logger\n",
    "        \n",
    "            BC_path = os.path.join(self.output_path, 'BC.pickle')\n",
    "            if not os.path.exists(BC_path):\n",
    "                Nz = parameters['Nz']\n",
    "                b_norm = parameters['b_norm']\n",
    "                cal_and_save_potential_boundary_for_spinn(b_bottom, Nz, b_norm, BC_path, potential_boundary_batch_size) \n",
    "                \n",
    "                import torch\n",
    "                torch.cuda.empty_cache()  \n",
    "\n",
    "            with open(BC_path, 'rb') as f:\n",
    "                boundary_data = pickle.load(f)\n",
    "\n",
    "            self.boundary_data = boundary_data\n",
    "\n",
    "            is_random = parameters['is_random']\n",
    "\n",
    "            Nx = parameters['Nx']\n",
    "            Ny = parameters['Ny']\n",
    "            Nz = parameters['Nz']\n",
    "            b_norm = parameters['b_norm']\n",
    "            n_max_x = parameters['n_max_x']\n",
    "            n_max_y = parameters['n_max_y']\n",
    "            n_max_z = parameters['n_max_z']\n",
    "            is_random = parameters['is_random']\n",
    "\n",
    "            if is_random is True:\n",
    "                Nc = parameters['Nc']\n",
    "                Ncx, Ncy, Ncz = parameters['NcxNcyNcz']\n",
    "                choice = parameters['choice']\n",
    "                train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "            else:\n",
    "                train_data = generate_train_data(Nx, Ny, Nz, n_max_x, n_max_y, n_max_z)\n",
    "\n",
    "            self.train_boundary_data = [train_data, boundary_data]\n",
    "\n",
    "            # losses = []\n",
    "            logger.info('Complie Start')\n",
    "\n",
    "            w_ff = parameters['w_ff']\n",
    "            w_div = parameters['w_div']\n",
    "            w_bc = parameters['w_bc']\n",
    "            w_bc_decay_iterations = parameters['w_bc_decay_iterations'][0]\n",
    "            w_bc_decay = (1 / w_bc) ** (1 / w_bc_decay_iterations) if w_bc_decay_iterations is not None else 1\n",
    "\n",
    "            bc_batch_size = parameters['bc_batch_size'][0]\n",
    "\n",
    "            start = time.time()\n",
    "            if bc_batch_size is not None:\n",
    "                loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "            else:\n",
    "                loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "\n",
    "            # losses.append(loss.item())\n",
    "            # loss_ff, loss_div, loss_bc = losses\n",
    "            # self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item(), \"w_ff*loss_ff\":w_ff*loss_ff.item(), \"w_div*loss_div\":w_div*loss_div.item(), \"w_bc*loss_bc\":w_bc*loss_bc.item()}})\n",
    "            params, state = update_model(self.optim, gradient, params, state)\n",
    "\n",
    "            if w_bc > 1:\n",
    "                w_bc *= w_bc_decay\n",
    "                if w_bc <= 1:\n",
    "                    w_bc = 1\n",
    "\n",
    "            if is_random is True:\n",
    "                train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                self.train_boundary_data = [train_data, self.boundary_data]\n",
    "            \n",
    "            if bc_batch_size is not None:\n",
    "                loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "            else:\n",
    "                loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "            # losses.append(loss.item())\n",
    "            # loss_ff, loss_div, loss_bc = losses\n",
    "            # self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item(), \"w_ff*loss_ff\":w_ff*loss_ff.item(), \"w_div*loss_div\":w_div*loss_div.item(), \"w_bc*loss_bc\":w_bc*loss_bc.item()}})\n",
    "            params, state = update_model(self.optim, gradient, params, state)\n",
    "            runtime = time.time() - start\n",
    "    \n",
    "            logger.info(f'Complie End --> total: {runtime:.2f}sec')\n",
    "\n",
    "            # self.losses = losses\n",
    "        \n",
    "        self.state = state\n",
    "        self.params = params\n",
    "        self.key = key\n",
    "        self.single = single\n",
    "\n",
    "    def train(self, pot_me, dV):\n",
    "\n",
    "        self.pot_me = pot_me\n",
    "        self.dV = dV\n",
    "\n",
    "        single = self.single\n",
    "\n",
    "        key = self.key\n",
    "        \n",
    "        logger = self.logger\n",
    "        params = self.params\n",
    "        state = self.state\n",
    "        parameters = self.parameters\n",
    "        \n",
    "        total_iterations = parameters['total_iterations']\n",
    "        log_iterations = parameters['log_interval']\n",
    "\n",
    "        series_total_iterations = parameters['series_iterations']\n",
    "        series_log_iterations = parameters['series_log_interval']\n",
    "\n",
    "        loss_threshold = parameters['loss_threshold']\n",
    "        random_interval = parameters['random_interval']\n",
    "        features = parameters['features']\n",
    "        n_layers = parameters['n_layers']\n",
    "        feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        lr = parameters['lr']\n",
    "        series_lr = parameters['series_lr']\n",
    "        series_lr_decay_iterations = parameters['series_lr_decay_iterations']\n",
    "        lr_decay_iterations = parameters['lr_decay_iterations'][0]\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "        is_random = parameters['is_random']\n",
    "        Nc = parameters['Nc']\n",
    "        decay_rate = parameters['decay_rate']\n",
    "        w_ff = parameters['w_ff']\n",
    "        w_div = parameters['w_div']\n",
    "        w_bc = parameters['w_bc']\n",
    "        w_bc_decay_iterations = parameters['w_bc_decay_iterations'][0]\n",
    "        potential_boundary_batch_size = parameters['potential_boundary_batch_size']\n",
    "        w_bc_decay = (1 / w_bc) ** (1 / w_bc_decay_iterations) if w_bc_decay_iterations is not None else 1\n",
    "\n",
    "        bc_batch_size = parameters['bc_batch_size'][0]\n",
    "\n",
    "        b_bottom_list = self.b_bottom_list\n",
    "\n",
    "        final_params_path = os.path.join(self.output_path, f\"final_params.pickle\")\n",
    "\n",
    "        if (not os.path.exists(final_params_path)) or (single is True):\n",
    "            # losses = self.losses\n",
    "\n",
    "            start = time.time()\n",
    "            for e in trange(1, total_iterations + 1):\n",
    "\n",
    "                if w_bc > 1:\n",
    "                    w_bc *= w_bc_decay\n",
    "                    if w_bc <= 1:\n",
    "                        w_bc = 1\n",
    "\n",
    "                if is_random is True:\n",
    "                    if e % random_interval == 0:\n",
    "                        # sample new input data\n",
    "                        key, subkey = jax.random.split(key, 2)\n",
    "                        Nc = parameters['Nc']\n",
    "                        Ncx, Ncy, Ncz = parameters['NcxNcyNcz']\n",
    "                        choice = parameters['choice']\n",
    "                        train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                        self.train_boundary_data = [train_data, self.boundary_data]\n",
    "\n",
    "                if bc_batch_size is not None:\n",
    "                    loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "                else:\n",
    "                    loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "                # losses.append(loss.item())\n",
    "                loss_ff, loss_div, loss_bc = losses\n",
    "                self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item(), \"w_ff*loss_ff\":w_ff*loss_ff.item(), \"w_div*loss_div\":w_div*loss_div.item(), \"w_bc*loss_bc\":w_bc*loss_bc.item()}})\n",
    "                if loss.item() < loss_threshold:\n",
    "                    if logger is None:\n",
    "                        print(f'Epoch: {e}/{total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                    else:\n",
    "                        logger.info(f'Epoch: {e}/{total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                    break\n",
    "                \n",
    "                params, state = update_model(self.optim, gradient, params, state)\n",
    "                \n",
    "                if e % log_iterations == 0:\n",
    "                    div, ff, b_diff, ratio, total_free_energy, val_loss = self.validation(params)\n",
    "                    logger.info(f'Epoch: {e}/{total_iterations} --> total loss: {loss:.8f}, div {div:.8f}, ff {ff:.8f}, b_diff {b_diff:.8f}, ratio {ratio:.4f}, total_free_energy {total_free_energy:.4f}, val_loss {val_loss:.8f}')\n",
    "\n",
    "            with open(final_params_path, \"wb\") as f:\n",
    "                pickle.dump(params, f)\n",
    "\n",
    "            # np.save(os.path.join(self.output_path, 'losses.npy'), losses)\n",
    "            # with open(os.path.join(self.output_path, 'losses.npy'), \"rb\") as f:\n",
    "            #     losses = np.load(f)\n",
    "\n",
    "            # fig, ax = plt.subplots(figsize=(4,3))\n",
    "            # ax.plot(losses)\n",
    "            # ax.set_xlabel('Iteration')\n",
    "            # ax.set_ylabel('Loss')\n",
    "            # ax.set_title('SPINN')\n",
    "            # plt.tight_layout()\n",
    "            # plt.savefig(os.path.join(self.output_path, 'loss_SPINN.png'), dpi=300)\n",
    "\n",
    "            runtime = time.time() - start\n",
    "            logger.info(f'Runtime --> total: {runtime:.2f}sec ({(runtime/(total_iterations-1)*1000):.2f}ms/iter.)')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        for b_bottom_path in b_bottom_list[1:]:\n",
    "            b_bottom_date = os.path.basename(b_bottom_path)[9:-4]\n",
    "            output_path = os.path.join(self.result_path, b_bottom_date)    \n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            final_params_path = os.path.join(output_path, f\"final_params.pickle\")\n",
    "\n",
    "            if os.path.exists(final_params_path):\n",
    "                with open(final_params_path, 'rb') as f:\n",
    "                    params = pickle.load(f)\n",
    "\n",
    "            if not os.path.exists(final_params_path):\n",
    "\n",
    "                if series_lr_decay_iterations is not None: \n",
    "                    optim = optax.adam(learning_rate=optax.exponential_decay(init_value=series_lr, transition_steps=series_lr_decay_iterations,\n",
    "                                                                                decay_rate=decay_rate))\n",
    "                else:\n",
    "                    optim = optax.adam(learning_rate=series_lr)\n",
    "\n",
    "                with open(b_bottom_path, 'rb') as f:\n",
    "                    b_bottom = np.load(f)\n",
    "\n",
    "\n",
    "                BC_path = os.path.join(output_path, 'BC.pickle')\n",
    "                if not os.path.exists(BC_path):\n",
    "                    Nz = parameters['Nz']\n",
    "                    b_norm = parameters['b_norm']\n",
    "                    cal_and_save_potential_boundary_for_spinn(b_bottom, Nz, b_norm, BC_path, potential_boundary_batch_size)  \n",
    "\n",
    "                with open(BC_path, 'rb') as f:\n",
    "                    boundary_data = pickle.load(f)\n",
    "\n",
    "                if is_random is True:\n",
    "                    train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                else:\n",
    "                    train_data = generate_train_data(Nx, Ny, Nz, n_max_x, n_max_y, n_max_z)\n",
    "\n",
    "                self.train_boundary_data = [train_data, boundary_data]\n",
    "\n",
    "                losses = []\n",
    "                start = time.time()\n",
    "                for e in trange(1, series_total_iterations + 1):\n",
    "\n",
    "                    if w_bc > 1:\n",
    "                        w_bc *= w_bc_decay\n",
    "                        if w_bc <= 1:\n",
    "                            w_bc = 1\n",
    "\n",
    "                    if is_random is True:\n",
    "                        if e % random_interval == 0:\n",
    "                            # sample new input data\n",
    "                            key, subkey = jax.random.split(key, 2)\n",
    "                            Nc = parameters['Nc']\n",
    "                            Ncx, Ncy, Ncz = parameters['NcxNcyNcz']\n",
    "                            choice = parameters['choice']\n",
    "                            train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                            self.train_boundary_data = [train_data, self.boundary_data]\n",
    "\n",
    "                    if bc_batch_size is not None:\n",
    "                        loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "                    else:\n",
    "                        loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "                    # losses.append(loss.item())\n",
    "                    loss_ff, loss_div, loss_bc = losses\n",
    "                    self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item(), \"w_ff*loss_ff\":w_ff*loss_bc.item(), \"w_div*loss_div\":w_div*loss_bc.item(), \"w_bc*loss_bc\":w_bc*loss_bc.item()}})\n",
    "                    if loss.item() < loss_threshold:\n",
    "                        if logger is None:\n",
    "                            print(f'Epoch: {e}/{series_total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                        else:\n",
    "                            logger.info(f'Epoch: {e}/{series_total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                        break\n",
    "                    \n",
    "                    params, state = update_model(optim, gradient, params, state)\n",
    "                    \n",
    "                    if e % series_log_iterations == 0:\n",
    "                        div, ff, b_diff, ratio, total_free_energy, val_loss = self.validation(params)\n",
    "                        logger.info(f'Epoch: {e}/{series_total_iterations} --> total loss: {loss:.8f}, div {div:.8f}, ff {ff:.8f}, b_diff {b_diff:.8f}, ratio {ratio:.4f}, total_free_energy {total_free_energy:.4f}, val_loss {val_loss:.8f}')\n",
    "\n",
    "                with open(final_params_path, \"wb\") as f:\n",
    "                    pickle.dump(params, f)\n",
    "\n",
    "                # np.save(os.path.join(output_path, 'losses.npy'), losses)\n",
    "\n",
    "                runtime = time.time() - start\n",
    "                logger.info(f'Runtime --> total: {runtime:.2f}sec ({(runtime/(series_total_iterations-1)*1000):.2f}ms/iter.)')\n",
    "            else:\n",
    "                # with open(os.path.join(output_path, 'losses.npy'), \"rb\") as f:\n",
    "                #     losses = np.load(f)\n",
    "\n",
    "                # fig, ax = plt.subplots(figsize=(4,3))\n",
    "                # ax.plot(losses)\n",
    "                # ax.set_xlabel('Iteration')\n",
    "                # ax.set_ylabel('Loss')\n",
    "                # ax.set_title('SPINN')\n",
    "                # plt.tight_layout()\n",
    "                # plt.savefig(os.path.join(output_path, 'loss_SPINN.png'), dpi=300)\n",
    "                pass\n",
    "\n",
    "    def validation(self, params):\n",
    "        b_norm = self.parameters['b_norm']\n",
    "        # pot_me = self.parameters['pot_me']\n",
    "        # dV = self.parameters['dV']\n",
    "        parameters = self.parameters\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_bottom = self.b_bottom\n",
    "\n",
    "        pot_me = self.pot_me\n",
    "        dV = self.dV\n",
    "\n",
    "        x = jnp.linspace(0, n_max_x, Nx).reshape(-1, 1)\n",
    "        y = jnp.linspace(0, n_max_y, Ny).reshape(-1, 1)\n",
    "        z = jnp.linspace(0, n_max_z, Nz).reshape(-1, 1)\n",
    "        x, y, z = jax.lax.stop_gradient(x), jax.lax.stop_gradient(y), jax.lax.stop_gradient(z)\n",
    "\n",
    "        Bx, By, Bz = self.apply_fn(params, x, y, z)\n",
    "        B = np.stack([Bx, By, Bz], axis=-1)*b_norm\n",
    "        J = curl(B)\n",
    "\n",
    "        div = (divergence(B)**2).mean()\n",
    "\n",
    "        JxB = np.cross(J, B)\n",
    "        ff = np.sum(JxB**2, axis=-1) / (np.sum(B**2, axis=-1) + 1e-7)\n",
    "        ff = np.mean(ff)\n",
    "\n",
    "        b_diff = np.mean((B[:, :, 0, :]/b_norm - b_bottom/b_norm)**2)\n",
    "\n",
    "        jmaps = magnitude(J).sum(2)\n",
    "\n",
    "        val_loss = div+ff+b_diff\n",
    "        self.wandb.log({\"val_loss\":val_loss})\n",
    "\n",
    "        me = magnetic_energy(B) * dV\n",
    "        free_me = me - pot_me \n",
    "\n",
    "        total_energy = me.sum()\n",
    "        total_potential_energy = pot_me.sum()\n",
    "        ratio = total_energy/total_potential_energy\n",
    "        total_free_energy = free_me.sum()/1e32\n",
    "\n",
    "        self.wandb.log({\"val\": {\"div\":div, \"ff\":ff, \"b_diff\":b_diff, \"ratio_E_and_E_pot\":ratio, \"free_energy(1e32)\":total_free_energy}})\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(jmaps.T, origin='lower')\n",
    "        fig.colorbar(im)\n",
    "        self.wandb.log({\"plot\": {\"J_map\":fig}})\n",
    "        plt.close()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(B[:, :, 0, 2].T, origin='lower', cmap='gray')\n",
    "        fig.colorbar(im)\n",
    "        self.wandb.log({\"plot\": {\"Bz\":fig}})\n",
    "        plt.close()\n",
    "\n",
    "        free_energy_map = free_me.sum(2)\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(free_energy_map.T, origin='lower', cmap='jet')\n",
    "        fig.colorbar(im)\n",
    "        self.wandb.log({\"plot\": {\"Free_energy_map\":fig}})\n",
    "        plt.close()\n",
    "\n",
    "        return div, ff, b_diff, ratio, total_free_energy, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/userhome/jeon_mg/workspace/workspace_mine/_data/NOAA11158/b_bottom/b_bottom_20110214_160000.npy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_path = os.path.expanduser('~/workspace/workspace_mine/_data/NOAA12673/')\n",
    "base_path = os.path.expanduser('~/workspace/workspace_mine/_data/NOAA11158/')\n",
    "\n",
    "result_path = os.path.join(base_path, 'result')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "b_bottom_path = os.path.join(base_path, 'b_bottom')\n",
    "\n",
    "b_bottom_list = sorted(glob.glob(os.path.join(b_bottom_path, '*.npy')))\n",
    "b_bottom_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "}\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "\n",
    "with open(b_bottom_list[0], 'rb') as f:\n",
    "    b_bottom = np.load(f)\n",
    "\n",
    "Nx, Ny, _ = b_bottom.shape\n",
    "Nz = 160\n",
    "b_norm = 2500\n",
    "total_iterations = 10000\n",
    "log_interval = 1000\n",
    "loss_threshold = 1e-3\n",
    "\n",
    "series_iterations = 200\n",
    "series_log_interval = 20\n",
    "series_lr = 5e-5\n",
    "series_lr_decay_iterations = series_iterations\n",
    "\n",
    "out_dim = 3 \n",
    "lr_decay_iterations = total_iterations\n",
    "\n",
    "pos_enc = 0\n",
    "\n",
    "spatial_norm = 160\n",
    "n_max_x = (Nx/spatial_norm)\n",
    "n_max_y = (Ny/spatial_norm)\n",
    "n_max_z = (Nz/spatial_norm)\n",
    "\n",
    "# Ncx = None\n",
    "# Ncy = None\n",
    "# Ncz = None\n",
    "\n",
    "potential_boundary_batch_size = 3000\n",
    "\n",
    "# parameters = {\n",
    "#     'features' : {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 128,\n",
    "#         'max': 512\n",
    "#     }, \n",
    "#     'n_layers' : {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 3,\n",
    "#         'max': 8\n",
    "#     }, \n",
    "#     'r' : {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 128,\n",
    "#         'max': 512\n",
    "#     }, \n",
    "#     'out_dim' : {\n",
    "#         'value': out_dim\n",
    "#     }, \n",
    "#     'Nx' : {\n",
    "#         'value': Nx\n",
    "#     }, \n",
    "#     'Ny' : {\n",
    "#         'value': Ny\n",
    "#     }, \n",
    "#     'Nz' : {\n",
    "#         'value': Nz\n",
    "#     }, \n",
    "#     'b_norm' : {\n",
    "#         'value': b_norm\n",
    "#     },\n",
    "#     'pos_enc' : {\n",
    "#         'value': pos_enc\n",
    "#     },\n",
    "#     'mlp' : {\n",
    "#         'values': ['mlp', 'modified_mlp']\n",
    "#     },\n",
    "#     'lr': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-5,\n",
    "#         'max': 1e-3\n",
    "#     },\n",
    "#     'series_lr': {\n",
    "#         'value': series_lr\n",
    "#     },\n",
    "#     'series_lr_decay_iterations': {\n",
    "#         'value': series_lr_decay_iterations\n",
    "#     },\n",
    "#     'lr_decay_iterations': {\n",
    "#         'values': [None, lr_decay_iterations//2, lr_decay_iterations]\n",
    "#     },\n",
    "#     'n_max_x': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 4*n_max_x\n",
    "#     },\n",
    "#     'n_max_y': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 4*n_max_y\n",
    "#     },\n",
    "#     'n_max_z': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 4*n_max_z\n",
    "#     },\n",
    "#     'is_random':{\n",
    "#         'values': [True, False]\n",
    "#     },\n",
    "#     'Nc':{\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 16,\n",
    "#         'max': max(Nx, Ny, Nz)\n",
    "#     },\n",
    "#     'random_interval':{\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 1000\n",
    "#     },\n",
    "#     'w_ff': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-4,\n",
    "#         'max': 1e3\n",
    "#     },\n",
    "#     'w_div': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-4,\n",
    "#         'max': 1e3\n",
    "#     },\n",
    "#     'w_bc': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-4,\n",
    "#         'max': 1e3\n",
    "#     },\n",
    "#     'w_bc_decay_iterations': {\n",
    "#         'values': [None, total_iterations, 2*total_iterations, 3*total_iterations]\n",
    "#     },\n",
    "#     'Ncx': {\n",
    "#         'value': Ncx\n",
    "#     },\n",
    "#     'Ncy': {\n",
    "#         'value': Ncy\n",
    "#     },\n",
    "#     'Ncz': {\n",
    "#         'value': Ncz\n",
    "#     },\n",
    "#     'bc_batch_size': {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 10000,\n",
    "#         'max': 50000\n",
    "#     },\n",
    "#     'choice': {\n",
    "#         'values': [True, False]\n",
    "#     },\n",
    "#     'decay_rate': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 0.5,\n",
    "#         'max': 1\n",
    "#     },\n",
    "#     'total_iterations': {\n",
    "#         'value': total_iterations\n",
    "#     },\n",
    "#     'log_interval': {\n",
    "#         'value': log_interval\n",
    "#     },\n",
    "#     'series_iterations': {\n",
    "#         'value': series_iterations\n",
    "#     },\n",
    "#     'series_log_interval': {\n",
    "#         'value': series_log_interval\n",
    "#     },\n",
    "#     'loss_threshold': {\n",
    "#         'value': loss_threshold\n",
    "#     },\n",
    "#     'potential_boundary_batch_size': {\n",
    "#         'value': potential_boundary_batch_size\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features' : {\n",
    "        'value': 256\n",
    "    }, \n",
    "    'n_layers' : {\n",
    "        'value': 3\n",
    "    }, \n",
    "    'r' : {\n",
    "        'value': 256\n",
    "    }, \n",
    "    'out_dim' : {\n",
    "        'value': out_dim\n",
    "    }, \n",
    "    'Nx' : {\n",
    "        'value': Nx\n",
    "    }, \n",
    "    'Ny' : {\n",
    "        'value': Ny\n",
    "    }, \n",
    "    'Nz' : {\n",
    "        'value': Nz\n",
    "    }, \n",
    "    'b_norm' : {\n",
    "        'value': b_norm\n",
    "    },\n",
    "    'pos_enc' : {\n",
    "        'value': pos_enc\n",
    "    },\n",
    "    'mlp' : {\n",
    "        'value': 'modified_mlp'\n",
    "    },\n",
    "    # 'mlp' : {\n",
    "    #     'value': 'modified_mlp'\n",
    "    # },\n",
    "    'activation': {\n",
    "        'values': ['tanh', 'sin']\n",
    "    },\n",
    "    'lr': {\n",
    "        'value': 5e-4\n",
    "    },\n",
    "    'series_lr': {\n",
    "        'value': series_lr\n",
    "    },\n",
    "    'series_lr_decay_iterations': {\n",
    "        'value': series_lr_decay_iterations\n",
    "    },\n",
    "    'lr_decay_iterations': {\n",
    "        'value': [lr_decay_iterations]\n",
    "    },\n",
    "    # 'n_max_x': {\n",
    "    #     'value': n_max_x\n",
    "    # },\n",
    "    # 'n_max_y': {\n",
    "    #     'value': n_max_y\n",
    "    # },\n",
    "    # 'n_max_z': {\n",
    "    #     'value': n_max_z\n",
    "    # },\n",
    "    'n_max_x': {\n",
    "        'value': n_max_x\n",
    "    },\n",
    "    'n_max_y': {\n",
    "        'value': n_max_y\n",
    "    },\n",
    "    'n_max_z': {\n",
    "        'value': n_max_z\n",
    "    },\n",
    "    'is_random':{\n",
    "        'value': False\n",
    "    },\n",
    "    # 'NcxNcyNcz': {\n",
    "    #     'values': [[None,None,None], [Nx//2, Ny//2, Nz//2], [Nx, Ny, Nz]]\n",
    "    # },\n",
    "    'NcxNcyNcz': {\n",
    "        'value': [None,None,None]\n",
    "    },\n",
    "    'Nc':{\n",
    "        'value': 32\n",
    "    },\n",
    "    'random_interval':{\n",
    "        'value': 1\n",
    "    },\n",
    "    'w_ff': {\n",
    "        'values': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'w_div': {\n",
    "        'values': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'w_bc': {\n",
    "        'values': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'w_bc_decay_iterations': {\n",
    "        'value': [None]\n",
    "    },\n",
    "    'bc_batch_size': {\n",
    "        'value': [None]\n",
    "    },\n",
    "    'choice': {\n",
    "        'value': True\n",
    "    },\n",
    "    'decay_rate': {\n",
    "        'value': 0.98\n",
    "    },\n",
    "    'total_iterations': {\n",
    "        'value': total_iterations\n",
    "    },\n",
    "    'log_interval': {\n",
    "        'value': log_interval\n",
    "    },\n",
    "    'series_iterations': {\n",
    "        'value': series_iterations\n",
    "    },\n",
    "    'series_log_interval': {\n",
    "        'value': series_log_interval\n",
    "    },\n",
    "    'loss_threshold': {\n",
    "        'value': loss_threshold\n",
    "    },\n",
    "    'potential_boundary_batch_size': {\n",
    "        'value': potential_boundary_batch_size\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(result_path, b_bottom_date), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(b_pot_me_path):\n\u001b[0;32m---> 15\u001b[0m     pot_me \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(b_pot_me_path)\n\u001b[1;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     b_pot_mesh \u001b[39m=\u001b[39m pv\u001b[39m.\u001b[39mread(b_pot_vtk_path)\n",
      "File \u001b[0;32m~/mambaforge/envs/pinf/lib/python3.10/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    457\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    458\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/mambaforge/envs/pinf/lib/python3.10/site-packages/numpy/lib/format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    808\u001b[0m         \u001b[39m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mfromfile(fp, dtype\u001b[39m=\u001b[39;49mdtype, count\u001b[39m=\u001b[39;49mcount)\n\u001b[1;32m    810\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m         \u001b[39m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mndarray(count, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stride = 2\n",
    "Mm_per_pixel = stride*0.36\n",
    "cm_per_pixel = Mm_per_pixel*1e8\n",
    "dV = cm_per_pixel ** 3\n",
    "\n",
    "b_bottom_path = b_bottom_list[0]\n",
    "b_bottom_date = os.path.basename(b_bottom_path)[9:-4]\n",
    "\n",
    "b_potential_path = os.path.join(base_path, 'b_potential')\n",
    "b_pot_vtk_path = os.path.join(b_potential_path, f'b_potential_{b_bottom_date}.vtk')\n",
    "b_pot_me_path = os.path.join(os.path.join(result_path, b_bottom_date), f'b_me_{b_bottom_date}.npy')\n",
    "os.makedirs(os.path.join(result_path, b_bottom_date), exist_ok=True)\n",
    "\n",
    "if os.path.exists(b_pot_me_path):\n",
    "    pot_me = np.load(b_pot_me_path)\n",
    "\n",
    "else:\n",
    "    b_pot_mesh = pv.read(b_pot_vtk_path)\n",
    "    b_pot = b_pot_mesh['B'].reshape(Nz, Ny, Nx, 3).transpose(2, 1, 0, 3)\n",
    "\n",
    "    pot_me = magnetic_energy(b_pot) * dV\n",
    "\n",
    "    np.save(b_pot_me_path, pot_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 85ajyik6\n",
      "Sweep URL: https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"spinns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "       \n",
    "        trainer = SPINN_series_Trainer(result_path, wandb, single=True)\n",
    "        trainer.setup(b_bottom_list)\n",
    "        trainer.train(pot_me, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: txvq8r7h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNc: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcxNcyNcz: [None, None, None]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNx: 372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNy: 188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNz: 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tb_norm: 2500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbc_batch_size: [None]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchoice: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay_rate: 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeatures: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_random: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_interval: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_iterations: [10000]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp: modified_mlp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_x: 2.325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_y: 1.175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_z: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_dim: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpos_enc: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpotential_boundary_batch_size: 3000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_interval: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_log_interval: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr_decay_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_iterations: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc_decay_iterations: [None]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_div: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_ff: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/userhome/jeon_mg/workspace/workspace_mine/zpinn/nbs/wandb/run-20230821_160120-txvq8r7h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgjeon/spinns/runs/txvq8r7h' target=\"_blank\">polar-sweep-1</a></strong> to <a href='https://wandb.ai/mgjeon/spinns' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6' target=\"_blank\">https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgjeon/spinns' target=\"_blank\">https://wandb.ai/mgjeon/spinns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6' target=\"_blank\">https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgjeon/spinns/runs/txvq8r7h' target=\"_blank\">https://wandb.ai/mgjeon/spinns/runs/txvq8r7h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Nc': 32, 'NcxNcyNcz': [None, None, None], 'Nx': 372, 'Ny': 188, 'Nz': 160, 'activation': 'tanh', 'b_norm': 2500, 'bc_batch_size': [None], 'choice': True, 'decay_rate': 0.98, 'features': 256, 'is_random': False, 'log_interval': 1000, 'loss_threshold': 0.001, 'lr': 0.0005, 'lr_decay_iterations': [10000], 'mlp': 'modified_mlp', 'n_layers': 3, 'n_max_x': 2.325, 'n_max_y': 1.175, 'n_max_z': 1, 'out_dim': 3, 'pos_enc': 0, 'potential_boundary_batch_size': 3000, 'r': 256, 'random_interval': 1, 'series_iterations': 200, 'series_log_interval': 20, 'series_lr': 5e-05, 'series_lr_decay_iterations': 200, 'total_iterations': 10000, 'w_bc': 100, 'w_bc_decay_iterations': [None], 'w_div': 100, 'w_ff': 1}\n",
      "Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-08-21 16:01:36.269606: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Complie Start\n",
      "Complie End --> total: 25.51sec\n",
      " 10%|         | 999/10000 [01:03<09:35, 15.65it/s]Epoch: 1000/10000 --> total loss: 0.98377371, div 0.08580123, ff 4.20129919, b_diff 0.00357915, ratio 0.8975, total_free_energy -0.7780, val_loss 4.29067947\n",
      " 20%|        | 1999/10000 [02:12<08:26, 15.79it/s]  Epoch: 2000/10000 --> total loss: 0.51314145, div 0.04175665, ff 6.09980392, b_diff 0.00217866, ratio 1.1828, total_free_energy 1.3875, val_loss 6.14373921\n",
      " 30%|       | 2999/10000 [03:19<07:25, 15.71it/s]  Epoch: 3000/10000 --> total loss: 0.39382279, div 0.03252078, ff 4.86589003, b_diff 0.00166000, ratio 1.2165, total_free_energy 1.6430, val_loss 4.90007079\n",
      " 40%|      | 3999/10000 [04:26<06:21, 15.75it/s]  Epoch: 4000/10000 --> total loss: 0.33404806, div 0.02708131, ff 4.53321552, b_diff 0.00142538, ratio 1.2326, total_free_energy 1.7658, val_loss 4.56172239\n",
      " 50%|     | 4999/10000 [05:32<05:22, 15.53it/s]Epoch: 5000/10000 --> total loss: 0.31711543, div 0.08172453, ff 4.27012396, b_diff 0.00129064, ratio 1.2358, total_free_energy 1.7895, val_loss 4.35313925\n",
      " 60%|    | 5999/10000 [06:39<04:11, 15.88it/s]Epoch: 6000/10000 --> total loss: 0.28491348, div 0.03445977, ff 3.75336337, b_diff 0.00119512, ratio 1.2421, total_free_energy 1.8375, val_loss 3.78901832\n",
      " 70%|   | 6999/10000 [07:45<03:11, 15.69it/s]Epoch: 7000/10000 --> total loss: 0.26765910, div 0.04422354, ff 3.61477637, b_diff 0.00112063, ratio 1.2472, total_free_energy 1.8766, val_loss 3.66012055\n",
      " 80%|  | 7999/10000 [08:52<02:06, 15.81it/s]Epoch: 8000/10000 --> total loss: 0.25604239, div 0.03382791, ff 3.53261471, b_diff 0.00107453, ratio 1.2505, total_free_energy 1.9010, val_loss 3.56751725\n",
      " 90%| | 8999/10000 [09:58<01:03, 15.77it/s]Epoch: 9000/10000 --> total loss: 0.25364360, div 0.07169183, ff 3.57126379, b_diff 0.00103894, ratio 1.2450, total_free_energy 1.8599, val_loss 3.64399448\n",
      "100%|| 9999/10000 [11:04<00:00, 15.74it/s]Epoch: 10000/10000 --> total loss: 0.23564529, div 0.04339542, ff 3.36527801, b_diff 0.00100029, ratio 1.2491, total_free_energy 1.8907, val_loss 3.40967382\n",
      "100%|| 10000/10000 [11:08<00:00, 14.97it/s]\n",
      "Runtime --> total: 668.15sec (66.82ms/iter.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8f472242140cdbfe1ccc05191b989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.053 MB of 1.053 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>val_loss</td><td>3.40967</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-1</strong> at: <a href='https://wandb.ai/mgjeon/spinns/runs/txvq8r7h' target=\"_blank\">https://wandb.ai/mgjeon/spinns/runs/txvq8r7h</a><br/>Synced 6 W&B file(s), 30 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230821_160120-txvq8r7h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q8z1jp1i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNc: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcxNcyNcz: [None, None, None]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNx: 372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNy: 188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNz: 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tb_norm: 2500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbc_batch_size: [None]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchoice: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay_rate: 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeatures: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_random: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_interval: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_iterations: [10000]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp: modified_mlp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_x: 2.325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_y: 1.175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_z: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_dim: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpos_enc: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpotential_boundary_batch_size: 3000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_interval: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_log_interval: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr_decay_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_iterations: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc_decay_iterations: [None]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_div: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_ff: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/userhome/jeon_mg/workspace/workspace_mine/zpinn/nbs/wandb/run-20230821_161325-q8z1jp1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgjeon/spinns/runs/q8z1jp1i' target=\"_blank\">comfy-sweep-2</a></strong> to <a href='https://wandb.ai/mgjeon/spinns' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6' target=\"_blank\">https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgjeon/spinns' target=\"_blank\">https://wandb.ai/mgjeon/spinns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6' target=\"_blank\">https://wandb.ai/mgjeon/spinns/sweeps/85ajyik6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgjeon/spinns/runs/q8z1jp1i' target=\"_blank\">https://wandb.ai/mgjeon/spinns/runs/q8z1jp1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Nc': 32, 'NcxNcyNcz': [None, None, None], 'Nx': 372, 'Ny': 188, 'Nz': 160, 'activation': 'tanh', 'b_norm': 2500, 'bc_batch_size': [None], 'choice': True, 'decay_rate': 0.98, 'features': 256, 'is_random': False, 'log_interval': 1000, 'loss_threshold': 0.001, 'lr': 0.0005, 'lr_decay_iterations': [10000], 'mlp': 'modified_mlp', 'n_layers': 3, 'n_max_x': 2.325, 'n_max_y': 1.175, 'n_max_z': 1, 'out_dim': 3, 'pos_enc': 0, 'potential_boundary_batch_size': 3000, 'r': 256, 'random_interval': 1, 'series_iterations': 200, 'series_log_interval': 20, 'series_lr': 5e-05, 'series_lr_decay_iterations': 200, 'total_iterations': 10000, 'w_bc': 0.01, 'w_bc_decay_iterations': [None], 'w_div': 10, 'w_ff': 1}\n",
      "Complie Start\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
