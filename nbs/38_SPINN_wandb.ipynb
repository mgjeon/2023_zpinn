{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "setproctitle(\"SPINN\")\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import jvp\n",
    "import optax\n",
    "from flax import linen as nn \n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "from typing import Sequence\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "\n",
    "import random\n",
    "\n",
    "import pyvista as pv\n",
    "import pickle\n",
    "from cmspinn.mag_viz import create_coordinates\n",
    "\n",
    "from cmspinn.potential_field import cal_and_save_potential_boundary_for_spinn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "from cmspinn.evaluation import magnetic_energy, divergence, magnitude, curl, laplacian_vector\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp_fwdfwd(f, primals, tangents, return_primals=False):\n",
    "    g = lambda primals: jvp(f, (primals,), tangents)[1]\n",
    "    primals_out, tangents_out = jvp(g, primals, tangents)\n",
    "    if return_primals:\n",
    "        return primals_out, tangents_out\n",
    "    else:\n",
    "        return tangents_out\n",
    "    \n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def update_model(optim, gradient, params, state):\n",
    "    updates, state = optim.update(gradient, state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, state\n",
    "\n",
    "def curlx(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_x = uz_y - uy_z\n",
    "    vec_z = jnp.ones(z.shape)\n",
    "    vec_y = jnp.ones(y.shape)\n",
    "    uy_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec_z,))[1]\n",
    "    uz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec_y,))[1]\n",
    "    wx = uz_y - uy_z\n",
    "    return wx\n",
    "\n",
    "\n",
    "def curly(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_y = ux_z - uz_x\n",
    "    vec_z = jnp.ones(z.shape)\n",
    "    vec_x = jnp.ones(x.shape)\n",
    "    ux_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec_z,))[1]\n",
    "    uz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec_x,))[1]\n",
    "    wy = ux_z - uz_x\n",
    "    return wy\n",
    "\n",
    "def curlz(apply_fn, params, x, y, z):\n",
    "    # curl vector w/ forward-mode AD\n",
    "    # w_z = uy_x - ux_y\n",
    "    vec_y = jnp.ones(y.shape)\n",
    "    vec_x = jnp.ones(x.shape)\n",
    "    ux_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec_y,))[1]\n",
    "    uy_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec_x,))[1]\n",
    "    wz = uy_x - ux_y\n",
    "    return wz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPINN3d(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    r: int\n",
    "    out_dim: int\n",
    "    pos_enc: int\n",
    "    mlp: str\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, y, z):\n",
    "        '''\n",
    "        inputs: input factorized coordinates\n",
    "        outputs: feature output of each body network\n",
    "        xy: intermediate tensor for feature merge btw. x and y axis\n",
    "        pred: final model prediction (e.g. for 2d output, pred=[u, v])\n",
    "        '''\n",
    "        if self.pos_enc != 0:\n",
    "            # positional encoding only to spatial coordinates\n",
    "            freq = jnp.expand_dims(jnp.arange(1, self.pos_enc+1, 1), 0)\n",
    "            x = jnp.concatenate((jnp.ones((x.shape[0], 1)), jnp.sin(x@freq), jnp.cos(x@freq)), 1)\n",
    "            y = jnp.concatenate((jnp.ones((y.shape[0], 1)), jnp.sin(y@freq), jnp.cos(y@freq)), 1)\n",
    "            z = jnp.concatenate((jnp.ones((z.shape[0], 1)), jnp.sin(z@freq), jnp.cos(z@freq)), 1)\n",
    "\n",
    "            # causal PINN version (also on time axis)\n",
    "            #  freq_x = jnp.expand_dims(jnp.power(10.0, jnp.arange(0, 3)), 0)\n",
    "            # x = x@freq_x\n",
    "            \n",
    "        inputs, outputs, xy, pred = [x, y, z], [], [], []\n",
    "        init = nn.initializers.glorot_normal()\n",
    "\n",
    "        if self.mlp == 'mlp':\n",
    "            for X in inputs:\n",
    "                for fs in self.features[:-1]:\n",
    "                    X = nn.Dense(fs, kernel_init=init)(X)\n",
    "                    # X = nn.activation.tanh(X)\n",
    "                    X = jnp.sin(X)\n",
    "                X = nn.Dense(self.r*self.out_dim, kernel_init=init)(X)\n",
    "                outputs += [jnp.transpose(X, (1, 0))]\n",
    "\n",
    "        elif self.mlp == 'modified_mlp':\n",
    "            for X in inputs:\n",
    "                U = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                V = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                H = jnp.sin(nn.Dense(self.features[0], kernel_init=init)(X))\n",
    "                for fs in self.features[:-1]:\n",
    "                    Z = nn.Dense(fs, kernel_init=init)(H)\n",
    "                    # Z = nn.activation.tanh(Z)\n",
    "                    Z = jnp.sin(Z)\n",
    "                    H = (jnp.ones_like(Z)-Z)*U + Z*V\n",
    "                H = nn.Dense(self.r*self.out_dim, kernel_init=init)(H)\n",
    "                outputs += [jnp.transpose(H, (1, 0))]\n",
    "        \n",
    "        for i in range(self.out_dim):\n",
    "            xy += [jnp.einsum('fx, fy->fxy', outputs[0][self.r*i:self.r*(i+1)], outputs[1][self.r*i:self.r*(i+1)])]\n",
    "            pred += [jnp.einsum('fxy, fz->xyz', xy[i], outputs[-1][self.r*i:self.r*(i+1)])]\n",
    "\n",
    "        if len(pred) == 1:\n",
    "            # 1-dimensional output\n",
    "            return pred[0]\n",
    "        else:\n",
    "            # n-dimensional output\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 2, 3, 4, 5))\n",
    "def generate_train_data(nx, ny, nz, n_max_x, n_max_y, n_max_z):\n",
    "\n",
    "    xc = jnp.linspace(0, n_max_x, nx).reshape(-1, 1)\n",
    "    yc = jnp.linspace(0, n_max_y, ny).reshape(-1, 1)\n",
    "    zc = jnp.linspace(0, n_max_z, nz).reshape(-1, 1)\n",
    "\n",
    "    # # boundary points\n",
    "    xb = [jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=0   bottom\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=2   top\n",
    "          jnp.array([[0.]]),                     # x=0   lateral_1\n",
    "          jnp.array([[n_max_x]]),                     # x=2   lateral_2\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # y=0   lateral_3\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1)] # y=2   lateral_4\n",
    "\n",
    "    yb = [jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_y]])]\n",
    "\n",
    "    zb = [jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_z]]), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1)]\n",
    "\n",
    "    return xc, yc, zc, xb, yb, zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
    "def generate_train_data_random(key, nx, ny, nz, n_max_x, n_max_y, n_max_z, nc, ncx=None, ncy=None, ncz=None, choice=False):\n",
    "    \n",
    "    keys = jax.random.split(key, 4)\n",
    "\n",
    "    if (ncx is not None) and (ncy is not None) and (ncz is not None):\n",
    "      xc = jax.random.uniform(keys[1], (ncx, 1), minval=0., maxval=n_max_x)\n",
    "      yc = jax.random.uniform(keys[2], (ncy, 1), minval=0., maxval=n_max_x)\n",
    "      zc = jax.random.uniform(keys[3], (ncz, 1), minval=0., maxval=n_max_x)\n",
    "    \n",
    "    elif choice is False:\n",
    "      xc = jax.random.uniform(keys[1], (nc, 1), minval=0., maxval=n_max_x)\n",
    "      yc = jax.random.uniform(keys[2], (nc, 1), minval=0., maxval=n_max_x)\n",
    "      zc = jax.random.uniform(keys[3], (nc, 1), minval=0., maxval=n_max_x)\n",
    "    else:\n",
    "      xc = jnp.linspace(0, n_max_x, nx).reshape(-1, 1)\n",
    "      yc = jnp.linspace(0, n_max_y, ny).reshape(-1, 1)\n",
    "      zc = jnp.linspace(0, n_max_z, nz).reshape(-1, 1)\n",
    "      xc = jax.random.choice(keys[1], xc, shape=(nc,))\n",
    "      yc = jax.random.choice(keys[2], yc, shape=(nc,))\n",
    "      zc = jax.random.choice(keys[3], zc, shape=(nc,))\n",
    "\n",
    "    # boundary points\n",
    "    xb = [jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=0   bottom\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # z=2   top\n",
    "          jnp.array([[0.]]),                     # x=0   lateral_1\n",
    "          jnp.array([[n_max_x]]),                     # x=2   lateral_2\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1), # y=0   lateral_3\n",
    "          jnp.linspace(0, n_max_x, nx).reshape(-1, 1)] # y=2   lateral_4\n",
    "\n",
    "    yb = [jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_y, ny).reshape(-1, 1), \n",
    "          jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_y]])]\n",
    "\n",
    "    zb = [jnp.array([[0.]]), \n",
    "          jnp.array([[n_max_z]]), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1), \n",
    "          jnp.linspace(0, n_max_z, nz).reshape(-1, 1)]\n",
    "\n",
    "    return xc, yc, zc, xb, yb, zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def apply_model_spinn(apply_fn, params, train_boundary_data, w_ff, w_div, w_bc):\n",
    "    def residual_loss(params, x, y, z, w_ff, w_div):\n",
    "        # calculate u\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)\n",
    "        \n",
    "        # calculate J\n",
    "        Jx = curlx(apply_fn, params, x, y, z)\n",
    "        Jy = curly(apply_fn, params, x, y, z)\n",
    "        Jz = curlz(apply_fn, params, x, y, z)\n",
    "        J = jnp.stack([Jx, Jy, Jz], axis=-1)\n",
    "\n",
    "        JxB = jnp.cross(J, B, axis=-1) \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # loss_ff = jnp.sum(JxB**2, axis=-1)\n",
    "        # loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        loss_ff = jnp.sum(JxB**2, axis=-1) / (jnp.sum(B**2, axis=-1) + 1e-7)\n",
    "        loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        # loss_ff = jnp.mean(JxB**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # assumes x, y, z have same shape (very important)\n",
    "        vec_x = jnp.ones(x.shape)\n",
    "        vec_y = jnp.ones(y.shape)\n",
    "        vec_z = jnp.ones(z.shape)\n",
    "        \n",
    "        Bx_x = jvp(lambda x: apply_fn(params, x, y, z)[0], (x,), (vec_x,))[1]\n",
    "        # Bx_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec,))[1]\n",
    "        # Bx_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec,))[1]\n",
    "\n",
    "        # By_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec,))[1]\n",
    "        By_y = jvp(lambda y: apply_fn(params, x, y, z)[1], (y,), (vec_y,))[1]\n",
    "        # By_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec,))[1]\n",
    "\n",
    "        # Bz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec,))[1]\n",
    "        # Bz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec,))[1]\n",
    "        Bz_z = jvp(lambda z: apply_fn(params, x, y, z)[2], (z,), (vec_z,))[1]\n",
    "\n",
    "        divB = Bx_x + By_y + Bz_z\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # loss_div = jnp.sum((divB)**2, axis=-1)\n",
    "        # loss_div = jnp.mean(loss_div)\n",
    "\n",
    "        loss_div = jnp.mean((divB)**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        loss = w_ff*loss_ff + w_div*loss_div\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def boundary_loss(params, x, y, z, *boundary_data):\n",
    "        \n",
    "        # loss = 0.\n",
    "        # for i in np.arange(4):\n",
    "        #     boundary_data_batched = boundary_batches[i, :, :, :]\n",
    "        #     xb = boundary_data_batched[:, 0, :][:, 0].reshape(-1, 1)\n",
    "        #     yb = boundary_data_batched[:, 0, :][:, 1].reshape(-1, 1)\n",
    "        #     zb = boundary_data_batched[:, 0, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bx, By, Bz = apply_fn(params, xb, yb, zb)\n",
    "        #     # Bx, By, Bz = Bx.reshape(-1, 1), By.reshape(-1, 1), Bz.reshape(-1, 1)\n",
    "\n",
    "        #     Bxb = boundary_data_batched[:, 1, :][:, 0].reshape(-1, 1)\n",
    "        #     Byb = boundary_data_batched[:, 1, :][:, 1].reshape(-1, 1)\n",
    "        #     Bzb = boundary_data_batched[:, 1, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bxb_mesh, Byb_mesh, Bzb_mesh = jnp.meshgrid(Bxb.ravel(), Byb.ravel(), Bzb.ravel(), indexing='ij')\n",
    "            \n",
    "        #     loss += jnp.mean((Bx - Bxb_mesh)**2) + jnp.mean((By - Byb_mesh)**2) + jnp.mean((Bz - Bzb_mesh)**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  \n",
    "        #2 x=0   lateral_1            \n",
    "        #3 x=2   lateral_2            \n",
    "        #4 y=0   lateral_3            \n",
    "        #5 y=2   lateral_4            \n",
    "\n",
    "        b_bottom, bp_top, bp_lateral_1, bp_lateral_2, bp_lateral_3, bp_lateral_4 = boundary_data\n",
    "        \n",
    "        loss = 0.\n",
    "        Bx, By, Bz = apply_fn(params,  x[0], y[0], z[0])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - b_bottom[:, :, 0])**2) + jnp.mean((By - b_bottom[:, :, 1])**2) + jnp.mean((Bz - b_bottom[:, :, 2])**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  -> Only normal(Bz), Bx=0, By=0\n",
    "        #2 x=0   lateral_1            -> Only tangential(By, Bz), Bx=0\n",
    "        #3 x=2   lateral_2            -> Only tangential(By, Bz), Bx=0\n",
    "        #4 y=0   lateral_3            -> Only tangential(Bx, Bz), By=0\n",
    "        #5 y=2   lateral_4            -> Only tangential(Bx, Bz), By=0\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2) + jnp.mean(By**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(Bx**2)\n",
    "        \n",
    "        # Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(By**2)\n",
    "\n",
    "        # Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        # Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        # loss += jnp.mean(By**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_top[:, :, 0])**2) + jnp.mean((By - bp_top[:, :, 1])**2) + jnp.mean((Bz - bp_top[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_1[:, :, 0])**2) + jnp.mean((By - bp_lateral_1[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_1[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_2[:, :, 0])**2) + jnp.mean((By - bp_lateral_2[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_2[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_3[:, :, 0])**2) + jnp.mean((By - bp_lateral_3[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_3[:, :, 2])**2)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        loss += (1/6)*jnp.mean((Bx - bp_lateral_4[:, :, 0])**2) + jnp.mean((By - bp_lateral_4[:, :, 1])**2) + jnp.mean((Bz - bp_lateral_4[:, :, 2])**2)\n",
    "\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # unpack data\n",
    "    train_data = train_boundary_data[0]\n",
    "    boundary_data = train_boundary_data[1]\n",
    "    xc, yc, zc, xb, yb, zb = train_data\n",
    "\n",
    "    # isolate loss func from redundant arguments\n",
    "    loss_fn = lambda params: residual_loss(params, xc, yc, zc, w_ff, w_div) + w_bc*boundary_loss(params, xb, yb, zb, *boundary_data)\n",
    "\n",
    "    loss, gradient = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0,6))\n",
    "def apply_model_spinn_random(apply_fn, params, train_boundary_data, w_ff, w_div, w_bc, bc_batch_size):\n",
    "    def residual_loss(params, x, y, z, w_ff, w_div):\n",
    "        # calculate u\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)\n",
    "        \n",
    "        # calculate J\n",
    "        Jx = curlx(apply_fn, params, x, y, z)\n",
    "        Jy = curly(apply_fn, params, x, y, z)\n",
    "        Jz = curlz(apply_fn, params, x, y, z)\n",
    "        J = jnp.stack([Jx, Jy, Jz], axis=-1)\n",
    "\n",
    "        JxB = jnp.cross(J, B, axis=-1) \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        # loss_ff = jnp.sum(JxB**2, axis=-1)\n",
    "        # loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        loss_ff = jnp.sum(JxB**2, axis=-1) / (jnp.sum(B**2, axis=-1) + 1e-7)\n",
    "        loss_ff = jnp.mean(loss_ff)\n",
    "\n",
    "        # loss_ff = jnp.mean(JxB**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        # tangent vector dx/dx\n",
    "        # assumes x, y, z have same shape (very important)\n",
    "        vec_x = jnp.ones(x.shape)\n",
    "        vec_y = jnp.ones(y.shape)\n",
    "        vec_z = jnp.ones(z.shape)\n",
    "        \n",
    "        Bx_x = jvp(lambda x: apply_fn(params, x, y, z)[0], (x,), (vec_x,))[1]\n",
    "        # Bx_y = jvp(lambda y: apply_fn(params, x, y, z)[0], (y,), (vec,))[1]\n",
    "        # Bx_z = jvp(lambda z: apply_fn(params, x, y, z)[0], (z,), (vec,))[1]\n",
    "\n",
    "        # By_x = jvp(lambda x: apply_fn(params, x, y, z)[1], (x,), (vec,))[1]\n",
    "        By_y = jvp(lambda y: apply_fn(params, x, y, z)[1], (y,), (vec_y,))[1]\n",
    "        # By_z = jvp(lambda z: apply_fn(params, x, y, z)[1], (z,), (vec,))[1]\n",
    "\n",
    "        # Bz_x = jvp(lambda x: apply_fn(params, x, y, z)[2], (x,), (vec,))[1]\n",
    "        # Bz_y = jvp(lambda y: apply_fn(params, x, y, z)[2], (y,), (vec,))[1]\n",
    "        Bz_z = jvp(lambda z: apply_fn(params, x, y, z)[2], (z,), (vec_z,))[1]\n",
    "\n",
    "        divB = Bx_x + By_y + Bz_z\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # loss_div = jnp.sum((divB)**2, axis=-1)\n",
    "        # loss_div = jnp.mean(loss_div)\n",
    "\n",
    "        loss_div = jnp.mean((divB)**2)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        loss = w_ff*loss_ff + w_div*loss_div\n",
    "\n",
    "        return loss, loss_ff, loss_div\n",
    "\n",
    "    def boundary_loss(params, x, y, z, bc_batch_size, *boundary_data):\n",
    "        \n",
    "        # loss = 0.\n",
    "        # for i in np.arange(4):\n",
    "        #     boundary_data_batched = boundary_batches[i, :, :, :]\n",
    "        #     xb = boundary_data_batched[:, 0, :][:, 0].reshape(-1, 1)\n",
    "        #     yb = boundary_data_batched[:, 0, :][:, 1].reshape(-1, 1)\n",
    "        #     zb = boundary_data_batched[:, 0, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bx, By, Bz = apply_fn(params, xb, yb, zb)\n",
    "        #     # Bx, By, Bz = Bx.reshape(-1, 1), By.reshape(-1, 1), Bz.reshape(-1, 1)\n",
    "\n",
    "        #     Bxb = boundary_data_batched[:, 1, :][:, 0].reshape(-1, 1)\n",
    "        #     Byb = boundary_data_batched[:, 1, :][:, 1].reshape(-1, 1)\n",
    "        #     Bzb = boundary_data_batched[:, 1, :][:, 2].reshape(-1, 1)\n",
    "\n",
    "        #     Bxb_mesh, Byb_mesh, Bzb_mesh = jnp.meshgrid(Bxb.ravel(), Byb.ravel(), Bzb.ravel(), indexing='ij')\n",
    "            \n",
    "        #     loss += jnp.mean((Bx - Bxb_mesh)**2) + jnp.mean((By - Byb_mesh)**2) + jnp.mean((Bz - Bzb_mesh)**2)\n",
    "\n",
    "        #0 z=0   bottom\n",
    "        #1 z=2   top                  \n",
    "        #2 x=0   lateral_1            \n",
    "        #3 x=2   lateral_2            \n",
    "        #4 y=0   lateral_3            \n",
    "        #5 y=2   lateral_4            \n",
    "\n",
    "        b_bottom, bp_top, bp_lateral_1, bp_lateral_2, bp_lateral_3, bp_lateral_4 = boundary_data\n",
    "\n",
    "        loss = 0.\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[0], y[0], z[0])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "        \n",
    "        bc_bottom_training_data = jnp.vstack([Bx.flatten(), b_bottom[:, :, 0].flatten()])\n",
    "        bc_bottom_training_data = jnp.concatenate([bc_bottom_training_data, jnp.vstack([By.flatten(), b_bottom[:, :, 1].flatten()])], axis=1)\n",
    "        bc_bottom_training_data = jnp.concatenate([bc_bottom_training_data, jnp.vstack([Bz.flatten(), b_bottom[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[1], y[1], z[1])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.vstack([Bx.flatten(), bp_top[:, :, 0].flatten()])\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_top[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_top[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[2], y[2], z[2])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_1[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_1[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_1[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[3], y[3], z[3])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_2[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_2[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_2[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[4], y[4], z[4])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_3[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_3[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_3[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params,  x[5], y[5], z[5])\n",
    "        Bx, By, Bz = jnp.squeeze(Bx), jnp.squeeze(By), jnp.squeeze(Bz)\n",
    "\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bx.flatten(), bp_lateral_4[:, :, 0].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([By.flatten(), bp_lateral_4[:, :, 1].flatten()])], axis=1)\n",
    "        bc_training_data = jnp.concatenate([bc_training_data, jnp.vstack([Bz.flatten(), bp_lateral_4[:, :, 2].flatten()])], axis=1)\n",
    "\n",
    "        bc_latent_batch_size = bc_batch_size // 6\n",
    "        bc_bottom_batch_size = bc_batch_size - bc_latent_batch_size\n",
    "\n",
    "        M = bc_training_data.shape[-1]\n",
    "        random_indices = random.sample(range(M), bc_latent_batch_size)\n",
    "        bc_training_data = bc_training_data[:, random_indices]\n",
    "        loss += jnp.mean((bc_training_data[0] - bc_training_data[1])**2)\n",
    "\n",
    "        M =  bc_bottom_training_data.shape[-1]\n",
    "        random_indices = random.sample(range(M), bc_bottom_batch_size)\n",
    "        bc_bottom_training_data = bc_bottom_training_data[:, random_indices]\n",
    "        loss += jnp.mean((bc_bottom_training_data[0] - bc_bottom_training_data[1])**2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # unpack data\n",
    "    train_data = train_boundary_data[0]\n",
    "    boundary_data = train_boundary_data[1]\n",
    "    xc, yc, zc, xb, yb, zb = train_data\n",
    "\n",
    "    # isolate loss func from redundant arguments\n",
    "    loss_pde, loss_ff, loss_div = residual_loss(params, xc, yc, zc, w_ff, w_div)\n",
    "    loss_bc = boundary_loss(params, xb, yb, zb, bc_batch_size, *boundary_data)\n",
    "\n",
    "    loss_fn = lambda params: residual_loss(params, xc, yc, zc, w_ff, w_div)[0] + w_bc*boundary_loss(params, xb, yb, zb, bc_batch_size, *boundary_data)\n",
    "\n",
    "    loss, gradient = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    return loss, gradient, [loss_ff, loss_div, loss_bc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spinn_cube:\n",
    "    def __init__(self, param_path, parameters_path):\n",
    "        self.param_path = param_path\n",
    "        self.parameters_path = parameters_path\n",
    "    \n",
    "    def calculate_magnetic_fields(self):\n",
    "        param_path = self.param_path\n",
    "        parameters_path = self.parameters_path\n",
    "\n",
    "        with open(parameters_path, \"rb\") as f:\n",
    "            parameters = pickle.load(f)\n",
    "\n",
    "        features = parameters['features']\n",
    "        n_layers = parameters['n_layers']\n",
    "        feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "\n",
    "        subkey = jax.random.PRNGKey(0)\n",
    "        model = SPINN3d(feat_sizes, r, out_dim, pos_enc=pos_enc, mlp=mlp)\n",
    "        model.init(\n",
    "                    subkey,\n",
    "                    jnp.ones((Nx, 1)),\n",
    "                    jnp.ones((Ny, 1)),\n",
    "                    jnp.ones((Nz, 1))\n",
    "                   )\n",
    "        apply_fn = jax.jit(model.apply)\n",
    "\n",
    "        with open(param_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        x = jnp.linspace(0, n_max_x, Nx).reshape(-1, 1)\n",
    "        y = jnp.linspace(0, n_max_y, Ny).reshape(-1, 1)\n",
    "        z = jnp.linspace(0, n_max_z, Nz).reshape(-1, 1)\n",
    "        x, y, z = jax.lax.stop_gradient(x), jax.lax.stop_gradient(y), jax.lax.stop_gradient(z)\n",
    "\n",
    "        Bx, By, Bz = apply_fn(params, x, y, z)\n",
    "        B = jnp.stack([Bx, By, Bz], axis=-1)*b_norm\n",
    "        \n",
    "        Bx = B[..., 0]\n",
    "        By = B[..., 1]\n",
    "        Bz = B[..., 2]\n",
    "\n",
    "        co_bounds = (0, Nx-1, 0, Ny-1, 0, Nz-1)\n",
    "        co_coords = create_coordinates(co_bounds).reshape(-1, 3)\n",
    "        co_coord = co_coords.reshape(Nx, Ny, Nz, 3)\n",
    "        x = co_coord[..., 0]\n",
    "        y = co_coord[..., 1]\n",
    "        z = co_coord[..., 2]\n",
    "        mesh = pv.StructuredGrid(x, y, z)\n",
    "        vectors = np.stack([Bx, By, Bz], axis=-1).transpose(2, 1, 0, 3).reshape(-1, 3)\n",
    "        mesh['B'] = vectors\n",
    "        mesh.active_vectors_name = 'B'\n",
    "        magnitude = np.linalg.norm(vectors, axis=-1)\n",
    "        mesh['mag'] = magnitude\n",
    "        mesh.active_scalars_name = 'mag'\n",
    "\n",
    "        self.grid = mesh \n",
    "        return self.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPINN_series_Trainer:\n",
    "    def __init__(self, output_path, wdb, single=False):\n",
    "        self.parameters = wdb.config\n",
    "        \n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        for hdlr in logger.handlers[:]:\n",
    "            logger.removeHandler(hdlr)\n",
    "        logger.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(output_path, \"info_log\"))) \n",
    "        logger.addHandler(logging.StreamHandler()) \n",
    "        self.logger = logger\n",
    "\n",
    "        logger.info(parameters)\n",
    "\n",
    "        parameters_path = os.path.join(output_path, \"parameters.pickle\")\n",
    "        with open(parameters_path, \"wb\") as f:\n",
    "            pickle.dump(parameters, f)\n",
    "\n",
    "        self.result_path = output_path\n",
    "\n",
    "        self.wandb = wdb\n",
    "        self.single = single\n",
    "        \n",
    "    def setup(self, b_bottom_list):\n",
    "\n",
    "        single = self.single\n",
    "\n",
    "        parameters = self.parameters\n",
    "\n",
    "        features = parameters['features']\n",
    "        n_layers = parameters['n_layers']\n",
    "        feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        lr = parameters['lr']\n",
    "        lr_decay_iterations = parameters['lr_decay_iterations']\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "        is_random = parameters['is_random']\n",
    "        Nc = parameters['Nc']\n",
    "        decay_rate = parameters['decay_rate']\n",
    "\n",
    "        seed = 111\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        self.subkey = subkey\n",
    "\n",
    "        model = SPINN3d(feat_sizes, r, out_dim, pos_enc=pos_enc, mlp=mlp)\n",
    "\n",
    "        params = model.init(\n",
    "                        subkey,\n",
    "                        jnp.ones((Nx, 1)),\n",
    "                        jnp.ones((Ny, 1)),\n",
    "                        jnp.ones((Nz, 1))\n",
    "                    )\n",
    "        apply_fn = jax.jit(model.apply)\n",
    "        self.apply_fn = apply_fn\n",
    "\n",
    "        b_bottom_path = b_bottom_list[0]\n",
    "        self.b_bottom_path = b_bottom_path\n",
    "        with open(b_bottom_path, 'rb') as f:\n",
    "            b_bottom = np.load(f)\n",
    "        self.b_bottom = b_bottom\n",
    "\n",
    "        b_bottom_date = os.path.basename(b_bottom_path)[9:-4]\n",
    "        self.output_path  = os.path.join(self.result_path, b_bottom_date)\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "\n",
    "        final_params_path = os.path.join(self.output_path, f\"final_params.pickle\")\n",
    "\n",
    "        if os.path.exists(final_params_path) and (single is False):\n",
    "            with open(final_params_path, 'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "        \n",
    "        if lr_decay_iterations is not None: \n",
    "            optim = optax.adam(learning_rate=optax.exponential_decay(init_value=lr, transition_steps=lr_decay_iterations,\n",
    "                                                                     decay_rate=decay_rate))\n",
    "        else:\n",
    "            optim = optax.adam(learning_rate=lr)\n",
    "\n",
    "        state = optim.init(params)\n",
    "\n",
    "        self.optim = optim\n",
    "        \n",
    "        self.b_bottom_list = b_bottom_list\n",
    "\n",
    "        if os.path.exists(final_params_path) and (single is False):\n",
    "            pass\n",
    "        else:\n",
    "            parameters = self.parameters\n",
    "            subkey = self.subkey\n",
    "            logger = self.logger\n",
    "        \n",
    "            BC_path = os.path.join(self.output_path, 'BC.pickle')\n",
    "            if not os.path.exists(BC_path):\n",
    "                Nz = parameters['Nz']\n",
    "                b_norm = parameters['b_norm']\n",
    "                cal_and_save_potential_boundary_for_spinn(b_bottom, Nz, b_norm, BC_path, potential_boundary_batch_size) \n",
    "                \n",
    "                import torch\n",
    "                torch.cuda.empty_cache()  \n",
    "\n",
    "            with open(BC_path, 'rb') as f:\n",
    "                boundary_data = pickle.load(f)\n",
    "\n",
    "            self.boundary_data = boundary_data\n",
    "\n",
    "            is_random = parameters['is_random']\n",
    "\n",
    "            Nx = parameters['Nx']\n",
    "            Ny = parameters['Ny']\n",
    "            Nz = parameters['Nz']\n",
    "            b_norm = parameters['b_norm']\n",
    "            n_max_x = parameters['n_max_x']\n",
    "            n_max_y = parameters['n_max_y']\n",
    "            n_max_z = parameters['n_max_z']\n",
    "            is_random = parameters['is_random']\n",
    "\n",
    "            if is_random is True:\n",
    "                Nc = parameters['Nc']\n",
    "                Ncx = parameters['Ncx']\n",
    "                Ncy = parameters['Ncy']\n",
    "                Ncz = parameters['Ncz']\n",
    "                choice = parameters['choice']\n",
    "                train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "            else:\n",
    "                train_data = generate_train_data(Nx, Ny, Nz, n_max_x, n_max_y, n_max_z)\n",
    "\n",
    "            self.train_boundary_data = [train_data, boundary_data]\n",
    "\n",
    "            # losses = []\n",
    "            logger.info('Complie Start')\n",
    "\n",
    "            w_ff = parameters['w_ff']\n",
    "            w_div = parameters['w_div']\n",
    "            w_bc = parameters['w_bc']\n",
    "            w_bc_decay_iterations = parameters['w_bc_decay_iterations']\n",
    "            w_bc_decay = (1 / w_bc) ** (1 / w_bc_decay_iterations) if w_bc_decay_iterations is not None else 1\n",
    "\n",
    "            bc_batch_size = parameters['bc_batch_size']\n",
    "\n",
    "            start = time.time()\n",
    "            if bc_batch_size is not None:\n",
    "                loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "            else:\n",
    "                loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "\n",
    "            # losses.append(loss.item())\n",
    "            loss_ff, loss_div, loss_bc = losses\n",
    "            self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item()}})\n",
    "            params, state = update_model(self.optim, gradient, params, state)\n",
    "\n",
    "            if w_bc > 1:\n",
    "                w_bc *= w_bc_decay\n",
    "                if w_bc <= 1:\n",
    "                    w_bc = 1\n",
    "\n",
    "            if is_random is True:\n",
    "                train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                self.train_boundary_data = [train_data, self.boundary_data]\n",
    "            \n",
    "            if bc_batch_size is not None:\n",
    "                loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "            else:\n",
    "                loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "            # losses.append(loss.item())\n",
    "            loss_ff, loss_div, loss_bc = losses\n",
    "            self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item()}})\n",
    "            params, state = update_model(self.optim, gradient, params, state)\n",
    "            runtime = time.time() - start\n",
    "    \n",
    "            logger.info(f'Complie End --> total: {runtime:.2f}sec')\n",
    "\n",
    "            # self.losses = losses\n",
    "        \n",
    "        self.state = state\n",
    "        self.params = params\n",
    "        self.key = key\n",
    "        self.single = single\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        single = self.single\n",
    "\n",
    "        key = self.key\n",
    "        \n",
    "        logger = self.logger\n",
    "        params = self.params\n",
    "        state = self.state\n",
    "        parameters = self.parameters\n",
    "        \n",
    "        total_iterations = parameters['total_iterations']\n",
    "        log_iterations = parameters['log_interval']\n",
    "\n",
    "        series_total_iterations = parameters['series_iterations']\n",
    "        series_log_iterations = parameters['series_log_interval']\n",
    "\n",
    "        loss_threshold = parameters['loss_threshold']\n",
    "        random_interval = parameters['random_interval']\n",
    "        features = parameters['features']\n",
    "        n_layers = parameters['n_layers']\n",
    "        feat_sizes = tuple([features for _ in range(n_layers)]) \n",
    "        r = parameters['r']\n",
    "        out_dim = parameters['out_dim']\n",
    "        Nx = parameters['Nx']\n",
    "        Ny = parameters['Ny']\n",
    "        Nz = parameters['Nz']\n",
    "        b_norm = parameters['b_norm']\n",
    "        pos_enc = parameters['pos_enc']\n",
    "        mlp = parameters['mlp']\n",
    "        lr = parameters['lr']\n",
    "        series_lr = parameters['series_lr']\n",
    "        series_lr_decay_iterations = parameters['series_lr_decay_iterations']\n",
    "        lr_decay_iterations = parameters['lr_decay_iterations']\n",
    "        n_max_x = parameters['n_max_x']\n",
    "        n_max_y = parameters['n_max_y']\n",
    "        n_max_z = parameters['n_max_z']\n",
    "        is_random = parameters['is_random']\n",
    "        Nc = parameters['Nc']\n",
    "        decay_rate = parameters['decay_rate']\n",
    "        w_ff = parameters['w_ff']\n",
    "        w_div = parameters['w_div']\n",
    "        w_bc = parameters['w_bc']\n",
    "        w_bc_decay_iterations = parameters['w_bc_decay_iterations']\n",
    "        potential_boundary_batch_size = parameters['potential_boundary_batch_size']\n",
    "        w_bc_decay = (1 / w_bc) ** (1 / w_bc_decay_iterations) if w_bc_decay_iterations is not None else 1\n",
    "\n",
    "        bc_batch_size = parameters['bc_batch_size']\n",
    "\n",
    "        b_bottom_list = self.b_bottom_list\n",
    "\n",
    "        final_params_path = os.path.join(self.output_path, f\"final_params.pickle\")\n",
    "\n",
    "        if (not os.path.exists(final_params_path)) or (single is True):\n",
    "            # losses = self.losses\n",
    "\n",
    "            start = time.time()\n",
    "            for e in trange(1, total_iterations + 1):\n",
    "\n",
    "                if w_bc > 1:\n",
    "                    w_bc *= w_bc_decay\n",
    "                    if w_bc <= 1:\n",
    "                        w_bc = 1\n",
    "\n",
    "                if is_random is True:\n",
    "                    if e % random_interval == 0:\n",
    "                        # sample new input data\n",
    "                        key, subkey = jax.random.split(key, 2)\n",
    "                        Nc = parameters['Nc']\n",
    "                        Ncx = parameters['Ncx']\n",
    "                        Ncy = parameters['Ncy']\n",
    "                        Ncz = parameters['Ncz']\n",
    "                        choice = parameters['choice']\n",
    "                        train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                        self.train_boundary_data = [train_data, self.boundary_data]\n",
    "\n",
    "                if bc_batch_size is not None:\n",
    "                    loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "                else:\n",
    "                    loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "                # losses.append(loss.item())\n",
    "                loss_ff, loss_div, loss_bc = losses\n",
    "                self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item()}})\n",
    "                if loss.item() < loss_threshold:\n",
    "                    if logger is None:\n",
    "                        print(f'Epoch: {e}/{total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                    else:\n",
    "                        logger.info(f'Epoch: {e}/{total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                    break\n",
    "                \n",
    "                params, state = update_model(self.optim, gradient, params, state)\n",
    "                \n",
    "                if e % log_iterations == 0:\n",
    "                    div, ff, b_diff = self.validation(params)\n",
    "                    logger.info(f'Epoch: {e}/{total_iterations} --> total loss: {loss:.8f}, div {div:.8f}, ff {ff:.8f}, b_diff {b_diff:.8f}')\n",
    "\n",
    "            with open(final_params_path, \"wb\") as f:\n",
    "                pickle.dump(params, f)\n",
    "\n",
    "            # np.save(os.path.join(self.output_path, 'losses.npy'), losses)\n",
    "            # with open(os.path.join(self.output_path, 'losses.npy'), \"rb\") as f:\n",
    "            #     losses = np.load(f)\n",
    "\n",
    "            # fig, ax = plt.subplots(figsize=(4,3))\n",
    "            # ax.plot(losses)\n",
    "            # ax.set_xlabel('Iteration')\n",
    "            # ax.set_ylabel('Loss')\n",
    "            # ax.set_title('SPINN')\n",
    "            # plt.tight_layout()\n",
    "            # plt.savefig(os.path.join(self.output_path, 'loss_SPINN.png'), dpi=300)\n",
    "\n",
    "            runtime = time.time() - start\n",
    "            logger.info(f'Runtime --> total: {runtime:.2f}sec ({(runtime/(total_iterations-1)*1000):.2f}ms/iter.)')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        for b_bottom_path in b_bottom_list[1:]:\n",
    "            b_bottom_date = os.path.basename(b_bottom_path)[9:-4]\n",
    "            output_path = os.path.join(self.result_path, b_bottom_date)    \n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            final_params_path = os.path.join(output_path, f\"final_params.pickle\")\n",
    "\n",
    "            if os.path.exists(final_params_path):\n",
    "                with open(final_params_path, 'rb') as f:\n",
    "                    params = pickle.load(f)\n",
    "\n",
    "            if not os.path.exists(final_params_path):\n",
    "\n",
    "                if series_lr_decay_iterations is not None: \n",
    "                    optim = optax.adam(learning_rate=optax.exponential_decay(init_value=series_lr, transition_steps=series_lr_decay_iterations,\n",
    "                                                                                decay_rate=decay_rate))\n",
    "                else:\n",
    "                    optim = optax.adam(learning_rate=series_lr)\n",
    "\n",
    "                with open(b_bottom_path, 'rb') as f:\n",
    "                    b_bottom = np.load(f)\n",
    "\n",
    "\n",
    "                BC_path = os.path.join(output_path, 'BC.pickle')\n",
    "                if not os.path.exists(BC_path):\n",
    "                    Nz = parameters['Nz']\n",
    "                    b_norm = parameters['b_norm']\n",
    "                    cal_and_save_potential_boundary_for_spinn(b_bottom, Nz, b_norm, BC_path, potential_boundary_batch_size)  \n",
    "\n",
    "                with open(BC_path, 'rb') as f:\n",
    "                    boundary_data = pickle.load(f)\n",
    "\n",
    "                if is_random is True:\n",
    "                    train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                else:\n",
    "                    train_data = generate_train_data(Nx, Ny, Nz, n_max_x, n_max_y, n_max_z)\n",
    "\n",
    "                self.train_boundary_data = [train_data, boundary_data]\n",
    "\n",
    "                losses = []\n",
    "                start = time.time()\n",
    "                for e in trange(1, series_total_iterations + 1):\n",
    "\n",
    "                    if w_bc > 1:\n",
    "                        w_bc *= w_bc_decay\n",
    "                        if w_bc <= 1:\n",
    "                            w_bc = 1\n",
    "\n",
    "                    if is_random is True:\n",
    "                        if e % random_interval == 0:\n",
    "                            # sample new input data\n",
    "                            key, subkey = jax.random.split(key, 2)\n",
    "                            Nc = parameters['Nc']\n",
    "                            Ncx = parameters['Ncx']\n",
    "                            Ncy = parameters['Ncy']\n",
    "                            Ncz = parameters['Ncz']\n",
    "                            choice = parameters['choice']\n",
    "                            train_data = generate_train_data_random(subkey, Nx, Ny, Nz, n_max_x, n_max_y, n_max_z, Nc, Ncx, Ncy, Ncz, choice)\n",
    "                            self.train_boundary_data = [train_data, self.boundary_data]\n",
    "\n",
    "                    if bc_batch_size is not None:\n",
    "                        loss, gradient, losses = apply_model_spinn_random(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc, bc_batch_size)\n",
    "                    else:\n",
    "                        loss, gradient, losses = apply_model_spinn(self.apply_fn, params, self.train_boundary_data, w_ff, w_div, w_bc)\n",
    "                    # losses.append(loss.item())\n",
    "                    loss_ff, loss_div, loss_bc = losses\n",
    "                    self.wandb.log({\"train\": {\"loss\":loss.item(), \"loss_ff\":loss_ff.item(), \"loss_div\":loss_div.item(), \"loss_bc\":loss_bc.item()}})\n",
    "                    if loss.item() < loss_threshold:\n",
    "                        if logger is None:\n",
    "                            print(f'Epoch: {e}/{series_total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                        else:\n",
    "                            logger.info(f'Epoch: {e}/{series_total_iterations} --> loss: {loss:.8f} < {loss_threshold}')\n",
    "                        break\n",
    "                    \n",
    "                    params, state = update_model(optim, gradient, params, state)\n",
    "                    \n",
    "                    if e % series_log_iterations == 0:\n",
    "                        div, ff, b_diff = self.validation(params)\n",
    "                        logger.info(f'Epoch: {e}/{series_total_iterations} --> total loss: {loss:.8f}, div {div:.8f}, ff {ff:.8f}, b_diff {b_diff:.8f}')\n",
    "\n",
    "                with open(final_params_path, \"wb\") as f:\n",
    "                    pickle.dump(params, f)\n",
    "\n",
    "                # np.save(os.path.join(output_path, 'losses.npy'), losses)\n",
    "\n",
    "                runtime = time.time() - start\n",
    "                logger.info(f'Runtime --> total: {runtime:.2f}sec ({(runtime/(series_total_iterations-1)*1000):.2f}ms/iter.)')\n",
    "            else:\n",
    "                # with open(os.path.join(output_path, 'losses.npy'), \"rb\") as f:\n",
    "                #     losses = np.load(f)\n",
    "\n",
    "                # fig, ax = plt.subplots(figsize=(4,3))\n",
    "                # ax.plot(losses)\n",
    "                # ax.set_xlabel('Iteration')\n",
    "                # ax.set_ylabel('Loss')\n",
    "                # ax.set_title('SPINN')\n",
    "                # plt.tight_layout()\n",
    "                # plt.savefig(os.path.join(output_path, 'loss_SPINN.png'), dpi=300)\n",
    "                pass\n",
    "\n",
    "    def validation(self, params):\n",
    "        b_norm = self.parameters['b_norm']\n",
    "        # pot_me = self.parameters['pot_me']\n",
    "        # dV = self.parameters['dV']\n",
    "\n",
    "        x = jnp.linspace(0, n_max_x, Nx).reshape(-1, 1)\n",
    "        y = jnp.linspace(0, n_max_y, Ny).reshape(-1, 1)\n",
    "        z = jnp.linspace(0, n_max_z, Nz).reshape(-1, 1)\n",
    "        x, y, z = jax.lax.stop_gradient(x), jax.lax.stop_gradient(y), jax.lax.stop_gradient(z)\n",
    "\n",
    "        Bx, By, Bz = self.apply_fn(params, x, y, z)\n",
    "        B = np.stack([Bx, By, Bz], axis=-1)*b_norm\n",
    "        J = curl(B)\n",
    "\n",
    "        div = (divergence(B)**2).mean()\n",
    "\n",
    "        JxB = np.cross(J, B)\n",
    "        ff = np.sum(JxB**2, axis=-1) / (np.sum(B**2, axis=-1) + 1e-7)\n",
    "        ff = np.mean(ff)\n",
    "\n",
    "        b_diff = np.mean((B[:, :, 0, :]/b_norm - b_bottom/b_norm)**2)\n",
    "\n",
    "        jmaps = magnitude(J).sum(2)\n",
    "\n",
    "        acc = div+ff+b_diff\n",
    "        self.wandb.log({\"val\": {\"div\":div, \"ff\":ff, \"b_diff\":b_diff, \"acc\":acc}})\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(jmaps.T, origin='lower')\n",
    "        fig.colorbar(im)\n",
    "        self.wandb.log({\"plot\": {\"jmap\":fig}})\n",
    "        plt.close()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(B[:, :, 0, 2].T, origin='lower', cmap='gray')\n",
    "        fig.colorbar(im)\n",
    "        self.wandb.log({\"plot\": {\"Bz\":fig}})\n",
    "        plt.close()\n",
    "\n",
    "        return div, ff, b_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgjeon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/userhome/jeon_mg/workspace/workspace_mine/_data/NOAA12673/b_bottom/b_bottom_20170906_083600.npy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = os.path.expanduser('~/workspace/workspace_mine/_data/NOAA12673/')\n",
    "\n",
    "result_path = os.path.join(base_path, 'result')\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "b_bottom_path = os.path.join(base_path, 'b_bottom')\n",
    "\n",
    "b_bottom_list = sorted(glob.glob(os.path.join(b_bottom_path, '*.npy')))\n",
    "b_bottom_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nz = 160\n",
    "\n",
    "# b_norm = 2500\n",
    "# total_iterations = 10000\n",
    "# log_interval = 1000\n",
    "# loss_threshold = 1e-3\n",
    "\n",
    "# series_iterations = 200\n",
    "# series_log_interval = 20\n",
    "# series_lr = 5e-5\n",
    "# series_lr_decay_iterations = series_iterations\n",
    "\n",
    "# with open(b_bottom_list[0], 'rb') as f:\n",
    "#     b_bottom = np.load(f)\n",
    "\n",
    "# Nx, Ny, _ = b_bottom.shape\n",
    "\n",
    "# features = 256\n",
    "# n_layers = 3\n",
    "# r = 256\n",
    "# mlp = 'modified_mlp'\n",
    "\n",
    "# out_dim = 3 \n",
    "# lr = 5e-5\n",
    "# lr_decay_iterations = total_iterations\n",
    "# decay_rate = 0.98\n",
    "\n",
    "# pos_enc = 0\n",
    "\n",
    "# spatial_norm = 160\n",
    "# n_max_x = (Nx/spatial_norm)\n",
    "# n_max_y = (Ny/spatial_norm)\n",
    "# n_max_z = (Nz/spatial_norm)\n",
    "\n",
    "# w_ff = 0.1\n",
    "# w_div = 1\n",
    "\n",
    "# w_bc = 1\n",
    "# w_bc_decay_iterations = None\n",
    "\n",
    "# is_random = True\n",
    "# Nc = 32\n",
    "# random_interval = 1\n",
    "# Ncx = None\n",
    "# Ncy = None\n",
    "# Ncz = None\n",
    "\n",
    "# bc_batch_size = 10000\n",
    "\n",
    "# choice = True\n",
    "\n",
    "# potential_boundary_batch_size = 3000\n",
    "\n",
    "# parameters = {'features' : features, \n",
    "#     'n_layers' : n_layers, \n",
    "#     'r' : r, \n",
    "#     'out_dim' : out_dim, \n",
    "#     'Nx' : Nx, \n",
    "#     'Ny' : Ny, \n",
    "#     'Nz' : Nz, \n",
    "#     'b_norm' : b_norm,\n",
    "#     'pos_enc' : pos_enc,\n",
    "#     'mlp' : mlp,\n",
    "#     'lr': lr,\n",
    "#     'series_lr': series_lr,\n",
    "#     'series_lr_decay_iterations': series_lr_decay_iterations,\n",
    "#     'lr_decay_iterations': lr_decay_iterations,\n",
    "#     'n_max_x': n_max_x,\n",
    "#     'n_max_y': n_max_y,\n",
    "#     'n_max_z': n_max_z,\n",
    "#     'is_random':is_random,\n",
    "#     'Nc':Nc,\n",
    "#     'random_interval':random_interval,\n",
    "#     'w_ff': w_ff,\n",
    "#     'w_div': w_div,\n",
    "#     'w_bc': w_bc,\n",
    "#     'w_bc_decay_iterations': w_bc_decay_iterations,\n",
    "#     'Ncx': Ncx,\n",
    "#     'Ncy': Ncy,\n",
    "#     'Ncz': Ncz,\n",
    "#     'bc_batch_size': bc_batch_size,\n",
    "#     'choice': choice,\n",
    "#     'decay_rate': decay_rate,\n",
    "#     'total_iterations': total_iterations,\n",
    "#     'log_interval': log_interval,\n",
    "#     'series_iterations': series_iterations,\n",
    "#     'series_log_interval': series_log_interval,\n",
    "#     'loss_threshold': loss_threshold,\n",
    "#     'potential_boundary_batch_size': potential_boundary_batch_size}\n",
    "\n",
    "# wandb.init(\n",
    "#       project=\"basic-intro\",\n",
    "#       config=parameters\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#       project=\"basic-intro\",\n",
    "#       config=parameters\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = SPINN_series_Trainer(result_path, wandb, single=True)\n",
    "# trainer.setup(b_bottom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    'name': 'acc',\n",
    "    'goal': 'minimize'   \n",
    "}\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "\n",
    "with open(b_bottom_list[0], 'rb') as f:\n",
    "    b_bottom = np.load(f)\n",
    "\n",
    "Nx, Ny, _ = b_bottom.shape\n",
    "Nz = 160\n",
    "b_norm = 2500\n",
    "total_iterations = 10000\n",
    "log_interval = 1000\n",
    "loss_threshold = 1e-3\n",
    "\n",
    "series_iterations = 200\n",
    "series_log_interval = 20\n",
    "series_lr = 5e-5\n",
    "series_lr_decay_iterations = series_iterations\n",
    "\n",
    "out_dim = 3 \n",
    "lr_decay_iterations = total_iterations\n",
    "\n",
    "pos_enc = 0\n",
    "\n",
    "spatial_norm = 160\n",
    "n_max_x = (Nx/spatial_norm)\n",
    "n_max_y = (Ny/spatial_norm)\n",
    "n_max_z = (Nz/spatial_norm)\n",
    "\n",
    "Ncx = None\n",
    "Ncy = None\n",
    "Ncz = None\n",
    "\n",
    "potential_boundary_batch_size = 3000\n",
    "\n",
    "# parameters = {\n",
    "#     'features' : {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 128,\n",
    "#         'max': 512\n",
    "#     }, \n",
    "#     'n_layers' : {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 3,\n",
    "#         'max': 8\n",
    "#     }, \n",
    "#     'r' : {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 128,\n",
    "#         'max': 512\n",
    "#     }, \n",
    "#     'out_dim' : {\n",
    "#         'value': out_dim\n",
    "#     }, \n",
    "#     'Nx' : {\n",
    "#         'value': Nx\n",
    "#     }, \n",
    "#     'Ny' : {\n",
    "#         'value': Ny\n",
    "#     }, \n",
    "#     'Nz' : {\n",
    "#         'value': Nz\n",
    "#     }, \n",
    "#     'b_norm' : {\n",
    "#         'value': b_norm\n",
    "#     },\n",
    "#     'pos_enc' : {\n",
    "#         'value': pos_enc\n",
    "#     },\n",
    "#     'mlp' : {\n",
    "#         'values': ['mlp', 'modified_mlp']\n",
    "#     },\n",
    "#     'lr': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-5,\n",
    "#         'max': 1e-3\n",
    "#     },\n",
    "#     'series_lr': {\n",
    "#         'value': series_lr\n",
    "#     },\n",
    "#     'series_lr_decay_iterations': {\n",
    "#         'value': series_lr_decay_iterations\n",
    "#     },\n",
    "#     'lr_decay_iterations': {\n",
    "#         'values': [None, lr_decay_iterations//2, lr_decay_iterations]\n",
    "#     },\n",
    "#     'n_max_x': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 4*n_max_x\n",
    "#     },\n",
    "#     'n_max_y': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 4*n_max_y\n",
    "#     },\n",
    "#     'n_max_z': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 4*n_max_z\n",
    "#     },\n",
    "#     'is_random':{\n",
    "#         'values': [True, False]\n",
    "#     },\n",
    "#     'Nc':{\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 16,\n",
    "#         'max': max(Nx, Ny, Nz)\n",
    "#     },\n",
    "#     'random_interval':{\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 1,\n",
    "#         'max': 1000\n",
    "#     },\n",
    "#     'w_ff': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-4,\n",
    "#         'max': 1e3\n",
    "#     },\n",
    "#     'w_div': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-4,\n",
    "#         'max': 1e3\n",
    "#     },\n",
    "#     'w_bc': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 1e-4,\n",
    "#         'max': 1e3\n",
    "#     },\n",
    "#     'w_bc_decay_iterations': {\n",
    "#         'values': [None, total_iterations, 2*total_iterations, 3*total_iterations]\n",
    "#     },\n",
    "#     'Ncx': {\n",
    "#         'value': Ncx\n",
    "#     },\n",
    "#     'Ncy': {\n",
    "#         'value': Ncy\n",
    "#     },\n",
    "#     'Ncz': {\n",
    "#         'value': Ncz\n",
    "#     },\n",
    "#     'bc_batch_size': {\n",
    "#         'distribution': 'int_uniform',\n",
    "#         'min': 10000,\n",
    "#         'max': 50000\n",
    "#     },\n",
    "#     'choice': {\n",
    "#         'values': [True, False]\n",
    "#     },\n",
    "#     'decay_rate': {\n",
    "#         'distribution': 'uniform',\n",
    "#         'min': 0.5,\n",
    "#         'max': 1\n",
    "#     },\n",
    "#     'total_iterations': {\n",
    "#         'value': total_iterations\n",
    "#     },\n",
    "#     'log_interval': {\n",
    "#         'value': log_interval\n",
    "#     },\n",
    "#     'series_iterations': {\n",
    "#         'value': series_iterations\n",
    "#     },\n",
    "#     'series_log_interval': {\n",
    "#         'value': series_log_interval\n",
    "#     },\n",
    "#     'loss_threshold': {\n",
    "#         'value': loss_threshold\n",
    "#     },\n",
    "#     'potential_boundary_batch_size': {\n",
    "#         'value': potential_boundary_batch_size\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features' : {\n",
    "        'value': 256\n",
    "    }, \n",
    "    'n_layers' : {\n",
    "        'value': 3\n",
    "    }, \n",
    "    'r' : {\n",
    "        'value': 256\n",
    "    }, \n",
    "    'out_dim' : {\n",
    "        'value': out_dim\n",
    "    }, \n",
    "    'Nx' : {\n",
    "        'value': Nx\n",
    "    }, \n",
    "    'Ny' : {\n",
    "        'value': Ny\n",
    "    }, \n",
    "    'Nz' : {\n",
    "        'value': Nz\n",
    "    }, \n",
    "    'b_norm' : {\n",
    "        'value': b_norm\n",
    "    },\n",
    "    'pos_enc' : {\n",
    "        'value': pos_enc\n",
    "    },\n",
    "    'mlp' : {\n",
    "        'values': ['mlp', 'modified_mlp']\n",
    "    },\n",
    "    'lr': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-5,\n",
    "        'max': 1e-3\n",
    "    },\n",
    "    'series_lr': {\n",
    "        'value': series_lr\n",
    "    },\n",
    "    'series_lr_decay_iterations': {\n",
    "        'value': series_lr_decay_iterations\n",
    "    },\n",
    "    'lr_decay_iterations': {\n",
    "        'values': [None, lr_decay_iterations//2, lr_decay_iterations]\n",
    "    },\n",
    "    'n_max_x': {\n",
    "        'value': n_max_x\n",
    "    },\n",
    "    'n_max_y': {\n",
    "        'value': n_max_y\n",
    "    },\n",
    "    'n_max_z': {\n",
    "        'value': n_max_z\n",
    "    },\n",
    "    'is_random':{\n",
    "        'value': True\n",
    "    },\n",
    "    'Nc':{\n",
    "        'value': 32\n",
    "    },\n",
    "    'random_interval':{\n",
    "        'value': 1\n",
    "    },\n",
    "    'w_ff': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-4,\n",
    "        'max': 1e3\n",
    "    },\n",
    "    'w_div': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-4,\n",
    "        'max': 1e3\n",
    "    },\n",
    "    'w_bc': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 1e-4,\n",
    "        'max': 1e3\n",
    "    },\n",
    "    'w_bc_decay_iterations': {\n",
    "        'values': [None, total_iterations, 2*total_iterations, 3*total_iterations]\n",
    "    },\n",
    "    'Ncx': {\n",
    "        'value': Ncx\n",
    "    },\n",
    "    'Ncy': {\n",
    "        'value': Ncy\n",
    "    },\n",
    "    'Ncz': {\n",
    "        'value': Ncz\n",
    "    },\n",
    "    'bc_batch_size': {\n",
    "        'value': 10000\n",
    "    },\n",
    "    'choice': {\n",
    "        'values': [True, False]\n",
    "    },\n",
    "    'decay_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.5,\n",
    "        'max': 1\n",
    "    },\n",
    "    'total_iterations': {\n",
    "        'value': total_iterations\n",
    "    },\n",
    "    'log_interval': {\n",
    "        'value': log_interval\n",
    "    },\n",
    "    'series_iterations': {\n",
    "        'value': series_iterations\n",
    "    },\n",
    "    'series_log_interval': {\n",
    "        'value': series_log_interval\n",
    "    },\n",
    "    'loss_threshold': {\n",
    "        'value': loss_threshold\n",
    "    },\n",
    "    'potential_boundary_batch_size': {\n",
    "        'value': potential_boundary_batch_size\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 5dczr4zq\n",
      "Sweep URL: https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"spinn-sweeps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        trainer = SPINN_series_Trainer(result_path, wandb, single=True)\n",
    "        trainer.setup(b_bottom_list)\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 58l2b4lj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNc: 90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcx: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcy: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcz: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNx: 344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNy: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNz: 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tb_norm: 2500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbc_batch_size: 22105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchoice: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay_rate: 0.5729721676051156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeatures: 202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_random: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_interval: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001863757619793979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_iterations: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp: modified_mlp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_x: 4.890698477309146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_y: 4.449616174877528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_z: 3.229832564482604\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_dim: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpos_enc: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpotential_boundary_batch_size: 3000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_interval: 78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_log_interval: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr_decay_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_iterations: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc: 314.5043086506869\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc_decay_iterations: 30000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_div: 558.508258902354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_ff: 186.92557118747925\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/userhome/jeon_mg/workspace/workspace_mine/zpinn/nbs/wandb/run-20230818_193046-58l2b4lj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgjeon/spinn-sweeps/runs/58l2b4lj' target=\"_blank\">still-sweep-1</a></strong> to <a href='https://wandb.ai/mgjeon/spinn-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgjeon/spinn-sweeps' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgjeon/spinn-sweeps/runs/58l2b4lj' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/runs/58l2b4lj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'features': {'distribution': 'int_uniform', 'min': 128, 'max': 512}, 'n_layers': {'distribution': 'int_uniform', 'min': 3, 'max': 8}, 'r': {'distribution': 'int_uniform', 'min': 128, 'max': 512}, 'out_dim': {'value': 3}, 'Nx': {'value': 344}, 'Ny': {'value': 224}, 'Nz': {'value': 160}, 'b_norm': {'value': 2500}, 'pos_enc': {'value': 0}, 'mlp': {'values': ['mlp', 'modified_mlp']}, 'lr': {'distribution': 'uniform', 'min': 1e-05, 'max': 0.001}, 'series_lr': {'value': 5e-05}, 'series_lr_decay_iterations': {'value': 200}, 'lr_decay_iterations': {'values': [None, 5000, 10000]}, 'n_max_x': {'distribution': 'uniform', 'min': 1, 'max': 8.6}, 'n_max_y': {'distribution': 'uniform', 'min': 1, 'max': 5.6}, 'n_max_z': {'distribution': 'uniform', 'min': 1, 'max': 4.0}, 'is_random': {'values': [True, False]}, 'Nc': {'distribution': 'int_uniform', 'min': 16, 'max': 344}, 'random_interval': {'distribution': 'int_uniform', 'min': 1, 'max': 1000}, 'w_ff': {'distribution': 'uniform', 'min': 0.0001, 'max': 1000.0}, 'w_div': {'distribution': 'uniform', 'min': 0.0001, 'max': 1000.0}, 'w_bc': {'distribution': 'uniform', 'min': 0.0001, 'max': 1000.0}, 'w_bc_decay_iterations': {'values': [None, 10000, 20000, 30000]}, 'Ncx': {'value': None}, 'Ncy': {'value': None}, 'Ncz': {'value': None}, 'bc_batch_size': {'distribution': 'int_uniform', 'min': 10000, 'max': 50000}, 'choice': {'values': [True, False]}, 'decay_rate': {'distribution': 'uniform', 'min': 0.5, 'max': 1}, 'total_iterations': {'value': 10000}, 'log_interval': {'value': 1000}, 'series_iterations': {'value': 200}, 'series_log_interval': {'value': 20}, 'loss_threshold': {'value': 0.001}, 'potential_boundary_batch_size': {'value': 3000}}\n",
      "Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-08-18 19:31:02.948637: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "Complie Start\n",
      "Complie End --> total: 34.21sec\n",
      " 10%|▉         | 998/10000 [01:32<13:55, 10.77it/s]Epoch: 1000/10000 --> total loss: 3.23943710, div 0.02201590, ff 0.07362944, b_diff 0.01455224\n",
      " 20%|█▉        | 1998/10000 [03:09<12:15, 10.87it/s]  Epoch: 2000/10000 --> total loss: 2.40018034, div 0.01557384, ff 0.12503861, b_diff 0.01539785\n",
      " 30%|██▉       | 2998/10000 [04:43<10:37, 10.99it/s]  Epoch: 3000/10000 --> total loss: 1.90939510, div 0.01236945, ff 0.08537655, b_diff 0.01517800\n",
      " 40%|███▉      | 3998/10000 [06:18<09:06, 10.98it/s]  Epoch: 4000/10000 --> total loss: 1.52743804, div 0.01197308, ff 0.06785249, b_diff 0.01502787\n",
      " 50%|████▉     | 4998/10000 [07:53<07:36, 10.96it/s]Epoch: 5000/10000 --> total loss: 1.13995922, div 0.01212835, ff 0.08257132, b_diff 0.01499027\n",
      " 60%|█████▉    | 5998/10000 [09:27<06:07, 10.90it/s]Epoch: 6000/10000 --> total loss: 0.87818789, div 0.00777471, ff 0.06705093, b_diff 0.01493747\n",
      " 70%|██████▉   | 6998/10000 [11:01<04:34, 10.95it/s]Epoch: 7000/10000 --> total loss: 0.69548929, div 0.00671545, ff 0.05642356, b_diff 0.01489687\n",
      " 80%|███████▉  | 7998/10000 [12:36<03:04, 10.86it/s]Epoch: 8000/10000 --> total loss: 0.56191027, div 0.00565667, ff 0.04580065, b_diff 0.01495707\n",
      " 90%|████████▉ | 8998/10000 [14:10<01:31, 10.97it/s]Epoch: 9000/10000 --> total loss: 0.45622191, div 0.00403199, ff 0.03396383, b_diff 0.01507400\n",
      "100%|█████████▉| 9998/10000 [15:45<00:00, 10.89it/s]Epoch: 10000/10000 --> total loss: 0.37122402, div 0.00292987, ff 0.02562858, b_diff 0.01515226\n",
      "100%|██████████| 10000/10000 [15:48<00:00, 10.54it/s]\n",
      "Runtime --> total: 948.42sec (94.85ms/iter.)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-sweep-1</strong> at: <a href='https://wandb.ai/mgjeon/spinn-sweeps/runs/58l2b4lj' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/runs/58l2b4lj</a><br/>Synced 6 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230818_193046-58l2b4lj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: udsieakk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNc: 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcx: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcy: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNcz: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNx: 344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNy: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNz: 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tb_norm: 2500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbc_batch_size: 38703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchoice: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay_rate: 0.6852663104609902\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeatures: 190\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tis_random: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlog_interval: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_threshold: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00046973987638591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_iterations: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp: mlp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_x: 3.1297977407670623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_y: 3.633963598829219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_max_z: 3.041092290627947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_dim: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpos_enc: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpotential_boundary_batch_size: 3000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_interval: 612\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_log_interval: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseries_lr_decay_iterations: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_iterations: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc: 426.08098595903266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_bc_decay_iterations: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_div: 143.07393412468076\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tw_ff: 545.7026474332998\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/userhome/jeon_mg/workspace/workspace_mine/zpinn/nbs/wandb/run-20230818_194743-udsieakk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgjeon/spinn-sweeps/runs/udsieakk' target=\"_blank\">honest-sweep-2</a></strong> to <a href='https://wandb.ai/mgjeon/spinn-sweeps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgjeon/spinn-sweeps' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/sweeps/5dczr4zq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgjeon/spinn-sweeps/runs/udsieakk' target=\"_blank\">https://wandb.ai/mgjeon/spinn-sweeps/runs/udsieakk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'features': {'distribution': 'int_uniform', 'min': 128, 'max': 512}, 'n_layers': {'distribution': 'int_uniform', 'min': 3, 'max': 8}, 'r': {'distribution': 'int_uniform', 'min': 128, 'max': 512}, 'out_dim': {'value': 3}, 'Nx': {'value': 344}, 'Ny': {'value': 224}, 'Nz': {'value': 160}, 'b_norm': {'value': 2500}, 'pos_enc': {'value': 0}, 'mlp': {'values': ['mlp', 'modified_mlp']}, 'lr': {'distribution': 'uniform', 'min': 1e-05, 'max': 0.001}, 'series_lr': {'value': 5e-05}, 'series_lr_decay_iterations': {'value': 200}, 'lr_decay_iterations': {'values': [None, 5000, 10000]}, 'n_max_x': {'distribution': 'uniform', 'min': 1, 'max': 8.6}, 'n_max_y': {'distribution': 'uniform', 'min': 1, 'max': 5.6}, 'n_max_z': {'distribution': 'uniform', 'min': 1, 'max': 4.0}, 'is_random': {'values': [True, False]}, 'Nc': {'distribution': 'int_uniform', 'min': 16, 'max': 344}, 'random_interval': {'distribution': 'int_uniform', 'min': 1, 'max': 1000}, 'w_ff': {'distribution': 'uniform', 'min': 0.0001, 'max': 1000.0}, 'w_div': {'distribution': 'uniform', 'min': 0.0001, 'max': 1000.0}, 'w_bc': {'distribution': 'uniform', 'min': 0.0001, 'max': 1000.0}, 'w_bc_decay_iterations': {'values': [None, 10000, 20000, 30000]}, 'Ncx': {'value': None}, 'Ncy': {'value': None}, 'Ncz': {'value': None}, 'bc_batch_size': {'distribution': 'int_uniform', 'min': 10000, 'max': 50000}, 'choice': {'values': [True, False]}, 'decay_rate': {'distribution': 'uniform', 'min': 0.5, 'max': 1}, 'total_iterations': {'value': 10000}, 'log_interval': {'value': 1000}, 'series_iterations': {'value': 200}, 'series_log_interval': {'value': 20}, 'loss_threshold': {'value': 0.001}, 'potential_boundary_batch_size': {'value': 3000}}\n",
      "Complie Start\n",
      "Complie End --> total: 32.73sec\n",
      "  5%|▌         | 538/10000 [00:32<09:28, 16.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 570/10000 [00:34<09:40, 16.25it/s]"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
