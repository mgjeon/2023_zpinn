{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a246a46e-cca7-45e6-9a3c-be55f8e7d048",
   "metadata": {},
   "source": [
    "# PINN_NF2_cleanup\n",
    "> https://github.com/RobertJaro/NF2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a6c4a2-8f1e-4234-920b-f27e92d57529",
   "metadata": {},
   "source": [
    "## Prepare `b_bottom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb7273-5a82-4940-b619-d2213ab1a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pinn_nf2_cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8b8a0-2daa-4dc4-97aa-9dbac97263e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import numpy as np\n",
    "import pickle\n",
    "from zpinn.lowloumag import LowLouMag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9b2b9-aa31-47b4-a8db-7f5d9e615154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "b = LowLouMag(resolutions=[64, 64, 64])\n",
    "b.calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a978210",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"b.pickle\",\"wb\") as f:\n",
    "    pickle.dump(b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"b.pickle\",\"rb\") as f:\n",
    "    b = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Ny, _ =  b.grid.dimensions\n",
    "bottom_subset = (0, Nx-1, 0, Ny-1, 0, 0)\n",
    "bottom = b.grid.extract_subset(bottom_subset).extract_surface()\n",
    "b_bottom = bottom['B'].reshape(Nx, Ny, 3)\n",
    "b_bottom = np.array(b_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22d1ae-de3f-4441-8540-88abef408aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_bottom.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ead1093-1f4c-4a92-9ef5-7e0a9f17781f",
   "metadata": {},
   "source": [
    "## PINN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ebcd0d-b284-4d13-bdae-575d8de74a79",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{B}(z=0)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32b759b5-fa5f-4236-8da4-950cdaf5a73b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_\\text{ff}(\\boldsymbol{\\theta}; \\mathcal{T}_f) = \\frac{1}{|\\mathcal{T}_f|} \\sum_{\\boldsymbol{x}\\in \\mathcal{T}_f} \\frac{|(\\nabla \\times \\mathbf{\\hat{B}})\\times \\mathbf{\\hat{B}}|^2}{|\\mathbf{\\hat{B}}|^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{div}(\\boldsymbol{\\theta}; \\mathcal{T}_f) = \\frac{1}{|\\mathcal{T}_f|} \\sum_{\\boldsymbol{x}\\in \\mathcal{T}_f} |\\nabla \\cdot \\mathbf{\\hat{B}}|^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{bc}(\\boldsymbol{\\theta};\\mathcal{T}_b)=\\frac{1}{|\\mathcal{T}_b|}\\sum_{\\boldsymbol{x}\\in\\mathcal{T}_b}{|\\mathbf{\\hat{B}}-\\mathbf{B}|^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = w_{\\text{ff}}\\mathcal{L}_\\text{ff} + w_{\\text{div}}\\mathcal{L}_\\text{div} +  w_{\\text{bc}}\\mathcal{L}_{\\text{bc}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601a1ab-87db-4165-9814-e957d13500f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/jeon_mg/mambaforge/envs/pinf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda import get_device_name\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b190e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3b560-4596-4f4e-a921-e764088655dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_coordinates(bounds):\n",
    "    xbounds = (bounds[0], bounds[1])\n",
    "    ybounds = (bounds[2], bounds[3])\n",
    "    zbounds = (bounds[4], bounds[5])\n",
    "    meshgrid = np.mgrid[xbounds[0]:xbounds[1]+1, ybounds[0]:ybounds[1]+1, zbounds[0]:zbounds[1]+1]\n",
    "    return np.stack(meshgrid, axis=-1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc36ba-c37c-4fac-af09-38bea4035a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BoundaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, batches_path):\n",
    "        self.batches_path = batches_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.load(self.batches_path, mmap_mode='r').shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy load data\n",
    "        d = np.load(self.batches_path, mmap_mode='r')[idx]\n",
    "        d = np.copy(d)\n",
    "        coord, field = d[:, 0],  d[:, 1]\n",
    "        return coord, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023565d-4248-4d81-8ce5-c961d5297342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, b_n, r_p):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b_n', b_n)\n",
    "        self.register_buffer('r_p', r_p)\n",
    "        c = np.array([[0, 0, 1/np.sqrt(2*np.pi)]])\n",
    "        c = torch.tensor(c, dtype=torch.float64)\n",
    "        self.register_buffer('c', c)\n",
    "\n",
    "    def forward(self, r):\n",
    "        numerator = self.b_n[:, None]\n",
    "        denominator = torch.sqrt(torch.sum((r[None, :] - self.r_p[:, None] + self.c[None])**2, -1))\n",
    "        potential = torch.sum(numerator/denominator, 0) / (2*np.pi)\n",
    "        return potential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2313f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_bc_data(b_bottom, height, b_norm, spatial_norm):\n",
    "    Nx, Ny, _ = b_bottom.shape\n",
    "    Nz = height\n",
    "\n",
    "    bottom_values = b_bottom.reshape(-1, 3)\n",
    "    bottom_bounds = (0, Nx-1, 0, Ny-1, 0, 0)\n",
    "    bottom_coords = create_coordinates(bottom_bounds).reshape(-1, 3)\n",
    "\n",
    "    # norms = {}\n",
    "    # if b_norm is None:\n",
    "    #     bottom_values_abs_max = np.max([np.abs(np.max(bottom_values)), np.abs(np.min(bottom_values))])\n",
    "    #     b_norm = int(bottom_values_abs_max)\n",
    "    #     norms['b_norm'] = b_norm\n",
    "    # if spatial_norm is None:\n",
    "    #     bottom_coords_max = np.max([np.abs(np.max(bottom_coords)), np.abs(np.min(bottom_coords))])\n",
    "    #     spatial_norm = int(bottom_coords_max)\n",
    "    #     norms['spatial_norm'] = spatial_norm\n",
    "\n",
    "    top_bounds = (0, Nx-1, 0, Ny-1, Nz-1, Nz-1)\n",
    "    lateral_bounds_1 = (0, 0, 0, Ny-1, 0, Nz-1)\n",
    "    lateral_bounds_2 = (Nx-1, Nx-1, 0, Ny-1, 0, Nz-1)\n",
    "    lateral_bounds_3 = (0, Nx-1, 0, 0, 0, Nz-1)\n",
    "    lateral_bounds_4 = (0, Nx-1, Ny-1, Ny-1, 0, Nz-1)\n",
    "\n",
    "    top_lateral_coordinates = [create_coordinates(top_bounds).reshape(-1, 3),\n",
    "                               create_coordinates(lateral_bounds_1).reshape(-1, 3),\n",
    "                               create_coordinates(lateral_bounds_2).reshape(-1, 3),\n",
    "                               create_coordinates(lateral_bounds_3).reshape(-1, 3),\n",
    "                               create_coordinates(lateral_bounds_4).reshape(-1, 3)]\n",
    "\n",
    "    b_n = torch.tensor(bottom_values[:, 2], dtype=torch.float64)\n",
    "    r_p = torch.tensor(bottom_coords, dtype=torch.float64)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = nn.DataParallel(PotentialModel(b_n, r_p)).to(device)\n",
    "\n",
    "    pf_fields = []\n",
    "    pf_coords = []\n",
    "    for r_coords in top_lateral_coordinates:\n",
    "        r_coords = torch.tensor(r_coords, dtype=torch.float64)\n",
    "        pf_batch_size = int(np.prod(r_coords.shape[:-1]) // 10)\n",
    "\n",
    "        fields = []\n",
    "        for r, in tqdm(DataLoader(TensorDataset(r_coords), batch_size=pf_batch_size, num_workers=2),\n",
    "                            desc='Potential Boundary'):\n",
    "            r = r.to(device).requires_grad_(True)\n",
    "            p_batch = model(r)\n",
    "            b_p = -1 * torch.autograd.grad(p_batch, r, torch.ones_like(p_batch), retain_graph=True, create_graph=True)[0]\n",
    "            fields += [b_p.clone().detach().cpu().numpy()]\n",
    "        pf_fields += [np.concatenate(fields)]\n",
    "        pf_coords += [r_coords.clone().detach().cpu().numpy()]\n",
    "\n",
    "    top_lateral_values = np.concatenate(pf_fields) \n",
    "    top_lateral_coords = np.concatenate(pf_coords)\n",
    "\n",
    "    boundary_values = np.concatenate([top_lateral_values, bottom_values])\n",
    "    boundary_coords = np.concatenate([top_lateral_coords, bottom_coords])\n",
    "\n",
    "    normalized_boundary_values = boundary_values / b_norm\n",
    "    normalized_boundary_coords = boundary_coords / spatial_norm\n",
    "\n",
    "    boundary_data = np.stack([normalized_boundary_coords, normalized_boundary_values], 1)\n",
    "\n",
    "    return boundary_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329d800-ff57-4f52-a11f-7017de5624e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_neurons, num_layers):\n",
    "        super().__init__()\n",
    "        self.d_in = nn.Linear(num_inputs, num_neurons)\n",
    "        lin = [nn.Linear(num_neurons, num_neurons) for _ in range(num_layers)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(num_neurons, num_outputs)\n",
    "        self.activation = torch.sin\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        B = self.d_out(x)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd68179-c83f-41f1-a0eb-9e7b2b8a3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NF2Trainer:\n",
    "    def __init__(self, base_path, b_bottom, height, b_norm=None, spatial_norm=None, meta_info=None, boundary_data=None):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        # meta info\n",
    "        self.meta_info = meta_info\n",
    "\n",
    "        # path\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "\n",
    "        # prepare boundary points + values\n",
    "        self.b_bottom = b_bottom\n",
    "        self.height = height\n",
    "        \n",
    "        self.b_norm = b_norm\n",
    "        self.spatial_norm = spatial_norm\n",
    "        if boundary_data is None:\n",
    "            self.boundary_data = prepare_bc_data(self.b_bottom, self.height, self.b_norm, self.spatial_norm)\n",
    "        else:\n",
    "            self.boundary_data = boundary_data\n",
    "\n",
    "        if self.b_norm is None:\n",
    "            self.b_norm = norms['b_norm']\n",
    "        if self.spatial_norm is None:\n",
    "            self.spatial_norm = norms['spatial_norm']\n",
    "        \n",
    "        # prepare collocation points\n",
    "        Nx, Ny, _ = self.b_bottom.shape\n",
    "        Nz = self.height\n",
    "        self.cube_shape = (Nx, Ny, Nz)\n",
    "        collocation_bounds = (0, Nx-1, 0, Ny-1, 0, Nz-1)\n",
    "        collocation_coords = create_coordinates(collocation_bounds).reshape(-1, 3)\n",
    "        normalized_collocation_coords = collocation_coords / self.spatial_norm\n",
    "        self.normalized_collocation_coords = torch.tensor(normalized_collocation_coords)  \n",
    "\n",
    "    def setup(self, total_iterations=50000, batch_size=10000, log_interval=100, num_workers=None,\n",
    "              num_neurons=256, num_layers=8, w_ff=1, w_div=1, w_bc_init=1000, decay_iterations=None,\n",
    "              transfer_learning_path=None, lr_init=5e-4, lr_final=5e-5, lr_decay_iterations=None):\n",
    "        device = self.device\n",
    "                        \n",
    "        self.total_iterations = total_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.log_interval = log_interval\n",
    "        self.num_workers = os.cpu_count() if num_workers is None else num_workers\n",
    "\n",
    "        # setup model and optimizer\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        # self.model = nn.DataParallel(BModel(3, 3, num_layers)).to(device)\n",
    "        self.model = nn.DataParallel(BModel(3, 3, num_neurons, num_layers)).to(device)\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr_init)\n",
    "        if lr_decay_iterations is None:\n",
    "            lr_decay_iterations = total_iterations\n",
    "        self.scheduler = ExponentialLR(self.opt, gamma=(lr_final / lr_init) ** (1 / lr_decay_iterations))\n",
    "\n",
    "        # loss weights\n",
    "        self.w_ff = w_ff\n",
    "        self.w_div = w_div\n",
    "        self.w_bc = w_bc_init\n",
    "        if decay_iterations is not None:\n",
    "            self.w_bc_decay = (1 / w_bc_init) ** (1 / decay_iterations)\n",
    "        else:\n",
    "            self.w_bc_decay = 1\n",
    "\n",
    "        self.required_iteration = self.total_iterations\n",
    "        self.start_iteration = 0\n",
    "        if transfer_learning_path is None:          \n",
    "            checkpoint_path = os.path.join(self.base_path, 'checkpoint.pt')\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "                self.start_iteration = state_dict['iteration']\n",
    "                self.model.load_state_dict(state_dict['m'])\n",
    "                self.opt.load_state_dict(state_dict['o'])\n",
    "                self.w_bc = state_dict['w_bc']\n",
    "                print(\"Resuming training from iteration %d\" % self.start_iteration)\n",
    "                self.required_iteration = self.required_iteration - self.start_iteration\n",
    "        else:\n",
    "            state_dict = torch.load(transfer_learning_path, map_location=device)\n",
    "            self.model.load_state_dict(state_dict['m'])\n",
    "            self.opt.load_state_dict(state_dict['o'])\n",
    "            print(f\"Load {transfer_learning_path}\")\n",
    "\n",
    "    def train(self):\n",
    "        device = self.device\n",
    "\n",
    "        boundary_data_loader, boundary_batches_path = self.create_boundary_batches(self.required_iteration)\n",
    "        \n",
    "        losses = []\n",
    "        losses_no_weight = []\n",
    "        self.model.train()\n",
    "        for iter, (boundary_coords, boundary_b) in tqdm(enumerate(boundary_data_loader, start=self.start_iteration),\n",
    "                                                        total=len(boundary_data_loader), desc='Training'):\n",
    "            # PDE loss\n",
    "            perm = torch.randperm(self.normalized_collocation_coords.shape[0])\n",
    "            idx = perm[:self.batch_size]\n",
    "            co_coords = self.normalized_collocation_coords[idx].to(device)\n",
    "            r = co_coords\n",
    "            r.requires_grad = True                   \n",
    "            B = self.model(r)\n",
    "            self.loss_ff, self.loss_div = self.PDEloss(B, r)\n",
    "                                                            \n",
    "            # boundary loss\n",
    "            boundary_coords, boundary_b = boundary_coords.to(device), boundary_b.to(device)\n",
    "            boundary_B = self.model(boundary_coords)                                 \n",
    "            loss_bc = torch.sum((boundary_B - boundary_b)**2, dim=-1)\n",
    "            self.loss_bc = torch.mean(loss_bc)\n",
    "\n",
    "            # total loss\n",
    "            self.loss = self.w_bc*self.loss_bc + self.w_ff*self.loss_ff + self.w_div*self.loss_div\n",
    "            losses.append(self.loss.detach().cpu().numpy())\n",
    "            loss_no_weight = self.loss_bc + self.loss_ff + self.loss_div\n",
    "            losses_no_weight.append(loss_no_weight.detach().cpu().numpy())\n",
    "\n",
    "            # save initial state\n",
    "            if iter == 0:\n",
    "                self.print_log(iter)\n",
    "                self.save(iter)\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "            self.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
    "            self.opt.step()\n",
    "\n",
    "            # save checkpoint state\n",
    "            if (self.log_interval > 0 and (iter + 1) % self.log_interval == 0):\n",
    "                self.print_log(iter+1)\n",
    "                self.save(iter+1)\n",
    "                self.save_state_dict(iter+1, 'checkpoint.pt')\n",
    "\n",
    "            # update training parameters\n",
    "            if self.w_bc > 1:\n",
    "                self.w_bc *= self.w_bc_decay\n",
    "                if self.w_bc <= 1:\n",
    "                    self.w_bc = 1\n",
    "            if self.scheduler.get_last_lr()[0] > 5e-5:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        np.save(os.path.join(self.base_path, 'losses.npy'), np.array(losses))\n",
    "        np.save(os.path.join(self.base_path, 'losses_no_weight.npy'), np.array(losses_no_weight))\n",
    "        # save final model state\n",
    "        self.print_log(self.total_iterations)\n",
    "        self.save(self.total_iterations)\n",
    "        self.save_state_dict(self.total_iterations, 'model_final.pt')\n",
    "        # cleanup\n",
    "        os.remove(boundary_batches_path)    \n",
    "\n",
    "    def create_boundary_batches(self, required_iterations):\n",
    "        boundary_data = self.boundary_data\n",
    "        batch_size = self.batch_size\n",
    "        num_workers = self.num_workers\n",
    "        \n",
    "        # shuffle data\n",
    "        r = np.random.permutation(boundary_data.shape[0])\n",
    "        boundary_data = boundary_data[r]\n",
    "    \n",
    "        # adjust to batch size\n",
    "        pad = batch_size - boundary_data.shape[0] % batch_size\n",
    "        boundary_data = np.concatenate([boundary_data, boundary_data[:pad]])\n",
    "    \n",
    "        # split data into batches\n",
    "        n_batches = boundary_data.shape[0] // batch_size\n",
    "        boundary_batches = np.array(np.split(boundary_data, n_batches), dtype=np.float32)\n",
    "    \n",
    "        # store batches to disk\n",
    "        boundary_batches_path = os.path.join(self.base_path, 'boundary_batches.npy')\n",
    "        np.save(boundary_batches_path, boundary_batches)\n",
    "        # create data loaders\n",
    "        boundary_dataset = BoundaryDataset(boundary_batches_path)\n",
    "        # create loader\n",
    "        boundary_data_loader = DataLoader(boundary_dataset, batch_size=None, num_workers=num_workers, pin_memory=True,\n",
    "                                 sampler=RandomSampler(boundary_dataset, replacement=True, num_samples=required_iterations))\n",
    "        return boundary_data_loader, boundary_batches_path\n",
    "\n",
    "    def PDEloss(self, B, r):\n",
    "        dBx_dr = torch.autograd.grad(B[:, 0], r, torch.ones_like(B[:, 0]), retain_graph=True, create_graph=True)[0]\n",
    "        dBy_dr = torch.autograd.grad(B[:, 1], r, torch.ones_like(B[:, 1]), retain_graph=True, create_graph=True)[0]\n",
    "        dBz_dr = torch.autograd.grad(B[:, 2], r, torch.ones_like(B[:, 2]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        dBx_dx = dBx_dr[:, 0]\n",
    "        dBx_dy = dBx_dr[:, 1]\n",
    "        dBx_dz = dBx_dr[:, 2]\n",
    "\n",
    "        dBy_dx = dBy_dr[:, 0]\n",
    "        dBy_dy = dBy_dr[:, 1]\n",
    "        dBy_dz = dBy_dr[:, 2]\n",
    "\n",
    "        dBz_dx = dBz_dr[:, 0]\n",
    "        dBz_dy = dBz_dr[:, 1]\n",
    "        dBz_dz = dBz_dr[:, 2]\n",
    "\n",
    "        rot_x = dBz_dy - dBy_dz\n",
    "        rot_y = dBx_dz - dBz_dx\n",
    "        rot_z = dBy_dx - dBx_dy\n",
    "\n",
    "        J = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "        JxB = torch.cross(J, B, dim=-1)\n",
    "\n",
    "        divB = dBx_dx + dBy_dy + dBz_dz\n",
    "\n",
    "        loss_ff = torch.sum(JxB**2, dim=-1) / (torch.sum(B**2, dim=-1) + 1e-7)\n",
    "        loss_ff = torch.mean(loss_ff)\n",
    "        # loss_div = torch.sum((divB)**2, dim=-1)\n",
    "        # loss_div = torch.mean(loss_div)\n",
    "        loss_div = torch.mean((divB)**2)\n",
    "\n",
    "        return loss_ff, loss_div\n",
    "        \n",
    "    def save(self, iteration):\n",
    "        torch.save({'iteration': iteration,\n",
    "                    'model': self.model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': self.b_norm,\n",
    "                    'spatial_norm': self.spatial_norm,\n",
    "                    'loss_bc': self.loss_bc.detach().cpu().numpy(),\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'loss_div': self.loss_div.detach().cpu().numpy(),\n",
    "                    'w_div': self.w_div,\n",
    "                    'loss_ff': self.loss_ff.detach().cpu().numpy(),\n",
    "                    'w_ff': self.w_ff,\n",
    "                    'LR':self.scheduler.get_last_lr()[0]}, \n",
    "                    os.path.join(self.base_path, 'model_%06d.pt' % iteration))\n",
    "        \n",
    "    def save_state_dict(self, iteration, filename):\n",
    "        torch.save({'iteration': iteration,\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'm': self.model.state_dict(),\n",
    "                    'o': self.opt.state_dict()},\n",
    "                    os.path.join(self.base_path, filename))\n",
    "\n",
    "    def print_log(self, iteration):\n",
    "        print('[Iteration %06d/%06d] [loss: %.08f] [loss_bc: %.08f; loss_div: %.08f; loss_ff: %.08f] [w_bc: %f, LR: %f]' %\n",
    "                (iteration, self.total_iterations,\n",
    "                self.loss,\n",
    "                self.w_bc*self.loss_bc,\n",
    "                self.w_ff*self.loss_ff,\n",
    "                self.w_div*self.loss_div,\n",
    "                self.w_bc,\n",
    "                self.scheduler.get_last_lr()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26ca5a57-209e-4731-a05c-2c8838a9c4f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414b0bb-e924-4726-8754-e65b57e825ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 15.40it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 14.70it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 16.22it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 14.50it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 16.87it/s]\n"
     ]
    }
   ],
   "source": [
    "base_path=\"results\"\n",
    "trainer = NF2Trainer(base_path, b_bottom, height=64, b_norm=100, spatial_norm=32, meta_info=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07355c-9810-4fb3-9880-0dfcf7b3db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.setup(total_iterations=10000, batch_size=10000, log_interval=1000, num_workers=2,\n",
    "              num_neurons=256, num_layers=8, w_ff=1, w_div=1, w_bc_init=1000, decay_iterations=25000)\n",
    "              # transfer_learning_path='/home/tensor/workspace/Python_stuff/zpinn/nbs/results/simul_01/model_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ba919-a7af-4af2-bca3-d7937a6c03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3f7ef-b08a-4a3d-a29f-1ccd97e0df55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BModel(\n",
       "    (d_in): Linear(in_features=3, out_features=256, bias=True)\n",
       "    (linear_layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (5): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (7): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (d_out): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b30b7-9956-44d2-87ec-df4390a80597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DataParallel                             --\n",
       "├─BModel: 1-1                            --\n",
       "│    └─Linear: 2-1                       1,024\n",
       "│    └─ModuleList: 2-2                   --\n",
       "│    │    └─Linear: 3-1                  65,792\n",
       "│    │    └─Linear: 3-2                  65,792\n",
       "│    │    └─Linear: 3-3                  65,792\n",
       "│    │    └─Linear: 3-4                  65,792\n",
       "│    │    └─Linear: 3-5                  65,792\n",
       "│    │    └─Linear: 3-6                  65,792\n",
       "│    │    └─Linear: 3-7                  65,792\n",
       "│    │    └─Linear: 3-8                  65,792\n",
       "│    └─Linear: 2-3                       771\n",
       "=================================================================\n",
       "Total params: 528,131\n",
       "Trainable params: 528,131\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebf4c3-61f6-48ea-a475-e9402bc059ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.cube_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a334ad8-4d02-401f-8e11-99fa3052e2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.b_norm, trainer.spatial_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536c11e-ed11-4bf5-97d5-0dcbcb5f674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4/10000 [00:01<54:21,  3.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 000000/010000] [loss: 38.52705765] [loss_bc: 38.52705002; loss_div: 0.00000794; loss_ff: 0.00000022] [w_bc: 1000.000000, LR: 0.000500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 825/10000 [00:36<06:51, 22.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 843/10000 [00:37<06:01, 25.30it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828e66d-36e5-4c23-85d3-ecd4f71f1451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
